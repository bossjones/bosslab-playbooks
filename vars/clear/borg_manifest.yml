# domain_root: hyenaclan.org
# domain_root: borglab.duckdns.org
domain_root: borglab.scarlettlab.home

base_dir_path: ~/dev/bossjones/bosslab-playbooks
dist_dir_path: "{{base_dir_path}}/dist"
manifest_dir_name: manifests/borg-manifests
manifest_dir_path: "{{dist_dir_path}}/{{manifest_dir_name}}"
path_to_network_disk: "/mnt/publicdata"
main_network_interface: 'ens192'
nfs_server_group: nfs_masters
nfs_client_group: nfs_clients
nfs_server_ip_override: 192.168.1.174
k8_admin_config_dir: ~/dev/bossjones/bosslab-playbooks
k8_admin_config_path: "{{k8_admin_config_dir}}/borg-admin.conf"

#      .########..######..##.....##..#######...######..########.########..##.....##.########.########.
#      .##.......##....##.##.....##.##.....##.##....##.##.......##.....##.##.....##.##.......##.....##
#      .##.......##.......##.....##.##.....##.##.......##.......##.....##.##.....##.##.......##.....##
#      .######...##.......#########.##.....##..######..######...########..##.....##.######...########.
#      .##.......##.......##.....##.##.....##.......##.##.......##...##....##...##..##.......##...##..
#      .##.......##....##.##.....##.##.....##.##....##.##.......##....##....##.##...##.......##....##.
#      .########..######..##.....##..#######...######..########.##.....##....###....########.##.....##


boss__echoserver__echoserver_subdomain: echoserver
boss__echoserver__manifest_path: "{{manifest_dir_path}}/echoserver"
boss__echoserver__namespace_name: echoserver
boss__echoserver__deployment_name: echoserver
boss__echoserver__echoserver_version: 2.1
boss__echoserver__echoserver_image_repo: "gcr.io/kubernetes-e2e-test-images/echoserver"
boss__echoserver__echoserver_image_tag: "{{ boss__echoserver__echoserver_version }}"
boss__echoserver__echoserver_cpu_limit: 100m
boss__echoserver__echoserver_mem_limit: 55Mi
boss__echoserver__echoserver_cpu_requests: 100m
boss__echoserver__echoserver_mem_requests: 20Mi
boss__echoserver__deployment_annotations_list:
  - name: nginx.ingress.kubernetes.io/ssl-redirect
    val: \"false\"
boss__echoserver__deployment_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
boss__echoserver__deployment_labels: |
  run: nginx
boss__echoserver__deployment_spec_replicas: 2
# boss__echoserver__deployment_spec_template_metadata_labels: "{{boss__echoserver__deployment_labels}}"
boss__echoserver__ingress_labels: |
  run: nginx
boss__echoserver__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__echoserver__service_labels: |
  run: nginx
boss__echoserver__service__spec_selector: |
  run: nginx
# boss__echoserver__echoserver_service_port: 9200


# # Kubernetes dashboard
# # RBAC required. see docs/getting-started.md for access details.
# dashboard_enabled: true

# # Addons which can be enabled
# efk_enabled: false
# helm_enabled: false
# istio_enabled: false
# registry_enabled: false
# enable_network_policy: false
# local_volume_provisioner_enabled: "{{ local_volumes_enabled | default('false') }}"
# persistent_volumes_enabled: false
# cephfs_provisioner_enabled: false
# ingress_nginx_enabled: false
# cert_manager_enabled: false

##########################################################################################################
#                   ..######.....###....##.......####..######...#######.
#                   .##....##...##.##...##........##..##....##.##.....##
#                   .##........##...##..##........##..##.......##.....##
#                   .##.......##.....##.##........##..##.......##.....##
#                   .##.......#########.##........##..##.......##.....##
#                   .##....##.##.....##.##........##..##....##.##.....##
#                   ..######..##.....##.########.####..######...#######.
##########################################################################################################

boss__calico__manifest_path: "{{manifest_dir_path}}/calico"
boss__calico__namespace_name: kube-system
boss__calico__prometheus_metrics_enabled: true

boss__calico__node_version: v3.3.2
boss__calico__node_image_repo: "quay.io/calico/node"
boss__calico__node_image_tag: "{{ boss__calico__node_version }}"

boss__calico__cni_version: v3.3.2
boss__calico__cni_image_repo: "quay.io/calico/cni"
boss__calico__cni_image_tag: "{{ boss__calico__node_version }}"

boss__calico__enable_prometheus_exporter_servicemonitor: "enabled"

boss__calico__prometheus_exporter_namespace_name: monitoring

boss__calico__felix_PrometheusMetricsPort: 9091
boss__calico__typha_PrometheusMetricsPort: 9093

boss__calico__node_daemonset_annotations: |
  scheduler.alpha.kubernetes.io/critical-pod: ''
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__felix_PrometheusMetricsPort}}"

boss__calico__enable_typha: "enabled"
boss__calico__typha_service_name: calico-typha  # or none

boss__calico__typha_deployment_spec_replicas: 3

boss__calico__typha_service_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"

boss__calico__typha_deployment_annotations: |
  # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
  # add-on, ensuring it gets priority scheduling and that its resources are reserved
  # if it ever gets evicted.
  scheduler.alpha.kubernetes.io/critical-pod: ''
  cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"

# {{ calico_node_image_repo }}:{{ calico_node_image_tag }}

##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__manifest_path: "{{manifest_dir_path}}/dashboard"
boss__dashboard__namespace_name: kube-system

boss__dashboard__version: v1.10.0
boss__dashboard__image_repo: "k8s.gcr.io/kubernetes-dashboard-amd64"
boss__dashboard__image_tag: "{{ boss__dashboard__version }}"

boss__dashboard__deployment_container_args: |
  - --auto-generate-certificates
  # Uncomment the following line to manually specify Kubernetes API server Host
  # If not specified, Dashboard will attempt to auto discover the API server and connect
  # to it. Uncomment only if the default does not work.
  # - --apiserver-host=http://my-address:port


##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__admin__manifest_path: "{{manifest_dir_path}}/dashboard-admin"
boss__dashboard__admin__namespace_name: kube-system


##########################################################################################################
#   .########.########.##....##
#   .##.......##.......##...##.
#   .##.......##.......##..##..
#   .######...######...#####...
#   .##.......##.......##..##..
#   .##.......##.......##...##.
#   .########.##.......##....##
##########################################################################################################
boss__efk__elasticsearch_subdomain: elasticsearch
boss__efk__manifest_path: "{{manifest_dir_path}}/efk"
boss__efk__namespace_name: kube-system
boss__efk__deployment_name: efk


##########################################################################################################
#    .########.##..........###.....######..########.####..######...######..########....###....########...######..##.....##
#    .##.......##.........##.##...##....##....##.....##..##....##.##....##.##.........##.##...##.....##.##....##.##.....##
#    .##.......##........##...##..##..........##.....##..##.......##.......##........##...##..##.....##.##.......##.....##
#    .######...##.......##.....##..######.....##.....##..##........######..######...##.....##.########..##.......#########
#    .##.......##.......#########.......##....##.....##..##.............##.##.......#########.##...##...##.......##.....##
#    .##.......##.......##.....##.##....##....##.....##..##....##.##....##.##.......##.....##.##....##..##....##.##.....##
#    .########.########.##.....##..######.....##....####..######...######..########.##.....##.##.....##..######..##.....##
##########################################################################################################
boss__efk__elasticsearch_ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__efk__elasticsearch_ingress_labels: |
  k8s-app: elasticsearch-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__elasticsearch_service_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Elasticsearch"
    boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__efk__elasticsearch_persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_version: v5.6.2
boss__efk__elasticsearch_image_repo: "bossjones/elasticsearch"
boss__efk__elasticsearch_image_tag: "{{ boss__efk__elasticsearch_version }}"
boss__efk__elasticsearch_cpu_limit: 1000m
boss__efk__elasticsearch_mem_limit: 4048Mi
boss__efk__elasticsearch_cpu_requests: 100m
boss__efk__elasticsearch_mem_requests: 2350Mi

boss__efk__elasticsearch_stateful_set_env_ES_JAVA_OPTS: "-Xms2048m -Xmx2048m"
boss__efk__elasticsearch_service_account_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_cluster_role_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_cluster_role_binding_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_stateful_set_labels: |
    k8s-app: elasticsearch-logging
    version: "{{boss__efk__elasticsearch_version}}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_stateful_set_spec_replicas: 1
boss__efk__elasticsearch_stateful_set_spec_selector_match_labels: |
        k8s-app: elasticsearch-logging
        version: v5.6.2
boss__efk__elasticsearch_stateful_set_spec_template_metadata_labels: |
        k8s-app: elasticsearch-logging
        version: v5.6.2
        kubernetes.io/cluster-service: "true"

#########################

##########################################################################################################
#     .########..######...........######..##.....##.########.....###....########..#######..########.
#     .##.......##....##.........##....##.##.....##.##.....##...##.##......##....##.....##.##.....##
#     .##.......##...............##.......##.....##.##.....##..##...##.....##....##.....##.##.....##
#     .######....######..#######.##.......##.....##.########..##.....##....##....##.....##.########.
#     .##.............##.........##.......##.....##.##...##...#########....##....##.....##.##...##..
#     .##.......##....##.........##....##.##.....##.##....##..##.....##....##....##.....##.##....##.
#     .########..######...........######...#######..##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__efk__elasticsearch_curator_deployment_labels: |
  k8s-app: es-curator
  boss-part-of: efk

boss__efk__elasticsearch_curator_version: 5.3.0-1
boss__efk__elasticsearch_curator_image_repo: "aknudsen/es-curator-service"
boss__efk__elasticsearch_curator_image_tag: "{{ boss__efk__elasticsearch_curator_version }}"
boss__efk__elasticsearch_curator_cpu_limit: 200m
boss__efk__elasticsearch_curator_mem_limit: 500Mi
boss__efk__elasticsearch_curator_cpu_requests: 100m
boss__efk__elasticsearch_curator_mem_requests: 200Mi

#########################


##########################################################################################################
#    .########.##.......##.....##.########.##....##.########.########.
#    .##.......##.......##.....##.##.......###...##....##....##.....##
#    .##.......##.......##.....##.##.......####..##....##....##.....##
#    .######...##.......##.....##.######...##.##.##....##....##.....##
#    .##.......##.......##.....##.##.......##..####....##....##.....##
#    .##.......##.......##.....##.##.......##...###....##....##.....##
#    .##.......########..#######..########.##....##....##....########.
##########################################################################################################
boss__efk__fluentd_version: v2.2.0
boss__efk__fluentd_image_repo: "bossjones/fluentd-elasticsearch"
boss__efk__fluentd_image_tag: "{{ boss__efk__fluentd_version }}"
boss__efk__fluentd_mem_limit: 500Mi
boss__efk__fluentd_cpu_requests: 100m
boss__efk__fluentd_mem_requests: 200Mi
boss__efk__fluentd_config_map_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_service_account_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_binding_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_labels: |
    k8s-app: fluentd-es
    version: v2.2.1
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_selector_match_labels: |
      k8s-app: fluentd-es
      version: v2.2.1
boss__efk__fluentd_daemon_set_spec_template_metadata_labels: |
        k8s-app: fluentd-es
        kubernetes.io/cluster-service: "true"
        version: v2.2.1
        boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_template_metadata_annotations: |
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'


boss__efk__fluentd_daemon_set_spec_template_spec_securityContext: |
  runAsNonRoot: false
  # runAsUser: 65534
  privileged: true

boss__efk__fluentd_daemon_set_spec_template_spec_volumeMounts: |
  - name: varlog
    mountPath: /var/log
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true
  # - name: libsystemddir
  #   mountPath: /host/lib
  #   readOnly: true
  - name: config-volume
    mountPath: /etc/fluent/config.d

boss__efk__fluentd_daemon_set_spec_template_spec_volumes: |
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
  # It is needed to copy systemd library to decompress journals
  # - name: libsystemddir
  #   hostPath:
  #     path: /usr/lib64
  - name: config-volume
    configMap:
      # name: fluentd-es-config-v0.1.0
      name: fluentd-es-config-v0.1.6

############################################


##########################################################################################################
#   .##....##.####.########.....###....##....##....###...
#   .##...##...##..##.....##...##.##...###...##...##.##..
#   .##..##....##..##.....##..##...##..####..##..##...##.
#   .#####.....##..########..##.....##.##.##.##.##.....##
#   .##..##....##..##.....##.#########.##..####.#########
#   .##...##...##..##.....##.##.....##.##...###.##.....##
#   .##....##.####.########..##.....##.##....##.##.....##
##########################################################################################################
boss__efk__kibana_subdomain: kibana
boss__efk__kibana_version: 5.6.2
boss__efk__kibana_image_repo: "bossjones/kibana"
boss__efk__kibana_image_tag: "{{ boss__efk__kibana_version }}"
boss__efk__kibana_cpu_limit: 1000m
boss__efk__kibana_cpu_requests: 1000m

boss__efk__kibana_deployment_labels: |
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__kibana_deployment_spec_replicas: 1
boss__efk__kibana_deployment_spec_selector_match_labels: |
      k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_metadata_labels: |
        k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "borg-worker-01"
boss__efk__kibana_ingress_labels: |
    k8s-app: kibana-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__kibana_ingress_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix
boss__efk__kibana_service_labels: |
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Kibana"
    boss-part-of: efk
############################################


boss__efk__elasticsearch_persistent_volume_spec_capacity_storage: 5Gi
boss__efk__elasticsearch_persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/elasticsearch"
# boss__efk__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__efk__nfs_master_node_ip: "{{ nfs_server_ip_override }}"


##########################################


##########################################################################################################
#      .########..########..######...####..######..########.########..##....##
#      .##.....##.##.......##....##...##..##....##....##....##.....##..##..##.
#      .##.....##.##.......##.........##..##..........##....##.....##...####..
#      .########..######...##...####..##...######.....##....########.....##...
#      .##...##...##.......##....##...##........##....##....##...##......##...
#      .##....##..##.......##....##...##..##....##....##....##....##.....##...
#      .##.....##.########..######...####..######.....##....##.....##....##...
##########################################################################################################
boss__registry__subdomain: registry
boss__registry__manifest_path: "{{manifest_dir_path}}/registry"
boss__registry__namespace_name: kube-system
boss__registry__deployment_name: registry
boss__registry__enable_pvc: false
boss__registry__enable_tls: true
boss__registry__tls_cert_cluster_issuer_name: selfsigning-issuer

boss__registry__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  nginx.ingress.kubernetes.io/rewrite-target: \"/\"
  traefik.frontend.rule.type: PathPrefix

boss__registry__ingress_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__service_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeRegistry"
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__version: 2.6
boss__registry__image_repo: "registry"
boss__registry__image_tag: "{{ boss__registry__version }}"
boss__registry__cpu_limit: 1000m
boss__registry__mem_limit: 500Mi
boss__registry__cpu_requests: 100m
boss__registry__mem_requests: 200Mi

boss__registry__service_account_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"
boss__registry__cluster_role_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__cluster_role_binding_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__stateful_set_labels: |
    k8s-app: registry
    version: "{{ boss__registry__version }}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry

boss__registry__stateful_set_spec_replicas: 1
boss__registry__stateful_set_spec_selector_match_labels: |
        k8s-app: registry
        version: "{{ boss__registry__version }}"

boss__registry__stateful_set_spec_template_metadata_labels: |
        k8s-app: registry
        version: "{{ boss__registry__version }}"
        kubernetes.io/cluster-service: "true"

boss__registry__persistent_volume_spec_capacity_storage: 10Gi
boss__registry__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__nfs_master_node_ip: "{{ nfs_server_ip_override }}"



##########################################################################################################
#         ..######..########.########..########.........##.....##....###....##....##....###.....######...########.########.
#         .##....##.##.......##.....##....##............###...###...##.##...###...##...##.##...##....##..##.......##.....##
#         .##.......##.......##.....##....##............####.####..##...##..####..##..##...##..##........##.......##.....##
#         .##.......######...########.....##....#######.##.###.##.##.....##.##.##.##.##.....##.##...####.######...########.
#         .##.......##.......##...##......##............##.....##.#########.##..####.#########.##....##..##.......##...##..
#         .##....##.##.......##....##.....##............##.....##.##.....##.##...###.##.....##.##....##..##.......##....##.
#         ..######..########.##.....##....##............##.....##.##.....##.##....##.##.....##..######...########.##.....##
##########################################################################################################

boss__certmanager__manifest_path: "{{manifest_dir_path}}/cert-manager"
boss__certmanager__namespace_name: kube-system
boss__certmanager__example_com_namespace_name: default


##########################################################################################################
#    .########..########..######...####..######..########.########..##....##.........##.....##.####
#    .##.....##.##.......##....##...##..##....##....##....##.....##..##..##..........##.....##..##.
#    .##.....##.##.......##.........##..##..........##....##.....##...####...........##.....##..##.
#    .########..######...##...####..##...######.....##....########.....##....#######.##.....##..##.
#    .##...##...##.......##....##...##........##....##....##...##......##............##.....##..##.
#    .##....##..##.......##....##...##..##....##....##....##....##.....##............##.....##..##.
#    .##.....##.########..######...####..######.....##....##.....##....##.............#######..####
##########################################################################################################
boss__registry__ui__subdomain: registry-ui
boss__registry__ui__manifest_path: "{{manifest_dir_path}}/registry-ui"
boss__registry__ui__namespace_name: default
boss__registry__ui__deployment_name: registry-ui
boss__registry__ui__enable_pvc: false
boss__registry__ui__enable_tls: true
boss__registry__ui__tls_cert_cluster_issuer_name: selfsigning-issuer

boss__registry__ui__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  nginx.ingress.kubernetes.io/rewrite-target: \"/\"
  traefik.frontend.rule.type: PathPrefix

boss__registry__ui__ingress_labels: |
  k8s-app: registry-ui
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry-ui
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_labels: |
    k8s-app: registry-ui
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeRegistry"
    boss-part-of: registry-ui
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry-ui
  boss-part-of: registry-ui
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__ui__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry-ui
  boss-part-of: registry-ui
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__version: 2.6
boss__registry__ui__image_repo: "registry"
boss__registry__ui__image_tag: "{{ boss__registry__ui__version }}"
boss__registry__ui__cpu_limit: 1000m
boss__registry__ui__mem_limit: 500Mi
boss__registry__ui__cpu_requests: 100m
boss__registry__ui__mem_requests: 200Mi

boss__registry__ui__service_account_labels: |
    k8s-app: registry-ui
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry-ui
    version: "{{ boss__registry__ui__version }}"
boss__registry__ui__cluster_role_labels: |
    k8s-app: registry-ui
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry-ui
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__cluster_role_binding_labels: |
    k8s-app: registry-ui
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry-ui
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_labels: |
    k8s-app: registry-ui
    version: "{{ boss__registry__ui__version }}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry-ui

boss__registry__ui__stateful_set_spec_replicas: 1
boss__registry__ui__stateful_set_spec_selector_match_labels: |
        k8s-app: registry-ui
        version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_spec_template_metadata_labels: |
        k8s-app: registry-ui
        version: "{{ boss__registry__ui__version }}"
        kubernetes.io/cluster-service: "true"

boss__registry__ui__persistent_volume_spec_capacity_storage: 10Gi
boss__registry__ui__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry-ui"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__ui__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__ui__nfs_master_node_ip: "{{ nfs_server_ip_override }}"




##########################################################################################################
#     .......##.########.##....##.##....##.####.##....##..######.
#     .......##.##.......###...##.##...##...##..###...##.##....##
#     .......##.##.......####..##.##..##....##..####..##.##......
#     .......##.######...##.##.##.#####.....##..##.##.##..######.
#     .##....##.##.......##..####.##..##....##..##..####.......##
#     .##....##.##.......##...###.##...##...##..##...###.##....##
#     ..######..########.##....##.##....##.####.##....##..######.
##########################################################################################################
boss__jenkins__subdomain: jenkins
boss__jenkins__version: lts
boss__jenkins__image_repo: "jenkins/jenkins"
boss__jenkins__image_tag: "{{ boss__jenkins__version }}"
boss__jenkins__cpu_limit: 1000m
boss__jenkins__cpu_requests: 1000m
boss__jenkins__manifest_path: "{{manifest_dir_path}}/jenkins-k8"
boss__jenkins__namespace_name: kube-system

boss__jenkins__deployment_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__deployment_spec_replicas: 1
boss__jenkins__deployment_spec_selector_match_labels: |
      k8s-app: jenkins
boss__jenkins__deployment_spec_template_metadata_labels: |
        k8s-app: jenkins
boss__jenkins__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "borg-worker-01"
boss__jenkins__ingress_labels: |
    k8s-app: jenkins-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__jenkins__service_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "jenkins"
    boss-part-of: jenkins

boss__jenkins__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__jenkins__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_spec_capacity_storage: 2Gi
boss__jenkins__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/jenkins"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__jenkins__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__jenkins__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#              .##.....##.########....###....########...######..########.########.########.
#              .##.....##.##.........##.##...##.....##.##....##....##....##.......##.....##
#              .##.....##.##........##...##..##.....##.##..........##....##.......##.....##
#              .#########.######...##.....##.########...######.....##....######...########.
#              .##.....##.##.......#########.##..............##....##....##.......##...##..
#              .##.....##.##.......##.....##.##........##....##....##....##.......##....##.
#              .##.....##.########.##.....##.##.........######.....##....########.##.....##
##########################################################################################################
boss__heapster__subdomain: heapster
boss__heapster__version: lts
boss__heapster__image_repo: "heapster/heapster"
boss__heapster__image_tag: "{{ boss__heapster__version }}"
boss__heapster__cpu_limit: 1000m
boss__heapster__cpu_requests: 1000m
boss__heapster__manifest_path: "{{manifest_dir_path}}/heapster2"
boss__heapster__namespace_name: kube-system

boss__heapster__deployment_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__deployment_spec_replicas: 1
boss__heapster__deployment_spec_selector_match_labels: |
      k8s-app: heapster
boss__heapster__deployment_spec_template_metadata_labels: |
        k8s-app: heapster
boss__heapster__ingress_labels: |
    k8s-app: heapster-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__heapster__service_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "heapster"
    boss-part-of: heapster

boss__heapster__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__heapster__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_spec_capacity_storage: 2Gi
boss__heapster__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/heapster"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__heapster__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__heapster__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################

##########################################################################################################
# ................##.....##.########.########.########..####..######...######...........######..########.########..##.....##.########.########.
# ................###...###.##..........##....##.....##..##..##....##.##....##.........##....##.##.......##.....##.##.....##.##.......##.....##
# ................####.####.##..........##....##.....##..##..##.......##...............##.......##.......##.....##.##.....##.##.......##.....##
# ................##.###.##.######......##....########...##..##........######..#######..######..######...########..##.....##.######...########.
# ................##.....##.##..........##....##...##....##..##.............##...............##.##.......##...##....##...##..##.......##...##..
# ................##.....##.##..........##....##....##...##..##....##.##....##.........##....##.##.......##....##....##.##...##.......##....##.
# ................##.....##.########....##....##.....##.####..######...######...........######..########.##.....##....###....########.##.....##
##########################################################################################################
boss__metrics__server__subdomain: metrics-server
boss__metrics__server__version: lts
boss__metrics__server__image_repo: "metrics-server/metrics-server"
boss__metrics__server__image_tag: "{{ boss__metrics__server__version }}"
boss__metrics__server__cpu_limit: 1000m
boss__metrics__server__cpu_requests: 1000m
boss__metrics__server__manifest_path: "{{manifest_dir_path}}/metrics-server"
boss__metrics__server__namespace_name: kube-system

boss__metrics__server__deployment_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__deployment_spec_replicas: 1
boss__metrics__server__deployment_spec_selector_match_labels: |
      k8s-app: metrics-server
boss__metrics__server__deployment_spec_template_metadata_labels: |
        k8s-app: metrics-server
boss__metrics__server__ingress_labels: |
    k8s-app: metrics-server-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__metrics__server__service_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "metrics-server"
    boss-part-of: metrics-server

boss__metrics__server__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__metrics__server__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_spec_capacity_storage: 2Gi
boss__metrics__server__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/metrics-server"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__metrics__server__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__metrics__server__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#   .########.##.....##.########.########.########..##....##....###....##...............########..##....##..######.
#   .##........##...##.....##....##.......##.....##.###...##...##.##...##...............##.....##.###...##.##....##
#   .##.........##.##......##....##.......##.....##.####..##..##...##..##...............##.....##.####..##.##......
#   .######......###.......##....######...########..##.##.##.##.....##.##.......#######.##.....##.##.##.##..######.
#   .##.........##.##......##....##.......##...##...##..####.#########.##...............##.....##.##..####.......##
#   .##........##...##.....##....##.......##....##..##...###.##.....##.##...............##.....##.##...###.##....##
#   .########.##.....##....##....########.##.....##.##....##.##.....##.########.........########..##....##..######.
##########################################################################################################
boss__external__dns__subdomain: external-dns
boss__external__dns__version: lts
boss__external__dns__image_repo: "external-dns/external-dns"
boss__external__dns__image_tag: "{{ boss__external__dns__version }}"
boss__external__dns__cpu_limit: 1000m
boss__external__dns__cpu_requests: 1000m
boss__external__dns__manifest_path: "{{manifest_dir_path}}/external-dns"
boss__external__dns__namespace_name: default

boss__external__dns__deployment_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__deployment_spec_replicas: 1
boss__external__dns__deployment_spec_selector_match_labels: |
      k8s-app: external-dns
boss__external__dns__deployment_spec_template_metadata_labels: |
        k8s-app: external-dns
boss__external__dns__ingress_labels: |
    k8s-app: external-dns-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__external__dns__service_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "external-dns"
    boss-part-of: external-dns

boss__external__dns__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__external__dns__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_spec_capacity_storage: 2Gi
boss__external__dns__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/external-dns"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__external__dns__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__external__dns__nfs_master_node_ip: "{{ nfs_server_ip_override }}"



##########################################################################################################
#              .##.....##.########.##.......##.....##
#              .##.....##.##.......##.......###...###
#              .##.....##.##.......##.......####.####
#              .#########.######...##.......##.###.##
#              .##.....##.##.......##.......##.....##
#              .##.....##.##.......##.......##.....##
#              .##.....##.########.########.##.....##
##########################################################################################################
boss__helm__subdomain: helm
boss__helm__manifest_path: "{{manifest_dir_path}}/helm"
boss__helm__namespace_name: kube-system

##########################################################################################################
#     .##.....##.########.########....###....##...............##.......########.
#     .###...###.##..........##......##.##...##...............##.......##.....##
#     .####.####.##..........##.....##...##..##...............##.......##.....##
#     .##.###.##.######......##....##.....##.##.......#######.##.......########.
#     .##.....##.##..........##....#########.##...............##.......##.....##
#     .##.....##.##..........##....##.....##.##...............##.......##.....##
#     .##.....##.########....##....##.....##.########.........########.########.
##########################################################################################################
boss__metallb__subdomain: metallb
boss__metallb__manifest_path: "{{manifest_dir_path}}/metallb"
boss__metallb__namespace_name: metallb-system
boss__metallb__address_pools: '192.168.1.30-192.168.1.40'




##########################################################################################################
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...########..########..######...######.
#              .###...##.##....##...##..###...##..##...##...........##..###...##.##....##..##.....##.##.......##....##.##....##
#              .####..##.##.........##..####..##...##.##............##..####..##.##........##.....##.##.......##.......##......
#              .##.##.##.##...####..##..##.##.##....###....#######..##..##.##.##.##...####.########..######....######...######.
#              .##..####.##....##...##..##..####...##.##............##..##..####.##....##..##...##...##.............##.......##
#              .##...###.##....##...##..##...###..##...##...........##..##...###.##....##..##....##..##.......##....##.##....##
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...##.....##.########..######...######.
##########################################################################################################
boss__ingress__nginx__subdomain: ingress-nginx
boss__ingress__nginx__manifest_path: "{{manifest_dir_path}}/ingress-nginx"
boss__ingress__nginx__namespace_name: kube-system


boss__ingress__nginx__controller_version: 0.21.0
boss__ingress__nginx__controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
boss__ingress__nginx__controller_image_tag: "{{ boss__ingress__nginx__controller_version }}"

boss__ingress__nginx__controller_runAsUser: 33

boss__ingress__nginx__deployment_labels: |
    app.kubernetes.io/name: nginx-ingress-controller
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: nginx-ingress
    addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_replicas: 1
boss__ingress__nginx__deployment_spec_selector_match_labels: |
      app.kubernetes.io/name: nginx-ingress-controller
      # app.kubernetes.io/part-of: kube-system
      app.kubernetes.io/part-of: nginx-ingress
      addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_labels: |
        app.kubernetes.io/name: nginx-ingress-controller
        # app.kubernetes.io/part-of: kube-system
        app.kubernetes.io/part-of: nginx-ingress
        addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_annotations: |
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
boss__ingress__nginx__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "borg-queen-01"

boss__ingress__nginx__service_account_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_default_http_backend_spec_type: NodePort
boss__ingress__nginx__service_ingress_nginx_spec_type: NodePort

boss__ingress__nginx__role_binding_labels: |
    # SOURCE: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Many of these are system: prefixed, which indicates that the resource is “owned” by the infrastructure. Modifications to these resources can result in non-functional clusters. One example is the system:node ClusterRole. This role defines permissions for kubelets. If the role is modified, it can prevent kubelets from working.
    # All of the default cluster roles and rolebindings are labeled with kubernetes.io/bootstrapping=rbac-defaults
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service__default_http_backend_labels: |
    app.kubernetes.io/name: default-http-backend
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: nginx-ingress
    # kubernetes.io/minikube-addons: ingress
    # kubernetes.io/minikube-addons-endpoint: ingress
    addonmanager.kubernetes.io/mode: Reconcile




####################################################################################################
#                  .########.########.....###....########.########.####.##....##.........####.##....##.########.########.########..##....##....###....##......
#                  ....##....##.....##...##.##...##.......##........##..##...##...........##..###...##....##....##.......##.....##.###...##...##.##...##......
#                  ....##....##.....##..##...##..##.......##........##..##..##............##..####..##....##....##.......##.....##.####..##..##...##..##......
#                  ....##....########..##.....##.######...######....##..#####....#######..##..##.##.##....##....######...########..##.##.##.##.....##.##......
#                  ....##....##...##...#########.##.......##........##..##..##............##..##..####....##....##.......##...##...##..####.#########.##......
#                  ....##....##....##..##.....##.##.......##........##..##...##...........##..##...###....##....##.......##....##..##...###.##.....##.##......
#                  ....##....##.....##.##.....##.########.##.......####.##....##.........####.##....##....##....########.##.....##.##....##.##.....##.########
##########################################################################################################
boss__traefik__internal__subdomain: traefik-internal
boss__traefik__internal__manifest_path: "{{manifest_dir_path}}/traefik-internal"
boss__traefik__internal__namespace_name: traefik-system
boss__traefik__internal__service_spec_type: LoadBalancer
boss__traefik__internal__service_spec_loadBalancerIP: 192.168.1.30


####################################################################################################
#          .##......##.########....###....##.....##.########..........######...######...#######..########..########
#          .##..##..##.##.........##.##...##.....##.##...............##....##.##....##.##.....##.##.....##.##......
#          .##..##..##.##........##...##..##.....##.##...............##.......##.......##.....##.##.....##.##......
#          .##..##..##.######...##.....##.##.....##.######...#######..######..##.......##.....##.########..######..
#          .##..##..##.##.......#########..##...##..##.....................##.##.......##.....##.##........##......
#          .##..##..##.##.......##.....##...##.##...##...............##....##.##....##.##.....##.##........##......
#          ..###..###..########.##.....##....###....########..........######...######...#######..##........########
##########################################################################################################
boss__weave__scope__subdomain: weave-scope
boss__weave__scope__manifest_path: "{{manifest_dir_path}}/weave-scope"
boss__weave__scope__namespace_name: weave
boss__weave__scope__manifest_filename_based_on_networking: "weave-calico-networking.yaml"
boss__weave__scope__enable_ingress_traefik: "enabled"

boss__weave__scope__ingress_metadata_annotations: |
  traefik.frontend.rule.type: PathPrefix


###########################

boss__prometheus__operator__subdomain: prometheus-operator
boss__prometheus__operator__manifest_path: "{{manifest_dir_path}}/prometheus-operator-v0-27-0"
boss__prometheus__operator__namespace_name: monitoring

# This doesn't have a last tag or anything like that
boss__prometheus__operator__alertmanager_subdomain: alertmanager
boss__prometheus__operator__alertmanager_version: v0.15.3
boss__prometheus__operator__alertmanager_crd_baseImage: "quay.io/prometheus/alertmanager"
boss__prometheus__operator__alertmanager_image_tag: "{{ boss__prometheus__operator__alertmanager_version }}"

boss__prometheus__operator__grafana_subdomain: grafana
boss__prometheus__operator__grafana_version: 5.2.4
boss__prometheus__operator__grafana_image_repo: "grafana/grafana"
boss__prometheus__operator__grafana_image_tag: "{{ boss__prometheus__operator__grafana_version }}"
boss__prometheus__operator__grafana_cpu_limit: 2000m
boss__prometheus__operator__grafana_mem_limit: 500Mi
boss__prometheus__operator__grafana_cpu_requests: 1000m
boss__prometheus__operator__grafana_mem_requests: 200Mi
boss__prometheus__operator__grafana_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 2000m
    memory: 200Mi
  requests:
    cpu: 1000m
    memory: 100Mi

boss__prometheus__operator__prometheus_subdomain: prometheus
boss__prometheus__operator__prometheus_version: v2.5.0
boss__prometheus__operator__prometheus_crd_baseImage: "quay.io/prometheus/prometheus"
boss__prometheus__operator__prometheus_image_tag: "{{ boss__prometheus__operator__prometheus_version }}"

boss__prometheus__operator__node_exporter_version: v0.16.0
boss__prometheus__operator__node_exporter_image_repo: "quay.io/prometheus/node-exporter"
boss__prometheus__operator__node_exporter_image_tag: "{{ boss__prometheus__operator__node_exporter_version }}"
boss__prometheus__operator__node_exporter_daemonset_spec_template_spec_containers_resources: |
  limits:
    cpu: 250m
    memory: 180Mi
  requests:
    cpu: 102m
    memory: 180Mi

boss__prometheus__operator__kube_rbac_proxy_version: v0.4.0
boss__prometheus__operator__kube_rbac_proxy_image_repo: "quay.io/coreos/kube-rbac-proxy"
boss__prometheus__operator__kube_rbac_proxy_image_tag: "{{ boss__prometheus__operator__kube_rbac_proxy_version }}"
boss__prometheus__operator__kube_rbac_proxy_daemonset_spec_template_spec_containers_resources: |
  # NOTE: Orig below ( 3/3/2019 )
  #limits:
  #  cpu: 20m
  #  memory: 40Mi
  #requests:
  #  cpu: 10m
  #  memory: 20Mi
  limits:
    cpu: 100m
    memory: 120Mi
  requests:
    cpu: 90m
    memory: 60Mi

boss__prometheus__operator__kube_rbac_proxy_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 20m
    memory: 40Mi
  requests:
    cpu: 10m
    memory: 20Mi

boss__prometheus__operator__kube_state_metrics_version: v1.4.0
boss__prometheus__operator__kube_state_metrics_image_repo: "quay.io/coreos/kube-state-metrics"
boss__prometheus__operator__kube_state_metrics_image_tag: "{{ boss__prometheus__operator__kube_state_metrics_version }}"
boss__prometheus__operator__kube_state_metrics_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 100m
    memory: 150Mi
  requests:
    cpu: 100m
    memory: 150Mi

boss__prometheus__operator__addon_resizer_version: 1.0
boss__prometheus__operator__addon_resizer_image_repo: "quay.io/coreos/addon-resizer"
boss__prometheus__operator__addon_resizer_image_tag: "{{ boss__prometheus__operator__addon_resizer_version }}"
boss__prometheus__operator__addon_resizer_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 50m
    memory: 30Mi
  requests:
    cpu: 10m
    memory: 30Mi

boss__prometheus__operator__prometheus_operator_version: v0.29.0
boss__prometheus__operator__prometheus_operator_image_repo: "quay.io/coreos/prometheus-operator"
boss__prometheus__operator__prometheus_operator_image_tag: "{{ boss__prometheus__operator__prometheus_operator_version }}"
boss__prometheus__operator__prometheus_operator_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

boss__prometheus__operator__configmap_reload_version: v0.0.1
boss__prometheus__operator__configmap_reload_image_repo: "quay.io/coreos/configmap-reload"
boss__prometheus__operator__configmap_reload_image_tag: "{{ boss__prometheus__operator__configmap_reload_version }}"

boss__prometheus__operator__prometheus_operator_config_reloader_version: v0.29.0
boss__prometheus__operator__prometheus_operator_config_reloader_image_repo: "quay.io/coreos/prometheus-config-reloader"
boss__prometheus__operator__prometheus_operator_config_reloader_image_tag: "{{ boss__prometheus__operator__prometheus_operator_config_reloader_version }}"
boss__prometheus__operator__prometheus_operator_config_reloader_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

boss__prometheus__operator__alertmanager_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__alertmanager_ingress_metadata_labels: |
  run: nginx
  alertmanager: main
  app: alertmanager


boss__prometheus__operator__prometheus_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__prometheus_ingress_metadata_labels: |
  app: prometheus
  prometheus: k8s
  run: nginx
  k8s-app: prometheus-operator


boss__prometheus__operator__grafana_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__grafana_ingress_metadata_labels: |
  run: nginx
  app: grafana

################# Persistent volume claims

# persistent volume claim - grafana
boss__prometheus__operator__grafana_pvc_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator

boss__prometheus__operator__grafana_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__grafana_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - grafana
boss__prometheus__operator__grafana_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__grafana_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__grafana_pv_spec_nfs: |
    server: 192.168.1.174
    path: "/mnt/publicdata/grafana"


# persistent volume claim - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator
  boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_nfs: |
    server: 192.168.1.174
    path: "/mnt/publicdata/prometheus-adapter-tmpfs"

# persistent volume claim - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_nfs: |
    server: 192.168.1.174
    path: "/mnt/publicdata/prometheus-adapter-volume-serving-cert"


boss__prometheus__operator__prometheus_additional_scrape_configs: |
  # RUN: kubectl --namespace monitoring create secret generic additional-scrape-configs --from-file=PLAINTEXT-SECRET-prometheus-additional.yaml --dry-run -oyaml > additional-scrape-configs.yaml
  - job_name: 'netdata-scrape'

    metrics_path: '/api/v1/allmetrics'
    params:
      # format: prometheus | prometheus_all_hosts
      # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP
      format: [prometheus]
      #
      # sources: as-collected | raw | average | sum | volume
      # default is: average
      #source: [as-collected]
      #
      # server name for this prometheus - the default is the client IP
      # for netdata to uniquely identify it
      #server: ['prometheus1']
    honor_labels: true

    static_configs:
      - targets: ['http://borg-queen-01.scarlettlab.home:19999','http://borg-worker-01.scarlettlab.home:19999','http://borg-worker-02.scarlettlab.home:19999']

# SOURCE: https://techtran.science/2018/06/11/unifi-to-grafana-using-prometheus-and-unifi_exporter/
# - job_name: 'unifi_exporter'
#   static_configs:
#     - targets: ['dockerswarm:9130']
#       labels:
#         alias: unifi_exporter


####################################################################################################
#   .##.....##.##....##.####.########.####.........########.##.....##.########...#######..########..########.########.########.
#   .##.....##.###...##..##..##........##..........##........##...##..##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.####..##..##..##........##..........##.........##.##...##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.##.##.##..##..######....##..#######.######......###....########..##.....##.########.....##....######...########.
#   .##.....##.##..####..##..##........##..........##.........##.##...##........##.....##.##...##......##....##.......##...##..
#   .##.....##.##...###..##..##........##..........##........##...##..##........##.....##.##....##.....##....##.......##....##.
#   ..#######..##....##.####.##.......####.........########.##.....##.##.........#######..##.....##....##....########.##.....##
####################################################################################################
boss__unifi__exporter__subdomain: unifi-exporter
boss__unifi__exporter__manifest_path: "{{manifest_dir_path}}/unifi-exporter"
boss__unifi__exporter__namespace_name: monitoring
boss__unifi__exporter__apply_changes_immediately: False
boss__unifi__exporter__delete_secrets: True

boss__unifi__exporter__port: 9130
boss__unifi__exporter__listen_address: ":{{boss__unifi__exporter__port}}"
boss__unifi__exporter__listen_metricspath: /metrics
boss__unifi__exporter__unifi_address: https://192.168.1.8:8443
boss__unifi__exporter__unifi_site: Hyenanet
boss__unifi__exporter__unifi_insecure: true
boss__unifi__exporter__unifi_timeout: 5s


boss__unifi__exporter__service_annotations: |
  prometheus.io/scrape: 'true'
  prometheus.io/port: '9091'




##########################################################################################################
#   .####.##....##.########.##.......##.....##.##.....##.########..########...........#######..########..########.########.....###....########..#######..########.
#   ..##..###...##.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##.....##.##.......##.....##...##.##......##....##.....##.##.....##
#   ..##..####..##.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##.....##.##.......##.....##..##...##.....##....##.....##.##.....##
#   ..##..##.##.##.######...##.......##.....##....###....##.....##.########..#######.##.....##.########..######...########..##.....##....##....##.....##.########.
#   ..##..##..####.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##........##.......##...##...#########....##....##.....##.##...##..
#   ..##..##...###.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##........##.......##....##..##.....##....##....##.....##.##....##.
#   .####.##....##.##.......########..#######..##.....##.########..########...........#######..##........########.##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__influxdb__operator__subdomain: influxdb
boss__influxdb__operator__manifest_path: "{{manifest_dir_path}}/influxdb-operator"
boss__influxdb__operator__namespace_name: monitoring
boss__influxdb__operator__name: influxdata-operator


boss__influxdb__operator__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__influxdb__operator__ingress_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__deployment_labels: |
  app: influxdata-operator
  name: influxdata-operator

boss__influxdb__operator__service_labels: |
    k8s-app: influxdata-operator
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "influxdata-operator"
    name: influxdata-operator
    boss-part-of: influxdb
    app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_spec_resources_requests_storage: "2Gi"

boss__influxdb__operator__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__version: 1.6.6
boss__influxdb__operator__image_repo: "influxdb"
boss__influxdb__operator__image_tag: "{{ boss__influxdb__operator__version }}"
boss__influxdb__operator__cpu_limit: 1000m
boss__influxdb__operator__mem_limit: 4048Mi
boss__influxdb__operator__cpu_requests: 100m
boss__influxdb__operator__mem_requests: 2350Mi

boss__influxdb__operator__service_account_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_binding_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__persistent_volume_spec_capacity_storage: 1Gi
boss__influxdb__operator__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/influxdb-operator"
# boss__influxdb__operator__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__influxdb__operator__nfs_master_node_ip: "{{ nfs_server_ip_override }}"

#######################

vagrant_playbook_enable_role_tools: "enabled"
vagrant_playbook_enable_role_bootstrap: "enabled"
vagrant_playbook_enable_role_nfs_master: "enabled"
vagrant_playbook_enable_role_nfs_client: "enabled"
vagrant_playbook_enable_role_rsyslogd: "enabled"
vagrant_playbook_enable_role_journald: "enabled"
vagrant_playbook_enable_role_core: "enabled"
vagrant_playbook_enable_role_update_hosts: "enabled"
vagrant_playbook_enable_role_fact: "enabled"
vagrant_playbook_enable_role_environment: "enabled"
vagrant_playbook_enable_role_etckeeper: "enabled"
vagrant_playbook_enable_role_timezone: "enabled"
vagrant_playbook_enable_role_ntp: "enabled"
vagrant_playbook_enable_role_debug: "enabled"
vagrant_playbook_enable_role_logrotate: "disabled"
vagrant_playbook_enable_role_netdata: "disabled"


# Template playbooks
vagrant_tools_roles: |
  - role: boss-ansible-role-tools
    task: tools
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: kube
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: docker
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: glances
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: rust
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools

vagrant_tools_all_roles: |
    - role: boss-ansible-role-tools
      task: tools
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: kube
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: docker
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: glances
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: rust
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: npm
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools

vagrant_playbook_role_core_data: |
    - role: boss-ansible-role-core
      tags:
        - boss-ansible-role-core

vagrant_playbook_role_update_hosts_data: |
    - role: boss-ansible-role-update-hosts
      boss__update__hosts__hosts_file: /etc/hosts
      # ansible group to use when finding ip addresses
      boss__update__hosts__ansible_group: "servers"

      boss__update__hosts__networking_interface: "{{main_network_interface}}" # {{main_network_interface}} (if vagrant)

      ### NEW vars
      # SOURCE: https://github.com/bertvv/ansible-role-hosts/blob/master/defaults/main.yml

      boss__update__hosts__hosts_playbook_version: "1.0.1"

      # If set to true, an entry for `ansible_hostname`, bound to the host's default IPv4 address is added added.
      boss__update__hosts__hosts_add_default_ipv4: true

      # If set to true, basic IPv6 entries (localhost6, ip6-localnet, etc) are added.
      boss__update__hosts__hosts_add_basic_ipv6: true

      # If set to true, an entry for every host managed by Ansible is added. Remark that this makes `boss__update__hosts__hosts_add_default_ipv4` unnecessary, as it will be added as wel by this setting.
      boss__update__hosts__hosts_add_ansible_managed_hosts: true

      # Select specific groups of Ansible managed hosts to be added in the hosts file.
      boss__update__hosts__hosts_add_ansible_managed_hosts_groups: ['servers']

      # Custom hosts entries to be added
      boss__update__hosts__hosts_entries: []

      # Custom host file snippets to be added
      boss__update__hosts__hosts_file_snippets: []

      # IP protocol to use
      boss__update__hosts__hosts_ip_protocol: 'ipv4'

      # Network interface to use
      boss__update__hosts__hosts_network_interface: "{{ boss__update__hosts__networking_interface }}"

      # convenience variable that has ansible_ as part of name for dynamic loading
      boss__update__hosts__hosts_ansible_network_interface: "ansible_{{ boss__update__hosts__networking_interface }}"

      # Backup of previous host
      boss__update__hosts__host_file_backup: yes

      # Use old 'override' style or new 'smart' style
      boss__update__hosts__default_task: "smart"
      tags:
        - boss-ansible-role-update-hosts

vagrant_playbook_role_bootstrap_data: |
    - role: boss-ansible-role-bootstrap
      # Disable raw commands to avoid sudo issues.
      boss__bootstrap_raw: False
      # Don't set domain on Travis.
      boss__bootstrap_domain: ''
      # Try bootstrapping a different IP address to avoid idempotency loop.
      # boss__bootstrap_ipv4: '127.0.1.2'
      # boss__hosts_file: /etc/hosts.molecule
      boss__bootstrap_admin_default_users:
        - name: bossjones
      boss__bootstrap_admin_groups: [ 'admins', 'staff', 'adm', 'sudo', 'bossjones' ]
      boss__bootstrap_admin_system: False
      tags:
        - boss-ansible-role-bootstrap

vagrant_playbook_role_fact_data: |
    - role: boss-ansible-role-fact
      tags:
        - boss-ansible-role-fact

vagrant_playbook_role_environment_data: |
    - role: boss-ansible-role-environment
      tags:
        - boss-ansible-role-environment

vagrant_playbook_role_etckeeper_data: |
    - role: boss-ansible-role-etckeeper
      tags:
        - boss-ansible-role-etckeeper

    # # - role: geerlingguy.pip
vagrant_playbook_role_timezone_data: |
    - role: boss-ansible-role-timezone
      timezone: 'UTC'
      timezone_update_hardware_clock: False
      tags:
        - boss-ansible-role-timezone

vagrant_playbook_role_ntp_data: |
    - role: boss-ansible-role-ntp
      task: install
      bossjones__user: vagrant
      bossjones__group: vagrant
      timezone: 'UTC'
      timezone_update_hardware_clock: False
      # defaults file for ansible-ntp
      # Defines if host is ntp_master
      # set ntp_master to true on specific group_vars/group
      ntp_master: False

      # Define your ntp_master_servers
      ntp_master_servers:
        - 0.ubuntu.pool.ntp.org
        - 1.ubuntu.pool.ntp.org
        - 2.ubuntu.pool.ntp.org
        - 3.ubuntu.pool.ntp.org
      tags:
        - boss-ansible-role-ntp

vagrant_playbook_role_tools: |
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: True
      tags:
        - boss-ansible-role-tools

vagrant_playbook_role_journald: |
      bossjones__user: vagrant
      bossjones__group: vagrant
      tags:
        - boss-ansible-role-journald

vagrant_playbook_role_nfs_master_data: |
    - role: boss-ansible-role-nfs
      boss__nfs__sysctl_fileno: 30000
      boss__nfs__systemd_limit_mem_lock: infinity
      boss__nfs__systemd_limit_no_file: 30000
      boss__nfs__systemd_limit_nproc: infinity
      boss__nfs__systemd_limit_core: infinity
      boss__nfs__etc_default_nfs_rpcnfsdcount: 64
      boss__nfs__nfs_server_group: nfs_masters
      boss__nfs__nfs_client_group: nfs_clients
      task: master
      boss__nfs__nfs_interface: '{{main_network_interface}}'
      # FIXME:(3/2/2019) THIS IS NORMALLY ENABLED, USING OVERRIDE FOR NOW # boss__nfs__master_node_ip: "{{ hostvars[groups[boss__nfs__nfs_server_group][0]]['ansible_' + boss__nfs__nfs_interface].ipv4.address }}"
      boss__nfs__master_node_ip: "{{nfs_server_ip_override}}"
      tags:
        - boss-ansible-role-nfs

vagrant_playbook_role_nfs_client_data: |
    - role: boss-ansible-role-nfs
      boss__nfs__sysctl_fileno: 30000
      boss__nfs__systemd_limit_mem_lock: infinity
      boss__nfs__systemd_limit_no_file: 30000
      boss__nfs__systemd_limit_nproc: infinity
      boss__nfs__systemd_limit_core: infinity
      boss__nfs__etc_default_nfs_rpcnfsdcount: 64
      boss__nfs__nfs_server_group: nfs_masters
      boss__nfs__nfs_client_group: nfs_clients
      task: client
      boss__nfs__nfs_interface: '{{main_network_interface}}'
      # FIXME:(3/2/2019) THIS IS NORMALLY ENABLED, USING OVERRIDE FOR NOW # boss__nfs__master_node_ip: "{{ hostvars[groups[boss__nfs__nfs_server_group][0]]['ansible_' + boss__nfs__nfs_interface].ipv4.address }}"
      boss__nfs__master_node_ip: "{{nfs_server_ip_override}}"
      # boss__nfs__nfs_interface: 'enp0s3'
      tags:
        - boss-ansible-role-nfs

# vagrant_playbook_role_netdata: |
#     - role: boss-ansible-role-netdata
#       ##############################################################
#       # SOURCE: https://blog.codybunch.com/2018/03/26/Metrics-Part-2-InfluxDB/
#       ##############################################################
#       # netdata_configure_archive: true
#       # netdata_archive_enabled: 'yes'
#       # netdata_archive_type: 'opentsdb'
#       # netdata_archive_destination: ":4242"
#       # netdata_archive_prefix: 'netdata'
#       # netdata_archive_data_source: 'average'
#       # netdata_archive_update: 1
#       # netdata_archive_buffer_on_failures: 30
#       # netdata_archive_timeout: 20000
#       # netdata_archive_send_names: true
#       ##############################################################
#       boss__netdata__netdata_proxy_enabled: False
#       boss__netdata__netdata_nginx_enabled: False
#       # graphite_secret_key: testtest123
#       # version: 1.1.3
#       # graphite_install_version: "{{ version }}"
#       # graphite_cache_graphite_url: 'http://127.0.0.1:8080'
#       # bossjones__user: vagrant
#       # bossjones__group: vagrant
#       boss__netdata__netdata_interface: '{{main_network_interface}}'

#       netdata_registry_enabled: True
#       netdata_registry_to_announce: "http://{{ netdata_stream_master_node }}:{{ netdata_default_port }}"
#       pri_domain_name: scarlett-office.local
#       netdata_stream_enabled: True
#       # You can generate API keys, with the linux command: uuidgen
#       netdata_stream_api_key: 6E6C3CA0-7B78-4115-AE2C-9681032F71D1
#       netdata_stream_master_node: "{{boss__netdata__netdata_master_ip}}"
#       nginx_listen_port: 8080
#       tags:
#         - boss-ansible-role-netdata

vagrant_playbook_role_logrotate: |
  # defaults file for ansible-logrotate
  # Defines if you want your log files compressed
  logrotate_compress: false

  # packages drop log rotation information into this directory
  logrotate_conf_dir: '/etc/logrotate.d/'

  logrotate_config: true

  logrotate_configs:
    - name: 'netdata'
      compress: true
      copytruncate: false
      create:
        mode: '640'
        owner: 'netdata'
        group: 'netdata'
      delaycompress: true
      logs:
        - '/var/log/netdata/*.log'
      # If the log file is missing, go on to the next one without
      # issuing an error message.
      missingok: true
      # Do not rotate the log if it is empty
      notifempty: true
      postrotate:
        - '/bin/kill -HUP `pidof netdata 2>/dev/null` 2>/dev/null || true'
      rotate: '14'
      rotation: 'daily'
      sharedscripts: true



  # create new (empty) log files after rotating old ones
  logrotate_create_new: true

  logrotate_default_backlogs_rotate: '4'

  logroate_default_configs:
    - 'apt'
    - 'dpkg'
    - 'rsyslog'
    - 'ufw'

  # Defines the default rotate schedule
  # hourly | daily | weekly | monthly
  logrotate_default_rotate: 'daily'

  # Defines if logrotate configs defined in logroate_default_configs
  # should be removed or not
  logrotate_remove_default_configs: false

  tags:
    - ansible-logrotate

# ---
# local_release_dir: /tmp/releases

# # Used to only evaluate vars from download role
# skip_downloads: false

# # if this is set to true will only download files once. Doesn't work
# # on Container Linux by CoreOS unless the download_localhost is true and localhost
# # is running another OS type. Default compress level is 1 (fastest).
# download_run_once: False
# download_compress: 1

# # if this is set to true, uses the localhost for download_run_once mode
# # (requires docker and sudo to access docker). You may want this option for
# # local caching of docker images or for Container Linux by CoreOS cluster nodes.
# # Otherwise, uses the first node in the kube-master group to store images
# # in the download_run_once mode.
# download_localhost: False

# # Always pull images if set to True. Otherwise check by the repo's tag/digest.
# download_always_pull: False

# # Use the first kube-master if download_localhost is not set
# download_delegate: "{% if download_localhost %}localhost{% else %}{{groups['kube-master'][0]}}{% endif %}"

# # Versions
# kube_version: v1.9.5
# kubeadm_version: "{{ kube_version }}"
# etcd_version: v3.2.4
# # TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults
# # after migration to container download
# calico_version: "v2.6.8"
# calico_ctl_version: "v1.6.3"
# calico_cni_version: "v1.11.4"
# calico_policy_version: "v1.0.3"
# calico_rr_version: "v0.4.2"
# flannel_version: "v0.10.0"
# flannel_cni_version: "v0.3.0"
# istio_version: "0.2.6"
# vault_version: 0.8.1
# weave_version: 2.2.1
# pod_infra_version: 3.0
# contiv_version: 1.1.7
# cilium_version: "v1.0.0-rc8"

# # Download URLs
# istioctl_download_url: "https://storage.googleapis.com/istio-release/releases/{{ istio_version }}/istioctl/istioctl-linux"
# kubeadm_download_url: "https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/amd64/kubeadm"
# vault_download_url: "https://releases.hashicorp.com/vault/{{ vault_version }}/vault_{{ vault_version }}_linux_amd64.zip"

# # Checksums
# istioctl_checksum: fd703063c540b8c0ab943f478c05ab257d88ae27224c746a27d0526ddbf7c370
# kubeadm_checksum: 12b6e9ac1624852b7c978bde70b9bde9ca0e4fc6581d09bddfb117bb41f93c74
# vault_binary_checksum: 3c4d70ba71619a43229e65c67830e30e050eab7a81ac6b28325ff707e5914188

# # Containers
# etcd_image_repo: "quay.io/coreos/etcd"
# etcd_image_tag: "{{ etcd_version }}"
# flannel_image_repo: "quay.io/coreos/flannel"
# flannel_image_tag: "{{ flannel_version }}"
# flannel_cni_image_repo: "quay.io/coreos/flannel-cni"
# flannel_cni_image_tag: "{{ flannel_cni_version }}"
# calicoctl_image_repo: "quay.io/calico/ctl"
# calicoctl_image_tag: "{{ calico_ctl_version }}"
# calico_node_image_repo: "quay.io/calico/node"
# calico_node_image_tag: "{{ calico_version }}"
# calico_cni_image_repo: "quay.io/calico/cni"
# calico_cni_image_tag: "{{ calico_cni_version }}"
# calico_policy_image_repo: "quay.io/calico/kube-controllers"
# calico_policy_image_tag: "{{ calico_policy_version }}"
# calico_rr_image_repo: "quay.io/calico/routereflector"
# calico_rr_image_tag: "{{ calico_rr_version }}"
# istio_proxy_image_repo: docker.io/istio/proxy
# istio_proxy_image_tag: "{{ istio_version }}"
# istio_proxy_init_image_repo: docker.io/istio/proxy_init
# istio_proxy_init_image_tag: "{{ istio_version }}"
# istio_ca_image_repo: docker.io/istio/istio-ca
# istio_ca_image_tag: "{{ istio_version }}"
# istio_mixer_image_repo: docker.io/istio/mixer
# istio_mixer_image_tag: "{{ istio_version }}"
# istio_pilot_image_repo: docker.io/istio/pilot
# istio_pilot_image_tag: "{{ istio_version }}"
# istio_proxy_debug_image_repo: docker.io/istio/proxy_debug
# istio_proxy_debug_image_tag: "{{ istio_version }}"
# istio_sidecar_initializer_image_repo: docker.io/istio/sidecar_initializer
# istio_sidecar_initializer_image_tag: "{{ istio_version }}"
# istio_statsd_image_repo: prom/statsd-exporter
# istio_statsd_image_tag: latest
# hyperkube_image_repo: "gcr.io/google-containers/hyperkube"
# hyperkube_image_tag: "{{ kube_version }}"
# pod_infra_image_repo: "gcr.io/google_containers/pause-amd64"
# pod_infra_image_tag: "{{ pod_infra_version }}"
# install_socat_image_repo: "xueshanf/install-socat"
# install_socat_image_tag: "latest"
# netcheck_version: "v1.0"
# netcheck_agent_img_repo: "quay.io/l23network/k8s-netchecker-agent"
# netcheck_agent_tag: "{{ netcheck_version }}"
# netcheck_server_img_repo: "quay.io/l23network/k8s-netchecker-server"
# netcheck_server_tag: "{{ netcheck_version }}"
# weave_kube_image_repo: "weaveworks/weave-kube"
# weave_kube_image_tag: "{{ weave_version }}"
# weave_npc_image_repo: "weaveworks/weave-npc"
# weave_npc_image_tag: "{{ weave_version }}"
# contiv_image_repo: "contiv/netplugin"
# contiv_image_tag: "{{ contiv_version }}"
# contiv_auth_proxy_image_repo: "contiv/auth_proxy"
# contiv_auth_proxy_image_tag: "{{ contiv_version }}"
# cilium_image_repo: "docker.io/cilium/cilium"
# cilium_image_tag: "{{ cilium_version }}"
# nginx_image_repo: nginx
# nginx_image_tag: 1.13
# dnsmasq_version: 2.78
# dnsmasq_image_repo: "andyshinn/dnsmasq"
# dnsmasq_image_tag: "{{ dnsmasq_version }}"
# kubedns_version: 1.14.8
# kubedns_image_repo: "gcr.io/google_containers/k8s-dns-kube-dns-amd64"
# kubedns_image_tag: "{{ kubedns_version }}"
# coredns_version: 1.1.0
# coredns_image_repo: "docker.io/coredns/coredns"
# coredns_image_tag: "{{ coredns_version }}"
# dnsmasq_nanny_image_repo: "gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64"
# dnsmasq_nanny_image_tag: "{{ kubedns_version }}"
# dnsmasq_sidecar_image_repo: "gcr.io/google_containers/k8s-dns-sidecar-amd64"
# dnsmasq_sidecar_image_tag: "{{ kubedns_version }}"
# dnsmasqautoscaler_version: 1.1.2
# dnsmasqautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# dnsmasqautoscaler_image_tag: "{{ dnsmasqautoscaler_version }}"
# kubednsautoscaler_version: 1.1.2
# kubednsautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# kubednsautoscaler_image_tag: "{{ kubednsautoscaler_version }}"
# test_image_repo: busybox
# test_image_tag: latest
# elasticsearch_version: "v2.4.1"
# elasticsearch_image_repo: "gcr.io/google_containers/elasticsearch"
# elasticsearch_image_tag: "{{ elasticsearch_version }}"
# fluentd_version: "1.22"
# fluentd_image_repo: "gcr.io/google_containers/fluentd-elasticsearch"
# fluentd_image_tag: "{{ fluentd_version }}"
# kibana_version: "v4.6.1"
# kibana_image_repo: "gcr.io/google_containers/kibana"
# kibana_image_tag: "{{ kibana_version }}"
# helm_version: "v2.8.1"
# helm_image_repo: "lachlanevenson/k8s-helm"
# helm_image_tag: "{{ helm_version }}"
# tiller_image_repo: "gcr.io/kubernetes-helm/tiller"
# tiller_image_tag: "{{ helm_version }}"
# vault_image_repo: "vault"
# vault_image_tag: "{{ vault_version }}"
# registry_image_repo: "registry"
# registry_image_tag: "2.6"
# registry_proxy_image_repo: "gcr.io/google_containers/kube-registry-proxy"
# registry_proxy_image_tag: "0.4"
# local_volume_provisioner_image_repo: "quay.io/external_storage/local-volume-provisioner"
# local_volume_provisioner_image_tag: "v2.0.0"
# cephfs_provisioner_image_repo: "quay.io/kubespray/cephfs-provisioner"
# cephfs_provisioner_image_tag: "92295a30"
# ingress_nginx_controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
# ingress_nginx_controller_image_tag: "0.12.0"
# ingress_nginx_default_backend_image_repo: "gcr.io/google_containers/defaultbackend"
# ingress_nginx_default_backend_image_tag: "1.4"
# cert_manager_version: "v0.2.3"
# cert_manager_controller_image_repo: "quay.io/jetstack/cert-manager-controller"
# cert_manager_controller_image_tag: "{{ cert_manager_version }}"
# cert_manager_ingress_shim_image_repo: "quay.io/jetstack/cert-manager-ingress-shim"
# cert_manager_ingress_shim_image_tag: "{{ cert_manager_version }}"

# downloads:
#   netcheck_server:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_server_img_repo }}"
#     tag: "{{ netcheck_server_tag }}"
#     sha256: "{{ netcheck_server_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   netcheck_agent:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_agent_img_repo }}"
#     tag: "{{ netcheck_agent_tag }}"
#     sha256: "{{ netcheck_agent_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   etcd:
#     enabled: true
#     container: true
#     repo: "{{ etcd_image_repo }}"
#     tag: "{{ etcd_image_tag }}"
#     sha256: "{{ etcd_digest_checksum|default(None) }}"
#     groups:
#       - etcd
#   kubeadm:
#     enabled: "{{ kubeadm_enabled }}"
#     file: true
#     version: "{{ kubeadm_version }}"
#     dest: "kubeadm"
#     sha256: "{{ kubeadm_checksum }}"
#     source_url: "{{ kubeadm_download_url }}"
#     url: "{{ kubeadm_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - k8s-cluster
#   istioctl:
#     enabled: "{{ istio_enabled }}"
#     file: true
#     version: "{{ istio_version }}"
#     dest: "istio/istioctl"
#     sha256: "{{ istioctl_checksum }}"
#     source_url: "{{ istioctl_download_url }}"
#     url: "{{ istioctl_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - kube-master
#   istio_proxy:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_image_repo }}"
#     tag: "{{ istio_proxy_image_tag }}"
#     sha256: "{{ istio_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_init:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_init_image_repo }}"
#     tag: "{{ istio_proxy_init_image_tag }}"
#     sha256: "{{ istio_proxy_init_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_ca:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_ca_image_repo }}"
#     tag: "{{ istio_ca_image_tag }}"
#     sha256: "{{ istio_ca_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_mixer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_mixer_image_repo }}"
#     tag: "{{ istio_mixer_image_tag }}"
#     sha256: "{{ istio_mixer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_pilot:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_pilot_image_repo }}"
#     tag: "{{ istio_pilot_image_tag }}"
#     sha256: "{{ istio_pilot_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_debug:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_debug_image_repo }}"
#     tag: "{{ istio_proxy_debug_image_tag }}"
#     sha256: "{{ istio_proxy_debug_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_sidecar_initializer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_sidecar_initializer_image_repo }}"
#     tag: "{{ istio_sidecar_initializer_image_tag }}"
#     sha256: "{{ istio_sidecar_initializer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_statsd:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_statsd_image_repo }}"
#     tag: "{{ istio_statsd_image_tag }}"
#     sha256: "{{ istio_statsd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   hyperkube:
#     enabled: true
#     container: true
#     repo: "{{ hyperkube_image_repo }}"
#     tag: "{{ hyperkube_image_tag }}"
#     sha256: "{{ hyperkube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   cilium:
#     enabled: "{{ kube_network_plugin == 'cilium' }}"
#     container: true
#     repo: "{{ cilium_image_repo }}"
#     tag: "{{ cilium_image_tag }}"
#     sha256: "{{ cilium_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel:
#     enabled: "{{ kube_network_plugin == 'flannel' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ flannel_image_repo }}"
#     tag: "{{ flannel_image_tag }}"
#     sha256: "{{ flannel_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel_cni:
#     enabled: "{{ kube_network_plugin == 'flannel' }}"
#     container: true
#     repo: "{{ flannel_cni_image_repo }}"
#     tag: "{{ flannel_cni_image_tag }}"
#     sha256: "{{ flannel_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calicoctl:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calicoctl_image_repo }}"
#     tag: "{{ calicoctl_image_tag }}"
#     sha256: "{{ calicoctl_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_node:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_node_image_repo }}"
#     tag: "{{ calico_node_image_tag }}"
#     sha256: "{{ calico_node_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_cni:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_cni_image_repo }}"
#     tag: "{{ calico_cni_image_tag }}"
#     sha256: "{{ calico_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_policy:
#     enabled: "{{ enable_network_policy or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_policy_image_repo }}"
#     tag: "{{ calico_policy_image_tag }}"
#     sha256: "{{ calico_policy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_rr:
#     enabled: "{{ peer_with_calico_rr is defined and peer_with_calico_rr and kube_network_plugin == 'calico' }}"
#     container: true
#     repo: "{{ calico_rr_image_repo }}"
#     tag: "{{ calico_rr_image_tag }}"
#     sha256: "{{ calico_rr_digest_checksum|default(None) }}"
#     groups:
#       - calico-rr
#   weave_kube:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_kube_image_repo }}"
#     tag: "{{ weave_kube_image_tag }}"
#     sha256: "{{ weave_kube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   weave_npc:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_npc_image_repo }}"
#     tag: "{{ weave_npc_image_tag }}"
#     sha256: "{{ weave_npc_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_image_repo }}"
#     tag: "{{ contiv_image_tag }}"
#     sha256: "{{ contiv_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv_auth_proxy:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_auth_proxy_image_repo }}"
#     tag: "{{ contiv_auth_proxy_image_tag }}"
#     sha256: "{{ contiv_auth_proxy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   pod_infra:
#     enabled: true
#     container: true
#     repo: "{{ pod_infra_image_repo }}"
#     tag: "{{ pod_infra_image_tag }}"
#     sha256: "{{ pod_infra_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   install_socat:
#     enabled: "{{ ansible_os_family in ['CoreOS', 'Container Linux by CoreOS'] }}"
#     container: true
#     repo: "{{ install_socat_image_repo }}"
#     tag: "{{ install_socat_image_tag }}"
#     sha256: "{{ install_socat_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   nginx:
#     enabled: "{{ loadbalancer_apiserver_localhost }}"
#     container: true
#     repo: "{{ nginx_image_repo }}"
#     tag: "{{ nginx_image_tag }}"
#     sha256: "{{ nginx_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq:
#     enabled: "{{ dns_mode == 'dnsmasq_kubedns' }}"
#     container: true
#     repo: "{{ dnsmasq_image_repo }}"
#     tag: "{{ dnsmasq_image_tag }}"
#     sha256: "{{ dnsmasq_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubedns:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubedns_image_repo }}"
#     tag: "{{ kubedns_image_tag }}"
#     sha256: "{{ kubedns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   coredns:
#     enabled: "{{ dns_mode in ['coredns', 'coredns_dual'] }}"
#     container: true
#     repo: "{{ coredns_image_repo }}"
#     tag: "{{ coredns_image_tag }}"
#     sha256: "{{ coredns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_nanny:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_nanny_image_repo }}"
#     tag: "{{ dnsmasq_nanny_image_tag }}"
#     sha256: "{{ dnsmasq_nanny_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_sidecar:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_sidecar_image_repo }}"
#     tag: "{{ dnsmasq_sidecar_image_tag }}"
#     sha256: "{{ dnsmasq_sidecar_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubednsautoscaler:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubednsautoscaler_image_repo }}"
#     tag: "{{ kubednsautoscaler_image_tag }}"
#     sha256: "{{ kubednsautoscaler_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   testbox:
#     enabled: false
#     container: true
#     repo: "{{ test_image_repo }}"
#     tag: "{{ test_image_tag }}"
#     sha256: "{{ testbox_digest_checksum|default(None) }}"
#   elasticsearch:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ elasticsearch_image_repo }}"
#     tag: "{{ elasticsearch_image_tag }}"
#     sha256: "{{ elasticsearch_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   fluentd:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ fluentd_image_repo }}"
#     tag: "{{ fluentd_image_tag }}"
#     sha256: "{{ fluentd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kibana:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ kibana_image_repo }}"
#     tag: "{{ kibana_image_tag }}"
#     sha256: "{{ kibana_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   helm:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ helm_image_repo }}"
#     tag: "{{ helm_image_tag }}"
#     sha256: "{{ helm_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   tiller:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ tiller_image_repo }}"
#     tag: "{{ tiller_image_tag }}"
#     sha256: "{{ tiller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   vault:
#     enabled: "{{ cert_management == 'vault' }}"
#     container: "{{ vault_deployment_type != 'host' }}"
#     file: "{{ vault_deployment_type == 'host' }}"
#     dest: "vault/vault_{{ vault_version }}_linux_amd64.zip"
#     mode: "0755"
#     owner: "vault"
#     repo: "{{ vault_image_repo }}"
#     sha256: "{{ vault_binary_checksum if vault_deployment_type == 'host' else vault_digest_checksum|d(none) }}"
#     source_url: "{{ vault_download_url }}"
#     tag: "{{ vault_image_tag }}"
#     unarchive: true
#     url: "{{ vault_download_url }}"
#     version: "{{ vault_version }}"
#     groups:
#       - vault
#   registry:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_image_repo }}"
#     tag: "{{ registry_image_tag }}"
#     sha256: "{{ registry_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   registry_proxy:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_proxy_image_repo }}"
#     tag: "{{ registry_proxy_image_tag }}"
#     sha256: "{{ registry_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   local_volume_provisioner:
#     enabled: "{{ local_volume_provisioner_enabled }}"
#     container: true
#     repo: "{{ local_volume_provisioner_image_repo }}"
#     tag: "{{ local_volume_provisioner_image_tag }}"
#     sha256: "{{ local_volume_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cephfs_provisioner:
#     enabled: "{{ cephfs_provisioner_enabled }}"
#     container: true
#     repo: "{{ cephfs_provisioner_image_repo }}"
#     tag: "{{ cephfs_provisioner_image_tag }}"
#     sha256: "{{ cephfs_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   ingress_nginx_controller:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_controller_image_repo }}"
#     tag: "{{ ingress_nginx_controller_image_tag }}"
#     sha256: "{{ ingress_nginx_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   ingress_nginx_default_backend:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_default_backend_image_repo }}"
#     tag: "{{ ingress_nginx_default_backend_image_tag }}"
#     sha256: "{{ ingress_nginx_default_backend_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   cert_manager_controller:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_controller_image_repo }}"
#     tag: "{{ cert_manager_controller_image_tag }}"
#     sha256: "{{ cert_manager_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cert_manager_ingress_shim:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_ingress_shim_image_repo }}"
#     tag: "{{ cert_manager_ingress_shim_image_tag }}"
#     sha256: "{{ cert_manager_ingress_shim_digest_checksum|default(None) }}"
#     groups:
#       - kube-node

# download_defaults:
#   container: false
#   file: false
#   repo: None
#   tag: None
#   enabled: false
#   dest: None
#   version: None
#   url: None
#   unarchive: false
#   owner: kube
#   mode: None



# ---
# elasticsearch_cpu_limit: 1000m
# elasticsearch_mem_limit: 0M
# elasticsearch_cpu_requests: 100m
# elasticsearch_mem_requests: 0M
# elasticsearch_service_port: 9200
