# domain_root: hyenaclan.org
# domain_root: borglab.duckdns.org
domain_root: borglab.scarlettlab.home

base_dir_path: ~/dev/bossjones/bosslab-playbooks
dist_dir_path: "{{base_dir_path}}/dist"
manifest_dir_name: manifests/borg-manifests
manifest_dir_path: "{{dist_dir_path}}/{{manifest_dir_name}}"
path_to_network_disk: "/mnt/publicdata"
main_network_interface: 'ens192'
nfs_server_group: nfs_masters
nfs_client_group: nfs_clients
nfs_server_ip_override: 192.168.1.174
k8_admin_config_dir: ~/dev/bossjones/bosslab-playbooks
k8_admin_config_path: "{{k8_admin_config_dir}}/borg-admin.conf"
queen_host: "borg-queen-01"

#      .########..######..##.....##..#######...######..########.########..##.....##.########.########.
#      .##.......##....##.##.....##.##.....##.##....##.##.......##.....##.##.....##.##.......##.....##
#      .##.......##.......##.....##.##.....##.##.......##.......##.....##.##.....##.##.......##.....##
#      .######...##.......#########.##.....##..######..######...########..##.....##.######...########.
#      .##.......##.......##.....##.##.....##.......##.##.......##...##....##...##..##.......##...##..
#      .##.......##....##.##.....##.##.....##.##....##.##.......##....##....##.##...##.......##....##.
#      .########..######..##.....##..#######...######..########.##.....##....###....########.##.....##


boss__echoserver__echoserver_subdomain: echoserver
boss__echoserver__manifest_path: "{{manifest_dir_path}}/echoserver"
boss__echoserver__namespace_name: echoserver
boss__echoserver__deployment_name: echoserver
boss__echoserver__echoserver_version: 2.1
boss__echoserver__echoserver_image_repo: "gcr.io/kubernetes-e2e-test-images/echoserver"
boss__echoserver__echoserver_image_tag: "{{ boss__echoserver__echoserver_version }}"
boss__echoserver__echoserver_cpu_limit: 100m
boss__echoserver__echoserver_mem_limit: 55Mi
boss__echoserver__echoserver_cpu_requests: 100m
boss__echoserver__echoserver_mem_requests: 20Mi
boss__echoserver__deployment_annotations_list:
  - name: nginx.ingress.kubernetes.io/ssl-redirect
    val: \"false\"
boss__echoserver__deployment_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
boss__echoserver__deployment_labels: |
  run: nginx
boss__echoserver__deployment_spec_replicas: 2
# boss__echoserver__deployment_spec_template_metadata_labels: "{{boss__echoserver__deployment_labels}}"
boss__echoserver__ingress_labels: |
  run: nginx
boss__echoserver__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__echoserver__service_labels: |
  run: nginx
boss__echoserver__service__spec_selector: |
  run: nginx
# boss__echoserver__echoserver_service_port: 9200


# # Kubernetes dashboard
# # RBAC required. see docs/getting-started.md for access details.
# dashboard_enabled: true

# # Addons which can be enabled
# efk_enabled: false
# helm_enabled: false
# istio_enabled: false
# registry_enabled: false
# enable_network_policy: false
# local_volume_provisioner_enabled: "{{ local_volumes_enabled | default('false') }}"
# persistent_volumes_enabled: false
# cephfs_provisioner_enabled: false
# ingress_nginx_enabled: false
# cert_manager_enabled: false

##########################################################################################################
#                   ..######.....###....##.......####..######...#######.
#                   .##....##...##.##...##........##..##....##.##.....##
#                   .##........##...##..##........##..##.......##.....##
#                   .##.......##.....##.##........##..##.......##.....##
#                   .##.......#########.##........##..##.......##.....##
#                   .##....##.##.....##.##........##..##....##.##.....##
#                   ..######..##.....##.########.####..######...#######.
##########################################################################################################

boss__calico__manifest_path: "{{manifest_dir_path}}/calico"
boss__calico__namespace_name: kube-system
boss__calico__prometheus_metrics_enabled: false
# NOTE: See https://docs.projectcalico.org/v3.5/usage/configuration/mtu for recomendations
boss__calico__veth_mtu: 1480 # 1480,
boss__calico__node_env_CALICO_IPV4POOL_IPIP: "Always"
boss__calico__debugger_deployment_spec_replicas: 1
boss__calico__debugger_enabled: "enabled"

boss__calico__node_version: v3.3.2
boss__calico__node_image_repo: "quay.io/calico/node"
boss__calico__node_image_tag: "{{ boss__calico__node_version }}"


boss__calico__cni_version: v3.3.2
boss__calico__cni_image_repo: "quay.io/calico/cni"
boss__calico__cni_image_tag: "{{ boss__calico__node_version }}"

boss__calico__enable_prometheus_exporter_servicemonitor: "enabled"

boss__calico__prometheus_exporter_namespace_name: monitoring

boss__calico__enable_prometheus_felix: "false" # false,enabled
boss__calico__felix_PrometheusMetricsPort: 9091
boss__calico__typha_PrometheusMetricsPort: 9093


boss__calico__node_daemonset_annotations: |
  scheduler.alpha.kubernetes.io/critical-pod: ''
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__felix_PrometheusMetricsPort}}"

boss__calico__enable_typha: "false" # false,enabled
boss__calico__enable_prometheus_typha: "false" # false,enabled
boss__calico__typha_service_name: "none"  # or none,calico-typha

boss__calico__typha_deployment_spec_replicas: 0 # or 0,3

boss__calico__typha_service_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"

boss__calico__typha_deployment_annotations: |
  # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
  # add-on, ensuring it gets priority scheduling and that its resources are reserved
  # if it ever gets evicted.
  scheduler.alpha.kubernetes.io/critical-pod: ''
  cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"



# {{ calico_node_image_repo }}:{{ calico_node_image_tag }}

##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__manifest_path: "{{manifest_dir_path}}/dashboard"
boss__dashboard__namespace_name: kube-system

boss__dashboard__version: v1.10.0
boss__dashboard__image_repo: "k8s.gcr.io/kubernetes-dashboard-amd64"
boss__dashboard__image_tag: "{{ boss__dashboard__version }}"

boss__dashboard__deployment_container_args: |
  - --auto-generate-certificates
  # Uncomment the following line to manually specify Kubernetes API server Host
  # If not specified, Dashboard will attempt to auto discover the API server and connect
  # to it. Uncomment only if the default does not work.
  # - --apiserver-host=http://my-address:port


##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__admin__manifest_path: "{{manifest_dir_path}}/dashboard-admin"
boss__dashboard__admin__namespace_name: kube-system


##########################################################################################################
#   .########.########.##....##
#   .##.......##.......##...##.
#   .##.......##.......##..##..
#   .######...######...#####...
#   .##.......##.......##..##..
#   .##.......##.......##...##.
#   .########.##.......##....##
##########################################################################################################
boss__efk__elasticsearch_subdomain: elasticsearch
boss__efk__manifest_path: "{{manifest_dir_path}}/efk"
boss__efk__namespace_name: kube-system
boss__efk__deployment_name: efk


##########################################################################################################
#    .########.##..........###.....######..########.####..######...######..########....###....########...######..##.....##
#    .##.......##.........##.##...##....##....##.....##..##....##.##....##.##.........##.##...##.....##.##....##.##.....##
#    .##.......##........##...##..##..........##.....##..##.......##.......##........##...##..##.....##.##.......##.....##
#    .######...##.......##.....##..######.....##.....##..##........######..######...##.....##.########..##.......#########
#    .##.......##.......#########.......##....##.....##..##.............##.##.......#########.##...##...##.......##.....##
#    .##.......##.......##.....##.##....##....##.....##..##....##.##....##.##.......##.....##.##....##..##....##.##.....##
#    .########.########.##.....##..######.....##....####..######...######..########.##.....##.##.....##..######..##.....##
##########################################################################################################
boss__efk__elasticsearch_ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__efk__elasticsearch_ingress_labels: |
  k8s-app: elasticsearch-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__elasticsearch_service_labels: |
  k8s-app: elasticsearch-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "Elasticsearch"
  boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__efk__elasticsearch_persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_version: v5.6.2
boss__efk__elasticsearch_image_repo: "bossjones/elasticsearch"
boss__efk__elasticsearch_image_tag: "{{ boss__efk__elasticsearch_version }}"
boss__efk__elasticsearch_cpu_limit: 1000m # same as 2 CPU
boss__efk__elasticsearch_mem_limit: 4048Mi
boss__efk__elasticsearch_cpu_requests: 250m # 0.5 cpu
boss__efk__elasticsearch_mem_requests: 2350Mi

boss__efk__elasticsearch_stateful_set_env_ES_JAVA_OPTS: "-Xms2048m -Xmx2048m"

# boss__efk__elasticsearch_stateful_set_spec_resources: False
boss__efk__elasticsearch_stateful_set_spec_resources: |
  # need more cpu upon initialization, therefore burstable class
  requests:
    cpu: 100m
    memory: 2350Mi
  limits:
    cpu: 1000m
    memory: 4048Mi

boss__efk__elasticsearch_service_account_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_cluster_role_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_cluster_role_binding_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_stateful_set_labels: |
    k8s-app: elasticsearch-logging
    version: "{{boss__efk__elasticsearch_version}}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_stateful_set_spec_replicas: 1
boss__efk__elasticsearch_stateful_set_spec_selector_match_labels: |
        k8s-app: elasticsearch-logging
        version: "{{ boss__efk__elasticsearch_version }}"
boss__efk__elasticsearch_stateful_set_spec_template_metadata_labels: |
        k8s-app: elasticsearch-logging
        version: "{{ boss__efk__elasticsearch_version }}"
        kubernetes.io/cluster-service: "true"

#########################

##########################################################################################################
#     .########..######...........######..##.....##.########.....###....########..#######..########.
#     .##.......##....##.........##....##.##.....##.##.....##...##.##......##....##.....##.##.....##
#     .##.......##...............##.......##.....##.##.....##..##...##.....##....##.....##.##.....##
#     .######....######..#######.##.......##.....##.########..##.....##....##....##.....##.########.
#     .##.............##.........##.......##.....##.##...##...#########....##....##.....##.##...##..
#     .##.......##....##.........##....##.##.....##.##....##..##.....##....##....##.....##.##....##.
#     .########..######...........######...#######..##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__efk__elasticsearch_curator_deployment_labels: |
  k8s-app: es-curator
  boss-part-of: efk

boss__efk__elasticsearch_curator_version: 5.3.0-1
boss__efk__elasticsearch_curator_image_repo: "aknudsen/es-curator-service"
boss__efk__elasticsearch_curator_image_tag: "{{ boss__efk__elasticsearch_curator_version }}"
boss__efk__elasticsearch_curator_cpu_limit: 200m # 0.2 CPU limit
boss__efk__elasticsearch_curator_mem_limit: 500Mi # 1 CPU limit
boss__efk__elasticsearch_curator_cpu_requests: 100m # 5% cpu
boss__efk__elasticsearch_curator_mem_requests: 200Mi #

#########################


##########################################################################################################
#    .########.##.......##.....##.########.##....##.########.########.
#    .##.......##.......##.....##.##.......###...##....##....##.....##
#    .##.......##.......##.....##.##.......####..##....##....##.....##
#    .######...##.......##.....##.######...##.##.##....##....##.....##
#    .##.......##.......##.....##.##.......##..####....##....##.....##
#    .##.......##.......##.....##.##.......##...###....##....##.....##
#    .##.......########..#######..########.##....##....##....########.
##########################################################################################################
boss__efk__fluentd_version: v2.3.0
boss__efk__fluentd_image_repo: "bossjones/fluentd-elasticsearch"
boss__efk__fluentd_image_tag: "{{ boss__efk__fluentd_version }}"
boss__efk__fluentd_mem_limit: 500Mi
boss__efk__fluentd_cpu_requests: 100m
boss__efk__fluentd_mem_requests: 200Mi
boss__efk__fluentd_config_map_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_service_account_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_binding_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_labels: |
    k8s-app: fluentd-es
    version: {{boss__efk__fluentd_version}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_selector_match_labels: |
      k8s-app: fluentd-es
      # version: {{boss__efk__fluentd_version}}
boss__efk__fluentd_daemon_set_spec_template_metadata_labels: |
        k8s-app: fluentd-es
        # kubernetes.io/cluster-service: "true"
        # version: {{boss__efk__fluentd_version}}
        # boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_template_metadata_annotations: |
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'


boss__efk__fluentd_daemon_set_spec_template_spec_securityContext: |
  runAsNonRoot: false
  # runAsUser: 65534
  privileged: true

boss__efk__fluentd_daemon_set_spec_template_spec_resources: |
  requests:
    cpu: 100m
    memory: 200Mi
  limits:
    memory: 500Mi
    # memory: 200Mi

boss__efk__fluentd_daemon_set_spec_template_spec_volumeMounts: |
  # - name: libsystemddir
  #   mountPath: /host/lib
  #   readOnly: true
  - name: varlog
    mountPath: /var/log
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true
  - name: config-volume
    mountPath: /etc/fluent/config.d
  - name: docker-sock
    # ORIG # mountPath: /var/run/docker.sock
    mountPath: /run/containerd/containerd.sock
  - name: varrun
    mountPath: /var/run
  - name: libsystemd
    # mountPath: /usr/lib64/libsystemd.so.0
    mountPath: /lib/x86_64-linux-gnu/libsystemd.so.0
    # mountPath: /usr/lib64/libsystemd.so.0

boss__efk__fluentd_daemon_set_spec_template_spec_volumes: |
  # It is needed to copy systemd library to decompress journals
  # - name: libsystemddir
  #   hostPath:
  #     path: /usr/lib64
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
  - name: config-volume
    configMap:
      # name: fluentd-es-config-v0.1.0
      name: fluentd-es-config-v0.1.6
  - name: docker-sock
    hostPath:
      # path: /var/run/docker/libcontainerd/docker-containerd.sock
      # path: /var/run/docker.sock
      path: /var/run/docker/containerd/containerd.sock
  - name: varrun
    hostPath:
      path: /var/run
  - name: libsystemd
    hostPath:
      path: /usr/lib64/libsystemd.so.0
      # path: /lib/x86_64-linux-gnu/libsystemd.so.0

boss__efk__fluentd_daemon_set_spec_template_spec_nodeSelector: |-
  beta.kubernetes.io/os: linux

############################################


##########################################################################################################
#   .##....##.####.########.....###....##....##....###...
#   .##...##...##..##.....##...##.##...###...##...##.##..
#   .##..##....##..##.....##..##...##..####..##..##...##.
#   .#####.....##..########..##.....##.##.##.##.##.....##
#   .##..##....##..##.....##.#########.##..####.#########
#   .##...##...##..##.....##.##.....##.##...###.##.....##
#   .##....##.####.########..##.....##.##....##.##.....##
##########################################################################################################
boss__efk__kibana_subdomain: kibana
boss__efk__kibana_version: 5.6.2
boss__efk__kibana_image_repo: "bossjones/kibana"
boss__efk__kibana_image_tag: "{{ boss__efk__kibana_version }}"
boss__efk__kibana_cpu_limit: 1000m
boss__efk__kibana_cpu_requests: 1000m

boss__efk__kibana_deployment_labels: |
  k8s-app: kibana-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__kibana_deployment_spec_replicas: 1
boss__efk__kibana_deployment_spec_selector_match_labels: |
  k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_metadata_labels: |
  k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_spec_node_selector: |
  kubernetes.io/hostname: "borg-worker-01"
boss__efk__kibana_ingress_labels: |
  k8s-app: kibana-ingress
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__kibana_ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__efk__kibana_service_labels: |
  k8s-app: kibana-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "Kibana"
  boss-part-of: efk

# boss__efk__kibana_service_annotations: False
boss__efk__kibana_service_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

# boss__efk__kibana_deployment_spec_template_spec_resources: False
boss__efk__kibana_deployment_spec_template_spec_resources: |
  # need more cpu upon initialization, therefore burstable class
  requests:
    cpu: 1000m
  limits:
    cpu: 1000m
############################################

boss__efk__elasticsearch_persistent_volume_spec_capacity_storage: 5Gi
boss__efk__elasticsearch_persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/elasticsearch"
# boss__efk__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__efk__nfs_master_node_ip: "{{ nfs_server_ip_override }}"


##########################################


##########################################################################################################
#      .########..########..######...####..######..########.########..##....##
#      .##.....##.##.......##....##...##..##....##....##....##.....##..##..##.
#      .##.....##.##.......##.........##..##..........##....##.....##...####..
#      .########..######...##...####..##...######.....##....########.....##...
#      .##...##...##.......##....##...##........##....##....##...##......##...
#      .##....##..##.......##....##...##..##....##....##....##....##.....##...
#      .##.....##.########..######...####..######.....##....##.....##....##...
##########################################################################################################
boss__registry__subdomain: registry
boss__registry__manifest_path: "{{manifest_dir_path}}/registry"
boss__registry__namespace_name: kube-system
boss__registry__deployment_name: registry
boss__registry__enable_pvc: false
boss__registry__enable_tls: true
boss__registry__tls_cert_cluster_issuer_name: selfsigning-issuer
boss__registry__ingress_tls_config: |
  - hosts:
    - {{boss__registry__subdomain}}.{{domain_root}}
    secretName: docker-registry-tls-certificate

boss__registry__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  nginx.ingress.kubernetes.io/rewrite-target: \"/\"
  traefik.frontend.rule.type: PathPrefix

boss__registry__ingress_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__service_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeRegistry"
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__version: 2.6
boss__registry__image_repo: "registry"
boss__registry__image_tag: "{{ boss__registry__version }}"
boss__registry__cpu_limit: 1000m
boss__registry__mem_limit: 500Mi
boss__registry__cpu_requests: 100m
boss__registry__mem_requests: 200Mi

boss__registry__service_account_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"
boss__registry__cluster_role_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__cluster_role_binding_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__stateful_set_labels: |
  k8s-app: registry
  version: "{{ boss__registry__version }}"
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry

boss__registry__stateful_set_spec_replicas: 1
boss__registry__stateful_set_spec_selector_match_labels: |
  k8s-app: registry
  version: "{{ boss__registry__version }}"

boss__registry__stateful_set_spec_template_metadata_labels: |
  k8s-app: registry
  version: "{{ boss__registry__version }}"
  kubernetes.io/cluster-service: "true"

boss__registry__persistent_volume_spec_capacity_storage: 10Gi
boss__registry__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__nfs_master_node_ip: "{{ nfs_server_ip_override }}"



##########################################################################################################
#         ..######..########.########..########.........##.....##....###....##....##....###.....######...########.########.
#         .##....##.##.......##.....##....##............###...###...##.##...###...##...##.##...##....##..##.......##.....##
#         .##.......##.......##.....##....##............####.####..##...##..####..##..##...##..##........##.......##.....##
#         .##.......######...########.....##....#######.##.###.##.##.....##.##.##.##.##.....##.##...####.######...########.
#         .##.......##.......##...##......##............##.....##.#########.##..####.#########.##....##..##.......##...##..
#         .##....##.##.......##....##.....##............##.....##.##.....##.##...###.##.....##.##....##..##.......##....##.
#         ..######..########.##.....##....##............##.....##.##.....##.##....##.##.....##..######...########.##.....##
##########################################################################################################

boss__certmanager__manifest_path: "{{manifest_dir_path}}/cert-manager"
boss__certmanager__namespace_name: kube-system
boss__certmanager__example_com_namespace_name: default
boss__certmanager__my_cluster_namespace_name: default
boss__certmanager__my_cluster_certificate_name: wildcard-borglab
boss__certmanager__my_cluster_commonName: "*.{{domain_root}}"
boss__certmanager__my_cluster_organization: |
  - Borglab
boss__certmanager__my_cluster_dnsNames: |
  - "*.{{domain_root}}"
boss__certmanager__my_cluster_generate_tls: "force"




##########################################################################################################
#    .########..########..######...####..######..########.########..##....##.........##.....##.####
#    .##.....##.##.......##....##...##..##....##....##....##.....##..##..##..........##.....##..##.
#    .##.....##.##.......##.........##..##..........##....##.....##...####...........##.....##..##.
#    .########..######...##...####..##...######.....##....########.....##....#######.##.....##..##.
#    .##...##...##.......##....##...##........##....##....##...##......##............##.....##..##.
#    .##....##..##.......##....##...##..##....##....##....##....##.....##............##.....##..##.
#    .##.....##.########..######...####..######.....##....##.....##....##.............#######..####
##########################################################################################################
boss__registry__ui__subdomain: registry-ui
boss__registry__ui__manifest_path: "{{manifest_dir_path}}/registry-ui"
boss__registry__ui__namespace_name: default
boss__registry__ui__deployment_name: docker-registry-ui
boss__registry__ui__enable_pvc: false
boss__registry__ui__enable_tls: "disabled"
boss__registry__ui__ingress_tls_config: |
  - hosts:
    - {{boss__registry__ui__subdomain}}.{{domain_root}}
    secretName: docker-registry-ui-tls-certificate
boss__registry__ui__tls_cert_cluster_issuer_name: selfsigning-issuer

boss__registry__ui__version: 0.6
boss__registry__ui__image_repo: "joxit/docker-registry-ui"
boss__registry__ui__image_tag: "{{ boss__registry__ui__version }}"

boss__registry__ui__cpu_limit: 1000m
boss__registry__ui__mem_limit: 500Mi
boss__registry__ui__cpu_requests: 100m
boss__registry__ui__mem_requests: 200Mi

# boss__registry__ui__ingress_annotations: |
#   nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
#   nginx.ingress.kubernetes.io/rewrite-target: \"/\"
#   traefik.frontend.rule.type: PathPrefix

boss__registry__ui__ingress_annotations: |
  # FIXME: Fix this
  # certmanager.k8s.io/issuer: "selfsigning-issuer"
  # NOTE: This is what was configured in
  nginx.ingress.kubernetes.io/proxy-body-size: "0"
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # This is what we had configured
  traefik.frontend.rule.type: PathPrefix
  # nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  # nginx.ingress.kubernetes.io/rewrite-target: \"/\"

boss__registry__ui__ingress_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_labels: |
  app: {{boss__registry__ui__deployment_name}}
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "KubeRegistry"
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix

boss__registry__ui__deployment_labels: |
  app: {{boss__registry__ui__deployment_name}}
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__deployment_spec_template_metadata_labels: |
  app: {{boss__registry__ui__deployment_name}}
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__deployment_spec_template_spec_containers_resources: |
  requests:
    cpu: 250m
    memory: 64Mi
  limits:
    # This is 0.5 of a cpu
    cpu: 500m
    memory: 128Mi

boss__registry__ui__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: {{boss__registry__ui__deployment_name}}
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__ui__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: {{boss__registry__ui__deployment_name}}
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_account_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"
boss__registry__ui__cluster_role_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__cluster_role_binding_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}

boss__registry__ui__stateful_set_spec_replicas: 1
boss__registry__ui__stateful_set_spec_selector_match_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_spec_template_metadata_labels: |
  k8s-app: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"
  kubernetes.io/cluster-service: "true"

boss__registry__ui__persistent_volume_spec_capacity_storage: 10Gi
boss__registry__ui__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry-ui"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__ui__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__ui__nfs_master_node_ip: "{{ nfs_server_ip_override }}"




##########################################################################################################
#     .......##.########.##....##.##....##.####.##....##..######.
#     .......##.##.......###...##.##...##...##..###...##.##....##
#     .......##.##.......####..##.##..##....##..####..##.##......
#     .......##.######...##.##.##.#####.....##..##.##.##..######.
#     .##....##.##.......##..####.##..##....##..##..####.......##
#     .##....##.##.......##...###.##...##...##..##...###.##....##
#     ..######..########.##....##.##....##.####.##....##..######.
##########################################################################################################
boss__jenkins__subdomain: jenkins
boss__jenkins__version: lts
boss__jenkins__image_repo: "jenkins/jenkins"
boss__jenkins__image_tag: "{{ boss__jenkins__version }}"
boss__jenkins__cpu_limit: 1000m
boss__jenkins__cpu_requests: 1000m
boss__jenkins__manifest_path: "{{manifest_dir_path}}/jenkins-k8"
boss__jenkins__namespace_name: kube-system

boss__jenkins__deployment_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__deployment_spec_replicas: 1
boss__jenkins__deployment_spec_selector_match_labels: |
      k8s-app: jenkins
boss__jenkins__deployment_spec_template_metadata_labels: |
        k8s-app: jenkins
boss__jenkins__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "borg-worker-01"
boss__jenkins__ingress_labels: |
    k8s-app: jenkins-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__jenkins__service_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "jenkins"
    boss-part-of: jenkins

boss__jenkins__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__jenkins__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_spec_capacity_storage: 2Gi
boss__jenkins__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/jenkins"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__jenkins__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__jenkins__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#              .##.....##.########....###....########...######..########.########.########.
#              .##.....##.##.........##.##...##.....##.##....##....##....##.......##.....##
#              .##.....##.##........##...##..##.....##.##..........##....##.......##.....##
#              .#########.######...##.....##.########...######.....##....######...########.
#              .##.....##.##.......#########.##..............##....##....##.......##...##..
#              .##.....##.##.......##.....##.##........##....##....##....##.......##....##.
#              .##.....##.########.##.....##.##.........######.....##....########.##.....##
##########################################################################################################
boss__heapster__subdomain: heapster
boss__heapster__version: lts
boss__heapster__image_repo: "heapster/heapster"
boss__heapster__image_tag: "{{ boss__heapster__version }}"
boss__heapster__cpu_limit: 1000m
boss__heapster__cpu_requests: 1000m
boss__heapster__manifest_path: "{{manifest_dir_path}}/heapster2"
boss__heapster__namespace_name: kube-system

boss__heapster__deployment_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__deployment_spec_replicas: 1
boss__heapster__deployment_spec_selector_match_labels: |
      k8s-app: heapster
boss__heapster__deployment_spec_template_metadata_labels: |
        k8s-app: heapster
boss__heapster__ingress_labels: |
    k8s-app: heapster-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__heapster__service_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "heapster"
    boss-part-of: heapster

boss__heapster__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__heapster__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_spec_capacity_storage: 2Gi
boss__heapster__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/heapster"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__heapster__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__heapster__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################

##########################################################################################################
# ................##.....##.########.########.########..####..######...######...........######..########.########..##.....##.########.########.
# ................###...###.##..........##....##.....##..##..##....##.##....##.........##....##.##.......##.....##.##.....##.##.......##.....##
# ................####.####.##..........##....##.....##..##..##.......##...............##.......##.......##.....##.##.....##.##.......##.....##
# ................##.###.##.######......##....########...##..##........######..#######..######..######...########..##.....##.######...########.
# ................##.....##.##..........##....##...##....##..##.............##...............##.##.......##...##....##...##..##.......##...##..
# ................##.....##.##..........##....##....##...##..##....##.##....##.........##....##.##.......##....##....##.##...##.......##....##.
# ................##.....##.########....##....##.....##.####..######...######...........######..########.##.....##....###....########.##.....##
##########################################################################################################
boss__metrics__server__subdomain: metrics-server
boss__metrics__server__version: lts
boss__metrics__server__image_repo: "metrics-server/metrics-server"
boss__metrics__server__image_tag: "{{ boss__metrics__server__version }}"
boss__metrics__server__cpu_limit: 1000m
boss__metrics__server__cpu_requests: 1000m
boss__metrics__server__manifest_path: "{{manifest_dir_path}}/metrics-server"
boss__metrics__server__namespace_name: kube-system

boss__metrics__server__deployment_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__deployment_spec_replicas: 1
boss__metrics__server__deployment_spec_selector_match_labels: |
      k8s-app: metrics-server
boss__metrics__server__deployment_spec_template_metadata_labels: |
        k8s-app: metrics-server
boss__metrics__server__ingress_labels: |
    k8s-app: metrics-server-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__metrics__server__service_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "metrics-server"
    boss-part-of: metrics-server

boss__metrics__server__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__metrics__server__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_spec_capacity_storage: 2Gi
boss__metrics__server__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/metrics-server"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__metrics__server__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__metrics__server__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#   .########.##.....##.########.########.########..##....##....###....##...............########..##....##..######.
#   .##........##...##.....##....##.......##.....##.###...##...##.##...##...............##.....##.###...##.##....##
#   .##.........##.##......##....##.......##.....##.####..##..##...##..##...............##.....##.####..##.##......
#   .######......###.......##....######...########..##.##.##.##.....##.##.......#######.##.....##.##.##.##..######.
#   .##.........##.##......##....##.......##...##...##..####.#########.##...............##.....##.##..####.......##
#   .##........##...##.....##....##.......##....##..##...###.##.....##.##...............##.....##.##...###.##....##
#   .########.##.....##....##....########.##.....##.##....##.##.....##.########.........########..##....##..######.
##########################################################################################################
boss__external__dns__subdomain: external-dns
boss__external__dns__version: lts
boss__external__dns__image_repo: "external-dns/external-dns"
boss__external__dns__image_tag: "{{ boss__external__dns__version }}"
boss__external__dns__cpu_limit: 1000m
boss__external__dns__cpu_requests: 1000m
boss__external__dns__manifest_path: "{{manifest_dir_path}}/external-dns"
boss__external__dns__namespace_name: default

boss__external__dns__deployment_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__deployment_spec_replicas: 1
boss__external__dns__deployment_spec_selector_match_labels: |
      k8s-app: external-dns
boss__external__dns__deployment_spec_template_metadata_labels: |
        k8s-app: external-dns
boss__external__dns__ingress_labels: |
    k8s-app: external-dns-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__external__dns__service_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "external-dns"
    boss-part-of: external-dns

boss__external__dns__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__external__dns__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_spec_capacity_storage: 2Gi
boss__external__dns__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/external-dns"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__external__dns__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__external__dns__nfs_master_node_ip: "{{ nfs_server_ip_override }}"



##########################################################################################################
#              .##.....##.########.##.......##.....##
#              .##.....##.##.......##.......###...###
#              .##.....##.##.......##.......####.####
#              .#########.######...##.......##.###.##
#              .##.....##.##.......##.......##.....##
#              .##.....##.##.......##.......##.....##
#              .##.....##.########.########.##.....##
##########################################################################################################
boss__helm__subdomain: helm
boss__helm__manifest_path: "{{manifest_dir_path}}/helm"
boss__helm__namespace_name: kube-system

##########################################################################################################
#     .##.....##.########.########....###....##...............##.......########.
#     .###...###.##..........##......##.##...##...............##.......##.....##
#     .####.####.##..........##.....##...##..##...............##.......##.....##
#     .##.###.##.######......##....##.....##.##.......#######.##.......########.
#     .##.....##.##..........##....#########.##...............##.......##.....##
#     .##.....##.##..........##....##.....##.##...............##.......##.....##
#     .##.....##.########....##....##.....##.########.........########.########.
##########################################################################################################
boss__metallb__subdomain: metallb
boss__metallb__manifest_path: "{{manifest_dir_path}}/metallb"
boss__metallb__namespace_name: metallb-system
boss__metallb__address_pools: '192.168.1.30-192.168.1.40'




##########################################################################################################
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...########..########..######...######.
#              .###...##.##....##...##..###...##..##...##...........##..###...##.##....##..##.....##.##.......##....##.##....##
#              .####..##.##.........##..####..##...##.##............##..####..##.##........##.....##.##.......##.......##......
#              .##.##.##.##...####..##..##.##.##....###....#######..##..##.##.##.##...####.########..######....######...######.
#              .##..####.##....##...##..##..####...##.##............##..##..####.##....##..##...##...##.............##.......##
#              .##...###.##....##...##..##...###..##...##...........##..##...###.##....##..##....##..##.......##....##.##....##
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...##.....##.########..######...######.
##########################################################################################################
boss__ingress__nginx__subdomain: ingress-nginx
boss__ingress__nginx__manifest_path: "{{manifest_dir_path}}/ingress-nginx"
boss__ingress__nginx__namespace_name: kube-system

boss__ingress__nginx__controller_version: 0.21.0
boss__ingress__nginx__controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
boss__ingress__nginx__controller_image_tag: "{{ boss__ingress__nginx__controller_version }}"


boss__ingress__nginx__defaultbackend_version: 1.4
boss__ingress__nginx__defaultbackend_image_repo: "gcr.io/google_containers/defaultbackend"
boss__ingress__nginx__defaultbackend_image_tag: "{{ boss__ingress__nginx__defaultbackend_version }}"

boss__ingress__nginx__controller_runAsUser: 33

boss__ingress__nginx__clusterrole_serviceaccount_labels: |
  kubernetes.io/bootstrapping: rbac-defaults
  addonmanager.kubernetes.io/mode: Reconcile
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__role_nginx_ingress_role_labels: |
  kubernetes.io/bootstrapping: rbac-defaults
  addonmanager.kubernetes.io/mode: Reconcile
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_labels: |
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__service_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "10254"

boss__ingress__nginx__serviceAccountName: nginx-ingress

boss__ingress__nginx__configmap_nginx_load_balancer_conf_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__configmap_tcp_services_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__configmap_udp_services_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__deployment_labels: |
    app.kubernetes.io/name: ingress-nginx
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_replicas: 1
boss__ingress__nginx__deployment_spec_selector_match_labels: |
      app.kubernetes.io/name: ingress-nginx
      # app.kubernetes.io/part-of: kube-system
      app.kubernetes.io/part-of: ingress-nginx
      addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_labels: |
        app.kubernetes.io/name: ingress-nginx
        # app.kubernetes.io/part-of: kube-system
        app.kubernetes.io/part-of: ingress-nginx
        addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_annotations: |
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
boss__ingress__nginx__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "{{queen_host}}"
        # app.kubernetes.io/name: ingress-nginx
        # app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_account_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_selector_labels: |
  # app.kubernetes.io/name: ingress-nginx
  # app.kubernetes.io/part-of: ingress-nginx
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_defaulthttpbackend_selector_labels: |
  app.kubernetes.io/name: default-http-backend
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_default_http_backend_spec_type: NodePort
boss__ingress__nginx__service_ingress_nginx_spec_type: NodePort

boss__ingress__nginx__service_spec_ports: |
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  - name: stats
    port: 18080
    targetPort: 18080
    protocol: TCP
  - name: metrics
    port: 10254
    targetPort: 10254
    protocol: TCP

boss__ingress__nginx__service_defaulthttpbackend_spec_ports: |
  - port: 80
    # Assign 8080 to <POD-IP>:<targetPort>
    targetPort: 8080
    protocol: TCP
    name: http
    # nodePort: The Service created in the last section already used NodePort, so your nginx HTTPS replica is ready to serve traffic on the internet if your node has a public IP.
    # curl https://<WORKER-NODE-IP>:<NODE-PORT> -k
    # nodePort: 30001
    # - name: http
    #   port: 80
    #   targetPort: 80
    #   protocol: TCP
    # - name: https
    #   port: 443
    #   targetPort: 443
    #   protocol: TCP

boss__ingress__nginx__role_binding_labels: |
    # SOURCE: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Many of these are system: prefixed, which indicates that the resource is owned by the infrastructure. Modifications to these resources can result in non-functional clusters. One example is the system:node ClusterRole. This role defines permissions for kubelets. If the role is modified, it can prevent kubelets from working.
    # All of the default cluster roles and rolebindings are labeled with kubernetes.io/bootstrapping=rbac-defaults
    kubernetes.io/bootstrapping: rbac-defaults
    # addonmanager.kubernetes.io/mode: EnsureExists
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service__default_http_backend_labels: |
    app.kubernetes.io/name: default-http-backend
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    # kubernetes.io/minikube-addons: ingress
    # kubernetes.io/minikube-addons-endpoint: ingress
    addonmanager.kubernetes.io/mode: Reconcile

boss__ingress__nginx__service__default_http_backend_annotations: False
# boss__ingress__nginx__service__default_http_backend_annotations: |
#   prometheus.io/scrape: "true"
#   prometheus.io/port: "10254"

boss__ingress__nginx__deployment__default_http_backend_labels: |
    app.kubernetes.io/name: default-http-backend
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    # kubernetes.io/minikube-addons: ingress
    # kubernetes.io/minikube-addons-endpoint: ingress
    addonmanager.kubernetes.io/mode: Reconcile

boss__ingress__nginx__defaulthttpbackend_deployment_spec_labels_matchLabels: |
  app.kubernetes.io/name: default-http-backend
  # addonmanager.kubernetes.io/mode: Reconcile


boss__ingress__nginx__defaulthttpbackend_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 20m
    memory: 30Mi
  requests:
    cpu: 20m
    memory: 30Mi

boss__ingress__nginx__defaulthttpbackend_deployment_spec_template_spec_containers_ports: |
  - containerPort: 8080

boss__ingress__nginx__nginx_ingress_controller_deployment_spec_template_spec_containers_ports: |
  - name: http
    containerPort: 80
    hostPort: 80
  - name: https
    containerPort: 443
    hostPort: 443
  # (Optional) we expose 18080 to access nginx stats in url /nginx-status
  - name: stats
    containerPort: 18080
    hostPort: 18080
  - name: metrics
    containerPort: 10254
    hostPort: 10254
    protocol: TCP

# boss__ingress__nginx__nginx_ingress_controller_deployment_spec_template_spec_containers_args: |
#   - /nginx-ingress-controller
#   # Service used to serve HTTP requests not matching any known server name (catch-all). Takes the form "namespace/name". The controller configures NGINX to forward requests to the first port of this Service. If not specified, a 404 page will be returned directly from NGINX.
#   - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
#   # Name of the ConfigMap containing custom global configurations for the controller.
#   - --configmap=$(POD_NAMESPACE)/nginx-load-balancer-conf
#   # Name of the ConfigMap containing the definition of the TCP services to expose. The key in the map indicates the external port to be used. The value is a reference to a Service in the form "namespace/name:port", where "port" can either be a port number or name. TCP ports 80 and 443 are reserved by the controller for servicing HTTP traffic.
#   - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
#   # Name of the ConfigMap containing the definition of the UDP services to expose. The key in the map indicates the external port to be used. The value is a reference to a Service in the form "namespace/name:port", where "port" can either be a port name or number.
#   - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
#   # Prefix of the Ingress annotations specific to the NGINX controller. (default "nginx.ingress.kubernetes.io")
#   - --annotations-prefix=nginx.ingress.kubernetes.io
#   # NOTE: 'report-node-internal-ip-address' - Set the load-balancer status of Ingress objects to internal Node addresses instead of external. Requires the update-status parameter.
#   # use minikube IP address in ingress status field
#   - --report-node-internal-ip-address
#   # log level for V logs
#   # - --v=5
#   # log to standard error instead of files (default true)
#   - --logtostderr
#   # Enable profiling via web interface host:port/debug/pprof/ (default true)
#   - --profiling

#   - --publish-service=$(POD_NAMESPACE)/ingress-nginx

####################################################################################################
#                  .########.########.....###....########.########.####.##....##.........####.##....##.########.########.########..##....##....###....##......
#                  ....##....##.....##...##.##...##.......##........##..##...##...........##..###...##....##....##.......##.....##.###...##...##.##...##......
#                  ....##....##.....##..##...##..##.......##........##..##..##............##..####..##....##....##.......##.....##.####..##..##...##..##......
#                  ....##....########..##.....##.######...######....##..#####....#######..##..##.##.##....##....######...########..##.##.##.##.....##.##......
#                  ....##....##...##...#########.##.......##........##..##..##............##..##..####....##....##.......##...##...##..####.#########.##......
#                  ....##....##....##..##.....##.##.......##........##..##...##...........##..##...###....##....##.......##....##..##...###.##.....##.##......
#                  ....##....##.....##.##.....##.########.##.......####.##....##.........####.##....##....##....########.##.....##.##....##.##.....##.########
##########################################################################################################
boss__traefik__internal__subdomain: traefik-internal
boss__traefik__internal__manifest_path: "{{manifest_dir_path}}/traefik-internal"
boss__traefik__internal__namespace_name: traefik-system
boss__traefik__internal__service_spec_type: LoadBalancer
boss__traefik__internal__service_spec_loadBalancerIP: 192.168.1.30


####################################################################################################
#          .##......##.########....###....##.....##.########..........######...######...#######..########..########
#          .##..##..##.##.........##.##...##.....##.##...............##....##.##....##.##.....##.##.....##.##......
#          .##..##..##.##........##...##..##.....##.##...............##.......##.......##.....##.##.....##.##......
#          .##..##..##.######...##.....##.##.....##.######...#######..######..##.......##.....##.########..######..
#          .##..##..##.##.......#########..##...##..##.....................##.##.......##.....##.##........##......
#          .##..##..##.##.......##.....##...##.##...##...............##....##.##....##.##.....##.##........##......
#          ..###..###..########.##.....##....###....########..........######...######...#######..##........########
##########################################################################################################
boss__weave__scope__subdomain: weave-scope
boss__weave__scope__manifest_path: "{{manifest_dir_path}}/weave-scope"
boss__weave__scope__namespace_name: weave
boss__weave__scope__manifest_filename_based_on_networking: "weave-calico-networking.yaml"
boss__weave__scope__enable_ingress_traefik: "enabled"

boss__weave__scope__ingress_metadata_annotations: |
  traefik.frontend.rule.type: PathPrefix


###########################

boss__prometheus__operator__subdomain: prometheus-operator
boss__prometheus__operator__manifest_path: "{{manifest_dir_path}}/prometheus-operator-v0-27-0"
boss__prometheus__operator__namespace_name: monitoring

# This doesn't have a last tag or anything like that
boss__prometheus__operator__alertmanager_subdomain: alertmanager
boss__prometheus__operator__alertmanager_version: v0.16.0
boss__prometheus__operator__alertmanager_crd_baseImage: "quay.io/prometheus/alertmanager"
boss__prometheus__operator__alertmanager_image_tag: "{{ boss__prometheus__operator__alertmanager_version }}"
boss__prometheus__operator__alertmanager_replicas: 3

boss__prometheus__operator__grafana_subdomain: grafana
boss__prometheus__operator__grafana_version: 6.0.2
boss__prometheus__operator__grafana_image_repo: "grafana/grafana"
boss__prometheus__operator__grafana_image_tag: "{{ boss__prometheus__operator__grafana_version }}"
boss__prometheus__operator__grafana_cpu_limit: 2000m
boss__prometheus__operator__grafana_mem_limit: 500Mi
boss__prometheus__operator__grafana_cpu_requests: 1000m
boss__prometheus__operator__grafana_mem_requests: 200Mi
boss__prometheus__operator__grafana_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 2000m
    memory: 200Mi
  requests:
    cpu: 1000m
    memory: 100Mi

boss__prometheus__operator__prometheus_adapter_version: v0.4.1
boss__prometheus__operator__prometheus_adapter_crd_baseImage: "quay.io/coreos/k8s-prometheus-adapter-amd64"
boss__prometheus__operator__prometheus_adapter_image_tag: "{{ boss__prometheus__operator__prometheus_adapter_version }}"

boss__prometheus__operator__prometheus_subdomain: prometheus
boss__prometheus__operator__prometheus_version: v2.5.0
boss__prometheus__operator__prometheus_crd_baseImage: "quay.io/prometheus/prometheus"
boss__prometheus__operator__prometheus_image_tag: "{{ boss__prometheus__operator__prometheus_version }}"

boss__prometheus__operator__node_exporter_version: v0.17.0
boss__prometheus__operator__node_exporter_image_repo: "quay.io/prometheus/node-exporter"
boss__prometheus__operator__node_exporter_image_tag: "{{ boss__prometheus__operator__node_exporter_version }}"
boss__prometheus__operator__node_exporter_daemonset_spec_template_spec_containers_resources: |
  limits:
    cpu: 250m
    memory: 180Mi
  requests:
    cpu: 102m
    memory: 180Mi

boss__prometheus__operator__kube_rbac_proxy_version: v0.4.1
boss__prometheus__operator__kube_rbac_proxy_image_repo: "quay.io/coreos/kube-rbac-proxy"
boss__prometheus__operator__kube_rbac_proxy_image_tag: "{{ boss__prometheus__operator__kube_rbac_proxy_version }}"
boss__prometheus__operator__kube_rbac_proxy_daemonset_spec_template_spec_containers_resources: |
  # NOTE: Orig below ( 3/3/2019 )
  #limits:
  #  cpu: 20m
  #  memory: 40Mi
  #requests:
  #  cpu: 10m
  #  memory: 20Mi
  limits:
    cpu: 100m
    memory: 120Mi
  requests:
    cpu: 90m
    memory: 60Mi

boss__prometheus__operator__kube_rbac_proxy_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 20m
    memory: 40Mi
  requests:
    cpu: 10m
    memory: 20Mi

boss__prometheus__operator__kube_state_metrics_version: v1.5.0
boss__prometheus__operator__kube_state_metrics_image_repo: "quay.io/coreos/kube-state-metrics"
boss__prometheus__operator__kube_state_metrics_image_tag: "{{ boss__prometheus__operator__kube_state_metrics_version }}"
boss__prometheus__operator__kube_state_metrics_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 100m
    memory: 150Mi
  requests:
    cpu: 100m
    memory: 150Mi

boss__prometheus__operator__addon_resizer_version: 2.1
boss__prometheus__operator__addon_resizer_image_repo: "gcr.io/google-containers/addon-resizer-amd64"
boss__prometheus__operator__addon_resizer_image_tag: "{{ boss__prometheus__operator__addon_resizer_version }}"
boss__prometheus__operator__addon_resizer_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 150m
    memory: 150Mi
  requests:
    cpu: 100m
    memory: 150Mi

boss__prometheus__operator__prometheus_operator_version: v0.29.0
boss__prometheus__operator__prometheus_operator_image_repo: "quay.io/coreos/prometheus-operator"
boss__prometheus__operator__prometheus_operator_image_tag: "{{ boss__prometheus__operator__prometheus_operator_version }}"
boss__prometheus__operator__prometheus_operator_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

boss__prometheus__operator__configmap_reload_version: v0.0.1
boss__prometheus__operator__configmap_reload_image_repo: "quay.io/coreos/configmap-reload"
boss__prometheus__operator__configmap_reload_image_tag: "{{ boss__prometheus__operator__configmap_reload_version }}"

boss__prometheus__operator__prometheus_operator_config_reloader_version: v0.29.0
boss__prometheus__operator__prometheus_operator_config_reloader_image_repo: "quay.io/coreos/prometheus-config-reloader"
boss__prometheus__operator__prometheus_operator_config_reloader_image_tag: "{{ boss__prometheus__operator__prometheus_operator_config_reloader_version }}"
boss__prometheus__operator__prometheus_operator_config_reloader_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

boss__prometheus__operator__alertmanager_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__alertmanager_ingress_metadata_labels: |
  run: nginx
  alertmanager: main
  app: alertmanager


boss__prometheus__operator__prometheus_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__prometheus_ingress_metadata_labels: |
  app: prometheus
  prometheus: k8s
  run: nginx
  k8s-app: prometheus-operator


boss__prometheus__operator__grafana_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__grafana_ingress_metadata_labels: |
  run: nginx
  app: grafana

################# Persistent volume claims

# persistent volume claim - grafana
boss__prometheus__operator__grafana_pvc_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator

boss__prometheus__operator__grafana_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__grafana_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - grafana
boss__prometheus__operator__grafana_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__grafana_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__grafana_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/grafana"


# persistent volume claim - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator
  boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/prometheus-adapter-tmpfs"

# persistent volume claim - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/prometheus-adapter-volume-serving-cert"


boss__prometheus__operator__prometheus_additional_scrape_configs: |
  # RUN: kubectl --namespace monitoring create secret generic additional-scrape-configs --from-file=PLAINTEXT-SECRET-prometheus-additional.yaml --dry-run -oyaml > additional-scrape-configs.yaml
  # global:
  #     scrape_interval: 10s
  - job_name: 'netdata-scrape'

    metrics_path: '/api/v1/allmetrics'
    params:
      # format: prometheus | prometheus_all_hosts
      # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP
      format: [prometheus]
      #
      # sources: as-collected | raw | average | sum | volume
      # default is: average
      #source: [as-collected]
      #
      # server name for this prometheus - the default is the client IP
      # for netdata to uniquely identify it
      #server: ['prometheus1']
    honor_labels: true

    static_configs:
      - targets: ['192.168.1.172:19999','192.168.1.173:19999','192.168.1.174:19999']

  - job_name: 'ingress-nginx-endpoints'
    kubernetes_sd_configs:
    - role: pod
      namespaces:
        names:
        - kube-system

    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
      action: replace
      target_label: __scheme__
      regex: (https?)
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      target_label: __address__
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2

    - source_labels: [__meta_kubernetes_service_name]
      regex: prometheus-server
      action: drop


# SOURCE: https://techtran.science/2018/06/11/unifi-to-grafana-using-prometheus-and-unifi_exporter/
# - job_name: 'unifi_exporter'
#   static_configs:
#     - targets: ['dockerswarm:9130']
#       labels:
#         alias: unifi_exporter

boss__prometheus__operator__alertmanager_plaintext_configs: |
  global:
    resolve_timeout: 5m
  route:
    group_by: ['job']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: 'null'
    routes:
    - match:
        alertname: Watchdog
      receiver: 'null'
  receivers:
  - name: 'null'


####################################################################################################
#   .##.....##.##....##.####.########.####.........########.##.....##.########...#######..########..########.########.########.
#   .##.....##.###...##..##..##........##..........##........##...##..##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.####..##..##..##........##..........##.........##.##...##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.##.##.##..##..######....##..#######.######......###....########..##.....##.########.....##....######...########.
#   .##.....##.##..####..##..##........##..........##.........##.##...##........##.....##.##...##......##....##.......##...##..
#   .##.....##.##...###..##..##........##..........##........##...##..##........##.....##.##....##.....##....##.......##....##.
#   ..#######..##....##.####.##.......####.........########.##.....##.##.........#######..##.....##....##....########.##.....##
####################################################################################################
boss__unifi__exporter__subdomain: unifi-exporter
boss__unifi__exporter__manifest_path: "{{manifest_dir_path}}/unifi-exporter"
boss__unifi__exporter__namespace_name: monitoring
boss__unifi__exporter__apply_changes_immediately: False
boss__unifi__exporter__delete_secrets: True

boss__unifi__exporter__port: 9130
boss__unifi__exporter__listen_address: ":{{boss__unifi__exporter__port}}"
boss__unifi__exporter__listen_metricspath: /metrics
boss__unifi__exporter__unifi_address: https://192.168.1.8:8443
boss__unifi__exporter__unifi_site: Hyenanet
boss__unifi__exporter__unifi_insecure: true
boss__unifi__exporter__unifi_timeout: 5s


boss__unifi__exporter__service_annotations: |
  prometheus.io/scrape: 'true'
  prometheus.io/port: '9091'




##########################################################################################################
#   .####.##....##.########.##.......##.....##.##.....##.########..########...........#######..########..########.########.....###....########..#######..########.
#   ..##..###...##.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##.....##.##.......##.....##...##.##......##....##.....##.##.....##
#   ..##..####..##.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##.....##.##.......##.....##..##...##.....##....##.....##.##.....##
#   ..##..##.##.##.######...##.......##.....##....###....##.....##.########..#######.##.....##.########..######...########..##.....##....##....##.....##.########.
#   ..##..##..####.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##........##.......##...##...#########....##....##.....##.##...##..
#   ..##..##...###.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##........##.......##....##..##.....##....##....##.....##.##....##.
#   .####.##....##.##.......########..#######..##.....##.########..########...........#######..##........########.##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__influxdb__operator__subdomain: influxdb
boss__influxdb__operator__manifest_path: "{{manifest_dir_path}}/influxdb-operator"
boss__influxdb__operator__namespace_name: monitoring
boss__influxdb__operator__name: influxdata-operator


boss__influxdb__operator__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__influxdb__operator__ingress_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__deployment_labels: |
  app: influxdata-operator
  name: influxdata-operator

boss__influxdb__operator__service_labels: |
    k8s-app: influxdata-operator
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "influxdata-operator"
    name: influxdata-operator
    boss-part-of: influxdb
    app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_spec_resources_requests_storage: "2Gi"

boss__influxdb__operator__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__version: 1.6.6
boss__influxdb__operator__image_repo: "influxdb"
boss__influxdb__operator__image_tag: "{{ boss__influxdb__operator__version }}"
boss__influxdb__operator__cpu_limit: 1000m
boss__influxdb__operator__mem_limit: 4048Mi
boss__influxdb__operator__cpu_requests: 100m
boss__influxdb__operator__mem_requests: 2350Mi

boss__influxdb__operator__service_account_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_binding_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__persistent_volume_spec_capacity_storage: 1Gi
boss__influxdb__operator__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/influxdb-operator"
# boss__influxdb__operator__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__influxdb__operator__nfs_master_node_ip: "{{ nfs_server_ip_override }}"

#######################

boss__bootstrap__subdomain: bootstrap
boss__bootstrap__manifest_path: "{{manifest_dir_path}}/bootstrap"
boss__bootstrap__namespace_name: kube-system
boss__bootstrap__deployment_name: bootstrap



##########################################################################################################
#   .########.##.......##.....##.########.##....##.########.........########..####.########..........######..########.##....##.########.########.....###....##.......####.########.########.########.
#   .##.......##.......##.....##.##.......###...##....##............##.....##..##.....##............##....##.##.......###...##....##....##.....##...##.##...##........##.......##..##.......##.....##
#   .##.......##.......##.....##.##.......####..##....##............##.....##..##.....##............##.......##.......####..##....##....##.....##..##...##..##........##......##...##.......##.....##
#   .######...##.......##.....##.######...##.##.##....##....#######.########...##.....##....#######.##.......######...##.##.##....##....########..##.....##.##........##.....##....######...##.....##
#   .##.......##.......##.....##.##.......##..####....##............##.....##..##.....##............##.......##.......##..####....##....##...##...#########.##........##....##.....##.......##.....##
#   .##.......##.......##.....##.##.......##...###....##............##.....##..##.....##............##....##.##.......##...###....##....##....##..##.....##.##........##...##......##.......##.....##
#   .##.......########..#######..########.##....##....##............########..####....##.............######..########.##....##....##....##.....##.##.....##.########.####.########.########.########.
##########################################################################################################
boss__fluent__bit__centralized__subdomain: fluent-bit-centralized
boss__fluent__bit__centralized__manifest_path: "{{manifest_dir_path}}/fluent-bit-centralized"
boss__fluent__bit__centralized__namespace_name: kube-system
boss__fluent__bit__centralized__fluent_bit_version: 1.0.5
boss__fluent__bit__centralized__fluent_bit_image_repo: "fluent/fluent-bit"
boss__fluent__bit__centralized__fluent_bit_image_tag: "{{ boss__fluent__bit__centralized__fluent_bit_version }}"
boss__fluent__bit__centralized__shared_name: fluent-bit-centralized
boss__fluent__bit__centralized__configmap_suffix: "-0-3-0"
boss__fluent__bit__centralized__create_namespace: "absent"
boss__fluent__bit__centralized__create_ingress: "present"
boss__fluent__bit__centralized__loglevel: "debug"

boss__fluent__bit__centralized__deployment_spec_template_containers_resources: |
  limits:
    cpu: ".2"
    memory: "100Mi"
  requests:
    cpu: "0.05"
    memory: "10Mi"

boss__fluent__bit__centralized__serviceAccountName: "{{boss__fluent__bit__centralized__shared_name}}"
boss__fluent__bit__centralized__deployment_spec_replicas: 3

# boss__fluent__bit__centralized__deployment_spec_template_containers_command: []

boss__fluent__bit__centralized__deployment_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__deployment_spec_matchLabels: "disabled"

# boss__fluent__bit__centralized__deployment_spec_matchLabels: |-
#   k8s-app: {{boss__fluent__bit__centralized__shared_name}}
#   version: v1

boss__fluent__bit__centralized__deployment_spec_template_metadata_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__deployment_spec_template_metadata_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "2020"
  prometheus.io/path: /api/v1/metrics/prometheus

boss__fluent__bit__centralized__deployment_spec_template_containers_ports: |-
  - containerPort: 2020
    name: http-metrics
  - containerPort: 5140
    name: syslog
    protocol: UDP

boss__fluent__bit__centralized__deployment_spec_template_containers_env: |-
  # - name: FLUENT_ELASTICSEARCH_HOST
  #   value: "elasticsearch"
  # - name: FLUENT_ELASTICSEARCH_PORT
  #   value: "9200"

    # Memory limit that the file tail plugin can use when appending data to the Engine
  - name: TAIL_BUF_LIMIT
    value: "5MB"

  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

boss__fluent__bit__centralized__deployment_spec_template_spec_volumeMounts: |
  # The volume where logs will be delivered by the Docker logging driver
  - name: logging-volume
    mountPath: /logging-volume
  # The volume where Fluent Bit stores persistent data (position databases for tracking ingested files)
  - name: fluent-data
    mountPath: /var/fluent-bit
  # The Fluent Bit config file to use
  - name: fluent-bit-config
    mountPath: /fluent-bit/etc

boss__fluent__bit__centralized__deployment_spec_template_spec_volumes: |
  - name: logging-volume
    emptyDir: {}
  - name: fluent-bit-config
    configMap:
      name: {{boss__fluent__bit__centralized__shared_name}}-config{{boss__fluent__bit__centralized__configmap_suffix}}
  - name: fluent-data
    emptyDir: {}

boss__fluent__bit__centralized__service_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__service_annotations: "disabled"

boss__fluent__bit__centralized__service_spec_type: ClusterIP

boss__fluent__bit__centralized__service_spec_ports: |-
  # - port: 2020
  #   targetPort: 2020
  #   protocol: TCP
  #   name: http-metrics
  - port: 5140
    targetPort: 5140
    protocol: UDP
    name: syslog

boss__fluent__bit__centralized__service_spec_selector: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}

boss__fluent__bit__centralized__ingress_spec_rules_http_paths: |
  - path: /
    backend:
      serviceName: {{boss__fluent__bit__centralized__shared_name}}
      servicePort: 5140

boss__fluent__bit__centralized__ingress_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__ingress_annotations: |-
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__fluent__bit__centralized__deployment_spec_template_spec_hostNetwork: True
boss__fluent__bit__centralized__deployment_spec_template_spec_hostPID: True


boss__fluent__bit__centralized__deployment_spec_template_spec_nodeSelector: |-
  beta.kubernetes.io/os: linux


boss__fluent__bit__centralized__deployment_spec_template_spec_securityContext: "disabled"

#      hostNetwork: true
#      hostPID: true
#      nodeSelector:
#        beta.kubernetes.io/os: linux
#      securityContext:
#        runAsNonRoot: true
#        runAsUser: 65534

# [20:34:19,104] <inform-707> INFO inform - from [MAC ADDRESS](UniFi Security Gateway 3P, UGW3, 4.4.36.5146617): state=CONNECTED, last_inform=3, ext/stun_ip=192.168.1.1, dev_ip=192.168.0.3, up=4955230

##########################################################################################################
# .##....##.########..########.
# .###...##.##.....##.##.....##
# .####..##.##.....##.##.....##
# .##.##.##.########..##.....##
# .##..####.##........##.....##
# .##...###.##........##.....##
# .##....##.##........########.
##########################################################################################################

boss__npd__subdomain: npd
boss__npd__manifest_path: "{{manifest_dir_path}}/npd"
boss__npd__namespace_name: kube-system



##########################################################################################################
# .########.##..........###.....######..########.####..######...######..########....###....########...######..##.....##.........########.##.....##.########...#######..########..########.########.########.
# .##.......##.........##.##...##....##....##.....##..##....##.##....##.##.........##.##...##.....##.##....##.##.....##.........##........##...##..##.....##.##.....##.##.....##....##....##.......##.....##
# .##.......##........##...##..##..........##.....##..##.......##.......##........##...##..##.....##.##.......##.....##.........##.........##.##...##.....##.##.....##.##.....##....##....##.......##.....##
# .######...##.......##.....##..######.....##.....##..##........######..######...##.....##.########..##.......#########.#######.######......###....########..##.....##.########.....##....######...########.
# .##.......##.......#########.......##....##.....##..##.............##.##.......#########.##...##...##.......##.....##.........##.........##.##...##........##.....##.##...##......##....##.......##...##..
# .##.......##.......##.....##.##....##....##.....##..##....##.##....##.##.......##.....##.##....##..##....##.##.....##.........##........##...##..##........##.....##.##....##.....##....##.......##....##.
# .########.########.##.....##..######.....##....####..######...######..########.##.....##.##.....##..######..##.....##.........########.##.....##.##.........#######..##.....##....##....########.##.....##
##########################################################################################################
boss__elasticsearch__exporter__subdomain: elasticsearch-exporter
boss__elasticsearch__exporter__manifest_path: "{{manifest_dir_path}}/elasticsearch-exporter"
boss__elasticsearch__exporter__namespace_name: kube-system
boss__elasticsearch__exporter__fluent_bit_version: 1.0.1
boss__elasticsearch__exporter__fluent_bit_image_repo: "fjustwatch/elasticsearch_exporter"
boss__elasticsearch__exporter__fluent_bit_image_tag: "{{ boss__elasticsearch__exporter__fluent_bit_version }}"
boss__elasticsearch__exporter__shared_name: p8s-elastic-exporter
boss__elasticsearch__exporter__configmap_suffix: "-0-3-0"
boss__elasticsearch__exporter__create_namespace: "absent"
boss__elasticsearch__exporter__create_ingress: "present"
boss__elasticsearch__exporter__loglevel: "debug"

boss__elasticsearch__exporter__deployment_spec_template_containers_resources: |
  limits:
    cpu: ".2"
    memory: "128Mi"
  requests:
    cpu: "0.05"
    memory: "64Mi"

boss__elasticsearch__exporter__serviceAccountName: "{{boss__elasticsearch__exporter__shared_name}}-read"
boss__elasticsearch__exporter__deployment_spec_replicas: 1
boss__elasticsearch__exporter__deployment_spec_strategy: |-
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
  type: RollingUpdate


boss__elasticsearch__exporter__deployment_spec_template_containers_command: |-
  - /bin/elasticsearch_exporter
  - -es.uri=http://elasticsearch-logging:9200
  - -es.all=true

boss__elasticsearch__exporter__deployment_labels: |-
  k8s-app: {{boss__elasticsearch__exporter__shared_name}}

boss__elasticsearch__exporter__deployment_spec_matchLabels: "disabled"

boss__elasticsearch__exporter__deployment_spec_template_metadata_labels: |-
  k8s-app: {{boss__elasticsearch__exporter__shared_name}}

boss__elasticsearch__exporter__deployment_spec_template_metadata_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "9114"

boss__elasticsearch__exporter__deployment_spec_template_containers_ports: |-
  - containerPort: 9114
    name: http-metrics

boss__elasticsearch__exporter__deployment_spec_template_containers_env: |-
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

boss__elasticsearch__exporter__deployment_spec_template_spec_volumeMounts: "disabled"

boss__elasticsearch__exporter__deployment_spec_template_spec_volumes: "disabled"

boss__elasticsearch__exporter__service_labels: |-
  k8s-app: {{boss__elasticsearch__exporter__shared_name}}

boss__elasticsearch__exporter__service_annotations: "disabled"

boss__elasticsearch__exporter__service_spec_type: ClusterIP

boss__elasticsearch__exporter__service_spec_ports: |-
  - port: 9114
    targetPort: 9114
    protocol: TCP
    name: http-metrics

boss__elasticsearch__exporter__service_spec_selector: |-
  k8s-app: {{boss__elasticsearch__exporter__shared_name}}

boss__elasticsearch__exporter__ingress_spec_rules_http_paths: |
  - path: /
    backend:
      serviceName: {{boss__elasticsearch__exporter__shared_name}}
      servicePort: 5140

boss__elasticsearch__exporter__ingress_labels: |-
  k8s-app: {{boss__elasticsearch__exporter__shared_name}}

boss__elasticsearch__exporter__ingress_annotations: |-
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__elasticsearch__exporter__deployment_spec_template_spec_hostNetwork: False
boss__elasticsearch__exporter__deployment_spec_template_spec_hostPID: False

boss__elasticsearch__exporter__deployment_spec_template_spec_nodeSelector: |-
  beta.kubernetes.io/os: linux

boss__elasticsearch__exporter__deployment_spec_template_spec_securityContext: |-
  runAsNonRoot: true
  runAsUser: 1000

boss__elasticsearch__exporter__deployment_spec_template_spec_containers_securityContext: |-
  capabilities:
    drop:
    - SETPCAP
    - MKNOD
    - AUDIT_WRITE
    - CHOWN
    - NET_RAW
    - DAC_OVERRIDE
    - FOWNER
    - FSETID
    - KILL
    - SETGID
    - SETUID
    - NET_BIND_SERVICE
    - SYS_CHROOT
    - SETFCAP
  readOnlyRootFilesystem: true

boss__elasticsearch__exporter__deployment_spec_template_spec_containers_livenessProbe: |-
  httpGet:
    path: /health
    port: 9114
  initialDelaySeconds: 30
  timeoutSeconds: 10

boss__elasticsearch__exporter__deployment_spec_template_spec_containers_readinessProbe: |-
  httpGet:
    path: /health
    port: 9114
  initialDelaySeconds: 10
  timeoutSeconds: 10

boss__elasticsearch__exporter__cluster_role_rules: |-
  - apiGroups:
    - ""
    resources:
    - "namespaces"
    - "pods"
    - "secrets"
    - "pods/log"
    - "deployments"
    - "batch"
    - "extensions"
    - "crontabs"
    verbs:
    - "get"
    - "watch"
    - "list"
    - "update"
    - "post"
  # DEBUG: CUSTOM FROM HERE ON OUT ---- 5/10/2019
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - volumeattachments
    verbs:
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - list
    - delete
  - apiGroups:
    - ""
    resources:
    - services
    - services/finalizers
    - endpoints
    verbs:
    - get
    - create
    - update
    - delete
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  # DEBUG: CUSTOM FROM HERE ON OUT END ---- 5/10/2019

##########################################################################################################
# .########.##.......##.....##.########.##....##.########.########...........######..########.##....##.########.########.....###....##.......####.########.########.########.
# .##.......##.......##.....##.##.......###...##....##....##.....##.........##....##.##.......###...##....##....##.....##...##.##...##........##.......##..##.......##.....##
# .##.......##.......##.....##.##.......####..##....##....##.....##.........##.......##.......####..##....##....##.....##..##...##..##........##......##...##.......##.....##
# .######...##.......##.....##.######...##.##.##....##....##.....##.#######.##.......######...##.##.##....##....########..##.....##.##........##.....##....######...##.....##
# .##.......##.......##.....##.##.......##..####....##....##.....##.........##.......##.......##..####....##....##...##...#########.##........##....##.....##.......##.....##
# .##.......##.......##.....##.##.......##...###....##....##.....##.........##....##.##.......##...###....##....##....##..##.....##.##........##...##......##.......##.....##
# .##.......########..#######..########.##....##....##....########...........######..########.##....##....##....##.....##.##.....##.########.####.########.########.########.
##########################################################################################################
boss__fluentd__centralized__subdomain: fluentd-centralized
boss__fluentd__centralized__manifest_path: "{{manifest_dir_path}}/fluentd-centralized"
boss__fluentd__centralized__namespace_name: kube-system
# boss__fluentd__centralized__fluentd_version: v2.0.4-ui
# boss__fluentd__centralized__fluentd_image_repo: "alogoc/fluentd-elasticsearch"
boss__fluentd__centralized__fluentd_version: v2.5.0
boss__fluentd__centralized__fluentd_image_repo: "bossjones/fluentd-elasticsearch"
boss__fluentd__centralized__fluentd_image_tag: "{{ boss__fluentd__centralized__fluentd_version }}"
boss__fluentd__centralized__shared_name: fluentd-centralized
boss__fluentd__centralized__configmap_suffix: "-0-3-0"
boss__fluentd__centralized__create_namespace: "absent"
boss__fluentd__centralized__create_ingress: "present"
boss__fluentd__centralized__loglevel: "debug"

boss__fluentd__centralized__deployment_spec_template_containers_resources: |
  limits:
    cpu: ".2"
    memory: "100Mi"
  requests:
    cpu: "0.05"
    memory: "10Mi"

boss__fluentd__centralized__serviceAccountName: "{{boss__fluentd__centralized__shared_name}}"
boss__fluentd__centralized__deployment_spec_replicas: 3

# boss__fluentd__centralized__deployment_spec_template_containers_command: []

boss__fluentd__centralized__cluster_role_rules: |-
  - apiGroups:
    - ""
    resources:
    - "namespaces"
    - "pods"
    - "secrets"
    - "pods/log"
    - "deployments"
    - "batch"
    - "extensions"
    - "crontabs"
    verbs:
    - "get"
    - "watch"
    - "list"
    - "update"
    - "post"

boss__fluentd__centralized__deployment_labels: |-
  k8s-app: {{boss__fluentd__centralized__shared_name}}
  version: v2.5.0
  kubernetes.io/cluster-service: "true"

boss__fluentd__centralized__deployment_spec_matchLabels: "disabled"

# boss__fluentd__centralized__deployment_spec_matchLabels: |-
#   k8s-app: {{boss__fluentd__centralized__shared_name}}
#   version: v2.5.0

boss__fluentd__centralized__deployment_spec_template_metadata_labels: |-
  k8s-app: {{boss__fluentd__centralized__shared_name}}
  version: v2.5.0
  kubernetes.io/cluster-service: "true"

boss__fluentd__centralized__deployment_spec_template_metadata_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "24231"

boss__fluentd__centralized__deployment_spec_template_containers_ports: |-
  # - containerPort: 2020
  #   name: http-metrics
  # - containerPort: 5140
  #   name: syslog
  #   protocol: UDP
  - containerPort: 24224
    name: in-forward
    protocol: TCP
  - containerPort: 9880
    name: http
    protocol: TCP
  - containerPort: 161
    name: snmp
    protocol: UDP
  - containerPort: 5160
    name: in-udp
    protocol: UDP
  - containerPort: 5170
    name: in-tcp
    protocol: TCP
  - containerPort: 5140
    name: in-syslog
    protocol: TCP
  - containerPort: 9292
    name: fluentd-ui
    protocol: TCP
  - containerPort: 24231
    name: promths-mtr
    protocol: TCP
  - containerPort: 24220
    name: monitor-agnt
    protocol: TCP

boss__fluentd__centralized__deployment_spec_template_containers_env: |-
  # - name: FLUENT_ELASTICSEARCH_HOST
  #   value: "elasticsearch"
  # - name: FLUENT_ELASTICSEARCH_PORT
  #   value: "9200"

    # Memory limit that the file tail plugin can use when appending data to the Engine
  - name: TAIL_BUF_LIMIT
    value: "5MB"

  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

boss__fluentd__centralized__deployment_spec_template_spec_volumeMounts: |
  # The volume where logs will be delivered by the Docker logging driver
  - name: logging-volume
    mountPath: /logging-volume
  # The volume where Fluent Bit stores persistent data (position databases for tracking ingested files)
  - name: fluent-data
    mountPath: /var/fluent-bit
  # The Fluent Bit config file to use
  - name: fluent-bit-config
    mountPath: /fluent-bit/etc

boss__fluentd__centralized__deployment_spec_template_spec_volumes: |
  - name: logging-volume
    emptyDir: {}
  - name: fluent-bit-config
    configMap:
      name: {{boss__fluentd__centralized__shared_name}}-config{{boss__fluentd__centralized__configmap_suffix}}
  - name: fluent-data
    emptyDir: {}

boss__fluentd__centralized__service_labels: |-
  k8s-app: {{boss__fluentd__centralized__shared_name}}
  version: v2.5.0
  kubernetes.io/cluster-service: "true"

boss__fluentd__centralized__service_annotations: "disabled"

boss__fluentd__centralized__service_spec_type: ClusterIP

boss__fluentd__centralized__service_spec_ports: |-
  # - port: 2020
  #   targetPort: 2020
  #   protocol: TCP
  #   name: http-metrics
  # - port: 5140
  #   targetPort: 5140
  #   protocol: UDP
  #   name: syslog
  - name: in-forward
    port: 24224
    protocol: TCP
    targetPort: in-forward
  - name: http
    port: 9880
    protocol: TCP
    targetPort: http
  - name: snmp
    port: 161
    protocol: UDP
    targetPort: snmp
  - name: in-udp
    port: 5160
    protocol: UDP
    targetPort: in-udp
  - name: in-tcp
    port: 5170
    protocol: TCP
    targetPort: in-tcp
  - name: in-syslog
    port: 5140
    protocol: TCP
    targetPort: in-syslog
  - name: fluentd-ui
    port: 9292
    protocol: TCP
    targetPort: fluentd-ui
  - name: promths-mtr
    port: 24231
    protocol: TCP
    targetPort: promths-mtr
  - name: monitor-agnt
    port: 24220
    protocol: TCP
    targetPort: monitor-agnt

boss__fluentd__centralized__service_spec_selector: |-
  k8s-app: {{boss__fluentd__centralized__shared_name}}

boss__fluentd__centralized__ingress_spec_rules_http_paths: |
  - path: /
    backend:
      serviceName: {{boss__fluentd__centralized__shared_name}}
      servicePort: 5160

boss__fluentd__centralized__ingress_labels: |-
  k8s-app: {{boss__fluentd__centralized__shared_name}}
  version: v2.5.0
  kubernetes.io/cluster-service: "true"

boss__fluentd__centralized__ingress_annotations: |-
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__fluentd__centralized__deployment_spec_template_spec_hostNetwork: True
boss__fluentd__centralized__deployment_spec_template_spec_hostPID: True


boss__fluentd__centralized__deployment_spec_template_spec_nodeSelector: |-
  beta.kubernetes.io/os: linux

boss__fluentd__centralized__deployment_spec_template_spec_securityContext: "disabled"

#      hostNetwork: true
#      hostPID: true
#      nodeSelector:
#        beta.kubernetes.io/os: linux
#      securityContext:
#        runAsNonRoot: true
#        runAsUser: 65534

# [20:34:19,104] <inform-707> INFO inform - from [MAC ADDRESS](UniFi Security Gateway 3P, UGW3, 4.4.36.5146617): state=CONNECTED, last_inform=3, ext/stun_ip=192.168.1.1, dev_ip=192.168.0.3, up=4955230

##########################################################################################################
# .########...######..##....##..######..##........#######...######............######..########.##....##.########.########.....###....##.......####.########.########.########.
# .##.....##.##....##..##..##..##....##.##.......##.....##.##....##..........##....##.##.......###...##....##....##.....##...##.##...##........##.......##..##.......##.....##
# .##.....##.##.........####...##.......##.......##.....##.##................##.......##.......####..##....##....##.....##..##...##..##........##......##...##.......##.....##
# .########...######.....##.....######..##.......##.....##.##...####.#######.##.......######...##.##.##....##....########..##.....##.##........##.....##....######...##.....##
# .##...##.........##....##..........##.##.......##.....##.##....##..........##.......##.......##..####....##....##...##...#########.##........##....##.....##.......##.....##
# .##....##..##....##....##....##....##.##.......##.....##.##....##..........##....##.##.......##...###....##....##....##..##.....##.##........##...##......##.......##.....##
# .##.....##..######.....##.....######..########..#######...######............######..########.##....##....##....##.....##.##.....##.########.####.########.########.########.
##########################################################################################################
boss__rsyslog__centralized__subdomain: rsyslog-centralized
boss__rsyslog__centralized__manifest_path: "{{manifest_dir_path}}/rsyslog-centralized"
boss__rsyslog__centralized__namespace_name: kube-system
boss__rsyslog__centralized__rsyslog_version: 0.2.0
boss__rsyslog__centralized__rsyslog_image_repo: "bossjones/rsyslog"
boss__rsyslog__centralized__rsyslog_image_tag: "{{ boss__rsyslog__centralized__rsyslog_version }}"
boss__rsyslog__centralized__shared_name: rsyslog-centralized
boss__rsyslog__centralized__configmap_suffix: "-0-3-0"
boss__rsyslog__centralized__create_namespace: "absent"
boss__rsyslog__centralized__create_ingress: "present"
boss__rsyslog__centralized__loglevel: "debug"
boss__rsyslog__centralized__main_config_folder_prefix: "/rsyslog-centralized/etc"
boss__rsyslog__centralized__udp_in_port: 6160
boss__rsyslog__centralized__tcp_in_port: 6170
boss__rsyslog__centralized__relp_in_port: 1601
boss__rsyslog__centralized__path_to_logs_inside_container: "/log"
boss__rsyslog__centralized__path_to_config_inside_container: "/config"
boss__rsyslog__centralized__path_to_work_inside_container: "/work"
boss__rsyslog__centralized__path_to_spool_inside_container: "/spool"

boss__rsyslog__centralized__logrotate_version: 1.3
boss__rsyslog__centralized__logrotate_image_repo: "blacklabelops/logrotate"
boss__rsyslog__centralized__logrotate_image_tag: "{{ boss__rsyslog__centralized__logrotate_version }}"


################# Persistent volume claims - begin #########################

# persistent volume claim - rsyslog
boss__rsyslog__centralized__rsyslog_pvc_labels: |-
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: {{boss__rsyslog__centralized__shared_name}}

boss__rsyslog__centralized__rsyslog_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__rsyslog__centralized__rsyslog_pvc_spec_resources_requests_storage: "100Mi"

# persistent volume - rsyslog
boss__rsyslog__centralized__rsyslog_pv_labels: |-
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: {{boss__rsyslog__centralized__shared_name}}

boss__rsyslog__centralized__rsyslog_pv_spec_capacity_storage: 100Mi
boss__rsyslog__centralized__rsyslog_pv_spec_nfs: |-
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/rsyslog"
################# Persistent volume claims - end #########################


# NOTE: Test using this!
# NOTE: Test using this!
# NOTE: Test using this!
#  malcolm    feature-centralized-fluentd-aggregator {24}   bosslab-playbooks2   2.4.3  ~/dev/bossjones/bosslab-playbooks   syslog-netcat-test rsyslog-centralized.scarlettlab.com 6170
# Looks like a macOS
# <150>Apr 10 00:17:13 darktop.localdomain syslog-netcat-test[76775]: testing baby
# rsyslog-centralized.scarlettlab.com [192.168.205.10] 6170 open
#  malcolm    feature-centralized-fluentd-aggregator {24}   bosslab-playbooks2   2.4.3  ~/dev/bossjones/bosslab-playbooks   syslog-netcat-test-udp rsyslog-centralized.scarlettlab.com 6160


boss__rsyslog__centralized__deployment_spec_template_containers_resources: |
  limits:
    cpu: ".2"
    memory: "100Mi"
  requests:
    cpu: "0.05"
    memory: "10Mi"

boss__rsyslog__centralized__serviceAccountName: "{{boss__rsyslog__centralized__shared_name}}"
boss__rsyslog__centralized__deployment_spec_replicas: 3

# boss__rsyslog__centralized__deployment_spec_template_containers_command: []

boss__rsyslog__centralized__cluster_role_rules: |-
  - apiGroups:
    - ""
    resources:
    - "namespaces"
    - "pods"
    - "secrets"
    - "pods/log"
    - "deployments"
    - "batch"
    - "extensions"
    - "crontabs"
    verbs:
    - "get"
    - "watch"
    - "list"
    - "update"
    - "post"
  # DEBUG: CUSTOM FROM HERE ON OUT ---- 5/10/2019
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - volumeattachments
    verbs:
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - '*'
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - list
    - delete
  - apiGroups:
    - ""
    resources:
    - services
    - services/finalizers
    - endpoints
    verbs:
    - get
    - create
    - update
    - delete
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  # DEBUG: CUSTOM FROM HERE ON OUT END ---- 5/10/2019

boss__rsyslog__centralized__deployment_labels: |-
  k8s-app: {{boss__rsyslog__centralized__shared_name}}
  version: {{boss__rsyslog__centralized__rsyslog_version}}
  kubernetes.io/cluster-service: "true"

boss__rsyslog__centralized__deployment_spec_matchLabels: "disabled"

# boss__rsyslog__centralized__deployment_spec_matchLabels: |-
#   k8s-app: {{boss__rsyslog__centralized__shared_name}}
#   version: {{boss__rsyslog__centralized__rsyslog_version}}

boss__rsyslog__centralized__deployment_spec_template_metadata_labels: |-
  k8s-app: {{boss__rsyslog__centralized__shared_name}}
  version: {{boss__rsyslog__centralized__rsyslog_version}}
  kubernetes.io/cluster-service: "true"

boss__rsyslog__centralized__deployment_spec_template_metadata_annotations: "disabled"

boss__rsyslog__centralized__deployment_spec_template_containers_ports: |-
  - containerPort: {{boss__rsyslog__centralized__udp_in_port}}
    name: in-udp
    # this is TCP and not UDP on purpose
    protocol: UDP
  - containerPort: {{boss__rsyslog__centralized__tcp_in_port}}
    name: in-tcp
    protocol: TCP
  - containerPort: {{boss__rsyslog__centralized__relp_in_port}}
    name: in-relp
    protocol: TCP

boss__rsyslog__centralized__deployment_spec_template_initContainers: |-
  - name: init-rsyslog
    image: {{boss__rsyslog__centralized__rsyslog_image_repo}}:{{boss__rsyslog__centralized__rsyslog_image_tag}}
    command: [ "/bin/sh" ]
    args: ['-c', 'mkdir -p /var/spool/rsyslog; mkdir -p {{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs; install -o rsyslog /dev/null -m 640 /var/log/rsyslog-debug']

boss__rsyslog__centralized__deployment_spec_template_containers_env: |-
  # - name: RSYSLOG_DEBUG_FLAG
  #   value: "-d"
  - name: _IMTCP_PORT
    value: "{{boss__rsyslog__centralized__tcp_in_port}}"
  - name: _IMUDP_PORT
    value: "{{boss__rsyslog__centralized__udp_in_port}}"
  - name: _IMRELP_PORT
    value: "{{boss__rsyslog__centralized__relp_in_port}}"
  - name: RSYSLOG_CONF
    value: "{{boss__rsyslog__centralized__main_config_folder_prefix}}/rsyslog.conf"
  - name: _RSYSLOG_SPOOL_PATH
    value: "{{boss__rsyslog__centralized__path_to_spool_inside_container}}"
  - name: TZ
    value: "UTC"

  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

boss__rsyslog__centralized__deployment_spec_template_spec_volumeMounts: |
  # rsyslog work files; do not share with multiple instances
  - name: work
    mountPath: {{boss__rsyslog__centralized__path_to_work_inside_container}}
    readOnly: false
  # log file store (if used)
  - name: log
    mountPath: {{boss__rsyslog__centralized__path_to_logs_inside_container}}
    readOnly: false

  - name: spool
    mountPath: {{boss__rsyslog__centralized__path_to_spool_inside_container}}
    readOnly: false

  # configuration files, can be mounted read-only once finalized
  - name: container-config
    mountPath: {{boss__rsyslog__centralized__path_to_config_inside_container}}
    readOnly: false

  # The Rsyslog config file to use
  - name: rsyslog-centralized-config
    mountPath: {{boss__rsyslog__centralized__main_config_folder_prefix}}
    readOnly: false

  - name: rsyslog-centralized-config-dot-d
    mountPath: {{boss__rsyslog__centralized__main_config_folder_prefix}}/rsyslog.d
    readOnly: false

  # - name: logrotate
  #   mountPath: /logrotate.d
  #   readOnly: false

  # - name: localtime
  #   mountPath: /etc/localtime
  #   readOnly: true

  # - name: {{boss__rsyslog__centralized__shared_name}}-pvc
  #   mountPath: {{boss__rsyslog__centralized__path_to_logs_inside_container}}
  #   readOnly: false

  # - mountPath: /etc/crontab
  #   name: datafeed-config

# SOURCE: https://github.com/instantlinux/docker-tools/blob/f4ce5659047cbf258908f8a090efa1a49688c337/images/rsyslogd/docker-compose.yml
# volumes:
# - ${ADMIN_PATH:-/opt}/rsyslogd/etc/logrotate.d:/etc/logrotate.d:ro
# - ${ADMIN_PATH:-/opt}/rsyslogd/etc/rsyslog.d:/etc/rsyslog.d:ro
# - logs:/var/log

boss__rsyslog__centralized__deployment_spec_template_spec_volumes: |
  - name: work
    emptyDir: {}
  - name: spool
    emptyDir: {}
  # log file store (if used)
  - name: log
    # emptyDir: {}
    persistentVolumeClaim:
      claimName: {{boss__rsyslog__centralized__shared_name}}-pvc
  - name: container-config
    configMap:
      name: {{boss__rsyslog__centralized__shared_name}}-container-config{{boss__rsyslog__centralized__configmap_suffix}}
      # defaultMode: 0775
  - name: rsyslog-centralized-config
    configMap:
      name: {{boss__rsyslog__centralized__shared_name}}-config{{boss__rsyslog__centralized__configmap_suffix}}
      # defaultMode: 0775

  - name: rsyslog-centralized-config-dot-d
    configMap:
      name: {{boss__rsyslog__centralized__shared_name}}-config-dot-d{{boss__rsyslog__centralized__configmap_suffix}}
      # defaultMode: 0775

  # - name: localtime
  #   hostPath:
  #     path: /etc/localtime
  #     type: "FileOrCreate"

  # - name: logrotate
  #   configMap:
  #     name: {{boss__rsyslog__centralized__shared_name}}-logrotate{{boss__rsyslog__centralized__configmap_suffix}}
  #     # defaultMode: 0775


  # - name: {{boss__rsyslog__centralized__shared_name}}-pvc
  # - name: {{boss__rsyslog__centralized__shared_name}}-pvc
  #   persistentVolumeClaim:
  #     claimName: {{boss__rsyslog__centralized__shared_name}}-pvc


  # - name: crontab
  #   configMap:
  #     name: {{boss__rsyslog__centralized__shared_name}}-crontab{{boss__rsyslog__centralized__configmap_suffix}}
  #     defaultMode: 0775

boss__rsyslog__centralized__deployment_spec_template_spec_sidcar_container: |-
  - name: logrotate
    image: {{boss__rsyslog__centralized__logrotate_image_repo}}:{{boss__rsyslog__centralized__logrotate_image_tag}}
    env:
    - name: LOG_DIRECTORIES
      value: "{{boss__rsyslog__centralized__path_to_logs_inside_container}}"
    - name: LOGROTATE_INTERVAL
      value: "hourly"
    - name: LOGROTATE_SIZE
      value: "50M"
    # - name: LOGROTATE_STATUSFILE
    #   value: "50M"
    # - name: LOGROTATE_CRONSCHEDULE
    #   value: "* * * * * *"
    - name: LOGROTATE_POSTROTATE_COMMAND
      value: "/bin/kill -HUP `ps aux | grep rsyslog.conf | cut -d ' ' -f1` 2> /dev/null || true"
    - name: TZ
      value: "UTC"
    # v: Verbose
    # d: Debug, Logrotate will be emulated but never executed!
    # f: Force
    # - name: LOGROTATE_PARAMETERS
    #   value: "v"
    volumeMounts:
    - name: log
      mountPath: {{boss__rsyslog__centralized__path_to_logs_inside_container}}

boss__rsyslog__centralized__service_labels: |-
  k8s-app: {{boss__rsyslog__centralized__shared_name}}
  version: {{boss__rsyslog__centralized__rsyslog_version}}
  kubernetes.io/cluster-service: "true"


boss__rsyslog__centralized__service_annotations: "disabled"

boss__rsyslog__centralized__service_spec_type: ClusterIP

boss__rsyslog__centralized__service_spec_ports: |-
  - name: in-udp
    port: {{boss__rsyslog__centralized__udp_in_port}}
    protocol: UDP
    targetPort: {{boss__rsyslog__centralized__udp_in_port}}
  - name: in-tcp
    port: {{boss__rsyslog__centralized__tcp_in_port}}
    protocol: TCP
    targetPort: {{boss__rsyslog__centralized__tcp_in_port}}
  - name: in-relp
    port: {{boss__rsyslog__centralized__relp_in_port}}
    protocol: TCP
    targetPort: {{boss__rsyslog__centralized__relp_in_port}}

boss__rsyslog__centralized__service_spec_selector: |-
  k8s-app: {{boss__rsyslog__centralized__shared_name}}

boss__rsyslog__centralized__ingress_spec_rules_http_paths: |
  - path: /
    backend:
      serviceName: {{boss__rsyslog__centralized__shared_name}}
      servicePort: {{boss__rsyslog__centralized__udp_in_port}}

boss__rsyslog__centralized__ingress_labels: |-
  k8s-app: {{boss__rsyslog__centralized__shared_name}}
  version: {{boss__rsyslog__centralized__rsyslog_version}}
  kubernetes.io/cluster-service: "true"

boss__rsyslog__centralized__ingress_annotations: |-
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__rsyslog__centralized__deployment_spec_template_spec_hostNetwork: True
boss__rsyslog__centralized__deployment_spec_template_spec_hostPID: True


boss__rsyslog__centralized__deployment_spec_template_spec_nodeSelector: |-
  beta.kubernetes.io/os: linux

# boss__rsyslog__centralized__deployment_spec_template_spec_securityContext: "disabled"
boss__rsyslog__centralized__deployment_spec_template_spec_securityContext: |-
  # SOURCE: https://docs.okd.io/latest/rest_api/apis-apps/v1beta2.Deployment.html
  privileged: true


boss__rsyslog__centralized__deployment_spec_template_spec_containers_livenessProbe: |-
  tcpSocket:
    port: {{boss__rsyslog__centralized__tcp_in_port}}
  initialDelaySeconds: 60
  timeoutSeconds: 10

boss__rsyslog__centralized__rsyslog_conf_config_map: |-
    # Quick overview of message flow and objects
    # msg sent -> input module -> ruleset(conditionally applied) -> rule match -> action (eg. write it to a file, database or forward it to remote host )

    # ORDER OF OPS: https://github.com/lilgreenwein/rsyslog-examples/blob/a546ad2c62b478bf57b004b5b9c522251686e6cf/rsyslog.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/globals/*.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/modules/*.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/templates/*.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/inputs/*.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/rules/*.conf
    # $IncludeConfig /etc/rsyslog/rsyslog.d/*.conf

    ###########################
    #### GLOBAL DIRECTIVES ####
    ###########################
    # SOURCE: https://github.com/rsyslog/rsyslog-docker/blob/master/appliance/alpine/rsyslog.conf
    # SOURCE: https://github.com/rsyslog/rsyslog-pkg-alpine/blob/master/daily/rsyslog/rsyslog.conf

    #
    # Use traditional timestamp format.
    # To enable high precision timestamps, comment out the following line.
    #
    $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
    # Set default permissions for all log files
    $FileOwner root
    $FileGroup adm
    $FileCreateMode 0640
    $DirCreateMode 0755
    $Umask 0022

    # Reduce repeating messages (default off)
    $RepeatedMsgReduction on

    $CreateDirs on

    # global(processInternalMessages="on")
    # SOURCE: https://itnext.io/metrics-from-kubernetes-logs-82cb1dcb3551
    global(parser.permitSlashInProgramName="on")
    # default location for work (spool) files
    # Raise limits within /etc/systemd/journald.conf on the host(s) - ie., RateLimitIntervalSec=30s + RateLimitBurst=1000000
    # global(workDirectory="/var/spool/rsyslog")

    #
    # where to place spool files
    #
    # global(workDirectory=`echo $_RSYSLOG_SPOOL_PATH`)
    global(
      workDirectory="{{boss__rsyslog__centralized__path_to_spool_inside_container}}"
      preserveFQDN                        = "on"
      action.reportSuspension             = "on"
      action.reportSuspensionContinuation = "on"
      senders.keepTrack                   = "on"
      senders.timeoutAfter                = "86400"
      senders.reportGoneAway              = "on"
      senders.reportNew                   = "on"
    )

    # Options for rsyslogd
    # -x disables DNS lookups for remote messages
    # See rsyslogd(8) for more details

    # LogFuncFlow - print out the logical flow of functions (entering and exiting them)

    # FileTrace - specifies which files to trace LogFuncFlow. If not set (the default), a LogFuncFlow trace is provided for all files. Set to limit it to the files specified. FileTrace may be specified multiple times, one file each (e.g. export RSYSLOG_DEBUG="LogFuncFlow FileTrace=vm.c FileTrace=expr.c"

    # PrintFuncDB - print the content of the debug function database whenever debug information is printed (e.g. abort case)!

    # PrintAllDebugInfoOnExit - print all debug information immediately before rsyslogd exits (currently not implemented!)

    # PrintMutexAction - print mutex action as it happens. Useful for finding deadlocks and such.

    # NoLogTimeStamp - do not prefix log lines with a timestamp (default is to do that).

    # NoStdOut - do not emit debug messages to stdout. If RSYSLOG_DEBUGLOG is not set, this means no messages will be displayed at all.

    # Debug - if present, turns on the debug system and enables debug output

    # DebugOnDemand - if present, turns on the debug system but does not enable debug output itself. You need to send SIGUSR1 to turn it on when desired.

    # OutputTidToStderr - if present, makes rsyslog output information about the thread id (tid) of newly created processes to stderr. Note that not necessarily all new threads are reported (depends on the code, e.g. of plugins). This is only available under Linux. This usually does NOT work when privileges have been dropped (that's not a bug, but the way it is). help - display a very short list of commands - hopefully a life saver if you can't access the documentation
    # $DebugFile /tmp/rsyslogd.debug.log
    # $DebugLevel 2

    # FIXME: Enable this 4/10/2019
    # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # ***********************************************************************
    # Setup Performance tuning
    # ***********************************************************************
    main_queue(
      queue.size="1000000" # Size of Queue
      queue.debatchsize="1000" # process messages in batches
      queue.workerthreads="2" # 2 threads for the queue
    )
    # bash-4.4# rsyslogd -v
    # rsyslogd 8.36.0, compiled with:
    #   PLATFORM:				x86_64-alpine-linux-musl
    #   PLATFORM (lsb_release -d):
    #   FEATURE_REGEXP:				Yes
    #   GSSAPI Kerberos 5 support:		No
    #   FEATURE_DEBUG (debug build, slow code):	No
    #   32bit Atomic operations supported:	Yes
    #   64bit Atomic operations supported:	Yes
    #   memory allocator:			system default
    #   Runtime Instrumentation (slow code):	No
    #   uuid support:				Yes
    #   systemd support:			No
    #   Number of Bits in RainerScript integers: 64

    # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # ***********************************************************************
    #################
    #### MODULES ####
    #################
    # ***********************************************************************
    # module(load="imjournal" ignorepreviousmessages="on" ratelimit.interval="60" ratelimit.burst="2000000" persiststateinterval="10000" statefile="/var/spool/rsyslog/imjournal.state")
    #module(load="imtcp" StreamDriver.AuthMode="anon" StreamDriver.Mode="1")
    # config.enabled=`echo $ENABLE_STATISTICS`)
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    # If you do not load inputs, nothing happens!
    # You may need to set the module load path if modules are not found.
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    module(load="impstats" interval="600")
    module(load="imrelp")
    module(load="imptcp" MaxSessions="5000")
    # module(load="imptcp")
    module(load="imudp" TimeRequery="500")
    module(load="omstdout")
    # module(load="builtin:omfile")
    # Default parameters for file output. Old-style global settings are not working with new-style actions
    module(load="builtin:omfile" FileOwner="root" FileGroup="adm" dirOwner="root" dirGroup="adm" FileCreateMode="0640" DirCreateMode="0755")
    module(load="omelasticsearch")
    module(load="mmjsonparse")
    module(load="mmutf8fix")
    # module(load="imuxsock")
    # module(load="imklog")
    # module(load="imfile" PollingInterval="10")
    # module(
    #     load = "mmfields"
    # )


    ###########################
    #### TEMPLATES         ####
    ###########################
    # Templates(SOURCE: https://linux-help.org/wiki/logging/rsyslog/advanced-rsyslog)
    # template(name="RemoteHost" type="string" string="/srv/log/%HOSTNAME%/%$YEAR%/%$MONTH%/syslog-%$DAY%.log")

    # DISABLED: # $IncludeConfig /rsyslog-centralized/etc/rsyslog.conf.d/*.template

    # includes done explicitely
    # include(file="/rsyslog-centralized/etc/rsyslog.conf.d/log_to_logsene.conf" config.enabled=`echo $ENABLE_LOGSENE`)
    # DISABLED: # include(file="/rsyslog-centralized/etc/rsyslog.conf.d/log_to_files.conf" config.enabled=`echo $ENABLE_LOGFILES`)
    # include(file="/rsyslog-centralized/etc/rsyslog.conf.d/log_to_stdout.conf" config.enabled=`echo $ENABLE_STDOUT`)


    # DISABLED: # $template RemoteHost, "/srv/log/%HOSTNAME%/%$YEAR%/%$MONTH%/syslog-%$DAY%.log"
    # DISABLED: # $template TmplMsg, "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/%HOSTNAME%/%PROGRAMNAME%.log"

    #########################################################################
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    # SOURCE: https://github.com/lilgreenwein/rsyslog-examples/blob/a546ad2c62b478bf57b004b5b9c522251686e6cf/rsyslog.d/templates/01_local.conf
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    #########################################################################
    template(
        name   = "LOCAL_MessagesFileTemplate"
        type   = "string"
        string = "/var/log/messages.log"
    )
    template(
        name   = "LOCAL_SecureFileTemplate"
        type   = "string"
        string = "/var/log/secure"
    )
    template(
        name   = "LOCAL_MailFileTemplate"
        type   = "string"
        string = "/var/log/maillog"
    )
    template(
        name   = "LOCAL_CronFileTemplate"
        type   = "string"
        string = "/var/log/cron"
    )
    template(
        name   = "LOCAL_SpoolerFileTemplate"
        type   = "string"
        string = "/var/log/spooler"
    )
    template(
        name   = "LOCAL_BootFileTemplate"
        type   = "string"
        string = "/var/log/boot.log"
    )
    template(
        name   = "LOCAL_SyslogFileTemplate"
        type   = "string"
        string = "/var/log/rsyslogd.log"
    )
    template(
        name   = "LOCAL_UncategorizedFileTemplate"
        type   = "string"
        string = "/var/log/uncategorized"
    )
    template(
        name   = "LOCAL_FacilitySeverityFileTemplate"
        type   = "list"
    ) {
        constant(value = "/var/log/")
        property(name = "syslogfacility-text")
        constant(value = ".")
        property(name = "syslogseverity-text")
        constant(value = ".log")
    }

    #########################################################################
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    # SOURCE: https://github.com/lilgreenwein/rsyslog-examples/blob/a546ad2c62b478bf57b004b5b9c522251686e6cf/rsyslog.d/templates/02_remote.conf
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    #########################################################################
    template(
        name = "REMOTE_MessagesFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/messages.log")
    }
    template(
        name = "REMOTE_SecureFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/secure")
    }
    template(
        name = "REMOTE_MailFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/maillog")
    }
    template(
        name = "REMOTE_CronFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/cron")
    }
    template(
        name = "REMOTE_SpoolerFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/spooler")
    }
    template(
        name = "REMOTE_BootFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/boot.log")
    }
    template(
        name = "REMOTE_SyslogFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/rsyslogd.log")
    }
    template(
        name = "REMOTE_UncategorizedFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/uncategorized")
    }
    template(
        name = "REMOTE_FacilitySeverityFileTemplate"
        type = "list"
    ) {
        constant(value = "{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/")
        property(name  = "fromhost")
        constant(value = "/")
        property(name  = "syslogfacility-text")
        constant(value = ".")
        property(name  = "syslogseverity-text")
        constant(value = ".log")
    }

    ###########################
    #### INPUTS            ####
    ###########################
    # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # ***********************************************************************
    # Setup listeners
    # ***********************************************************************
    # $UDPServerAddress 0.0.0.0
    # $UDPServerRun 6160
    input(type="imudp"
      Address="0.0.0.0"
      RateLimit.Interval="0"
      port=`echo $_IMUDP_PORT`)

    input(type="imptcp"
      Address="0.0.0.0"
      port=`echo $_IMTCP_PORT`
    )
    # ruleset="remote"
    # input(type="imptcp"
    #   port=`echo $_IMTCP_PORT`)

    input(type="imrelp"
      Address="0.0.0.0"
      port=`echo $_IMRELP_PORT`)
    # input(type="imrelp"
    # port=`echo $_IMRELP_PORT`)

    # SOURCE: https://github.com/PLOS-Formulas/ubiquiti-formula/blob/71305bcb06117953cd7b8ba6babbdd051aa44fea/ubiquiti/conf/etc/rsyslog.d/ubiquiti.conf
    # input(type="imfile"
    #   File="/log/unifi/server.log"
    #   Tag="unificontroller"
    #   StateFile="unifi_server_log"
    #   Facility="local0"
    #   Severity="info")

    # $MaxMessageSize 64k
    # $ActionQueueFileName fwdRule1     # unique name prefix for spool files
    # $ActionQueueMaxDiskSpace 1g       # 1gb space limit (use as much as possible)
    # $ActionQueueSaveOnShutdown on     # save messages to disk on shutdown
    # $ActionQueueType LinkedList       # run asynchronously
    # $ActionResumeRetryCount -1        # infinite retries if host is down

    # SOURCE: https://github.com/dcv-cloud/dcv-kubernetes/blob/50221c0fb3ee7b2f799930b336662ab1d896f871/kubeManifests/infra-repo-197/datafeed/addConfigMapDatafeed.yaml
    # *********************************************************
    #### GLOBAL DIRECTIVES ####
    # Where to place auxiliary files
    # $WorkDirectory /var/lib/rsyslog
    # Use default timestamp format
    # $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
    # Include all config files in /etc/rsyslog.d/
    # $IncludeConfig /etc/rsyslog.d/*.conf
    # Turn off message reception via local log socket;
    # local messages are retrieved through imjournal now.
    # $OmitLocalLogging on
    # File to store the position in the journal
    # $IMJournalStateFile imjournal.state
    # *********************************************************

    # $InputFilePollInterval 10
    # template(
    #   name="RemoteHostUnifiServer"
    #   type="string"
    #   string="/log/unifiserver.log"
    # )

    # $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

    # *********************************************************************************************************
    # SOURCE: https://github.com/rsyslog/rsyslog/issues/2040
    # *********************************************************************************************************
    # cat /etc/rsyslog.d/udp-nonlocaladdr-bug.conf
    # module(load="imudp")

    # input(type="imudp" address="127.0.0.1" port="514" RateLimit.Interval="0" ruleset="logtest")

    # input(type="imudp" address="192.168.99.99" port="514" RateLimit.Interval="0" ruleset="logtest")

    # ruleset(name="logtest") {
    #     action(type="omfile" file="/tmp/logtest")
    # }
    # *********************************************************************************************************

    # FIXME: Enable this 4/10/2019
    # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # ***********************************************************************
    # Setup filters
    # ***********************************************************************
    # SOURCE: https://itnext.io/metrics-from-kubernetes-logs-82cb1dcb3551
    # ***************************************************************************************
    # set $!custom_hostname = exec_template("hostname");
    # # When empty it's because message does not come from journald but directly from rsyslogd
    # if $!custom_hostname == "" then {
    #   set $!custom_hostname = "FROM-RSYSLOGD";
    # }
    # ***************************************************************************************

    # ***************************************************************************************
    # SOURCE: https://github.com/instantlinux/docker-tools/blob/f4ce5659047cbf258908f8a090efa1a49688c337/images/rsyslogd/kubernetes.yaml
    # SOURCE: https://www.rsyslog.com/doc/v8-stable/configuration/filters.html
    # ***************************************************************************************
    # :msg, regex, "Connection closed by 192.168.1.[0-9]\\{1,3\\} \\[preauth\\]" stop
    # :msg, contains, "Error: Request packet type/version was invalid" stop
    # :msg, contains, "Client request was invalid, bailing out..." stop
    # :msg, contains, "required revision has been compacted" stop
    # ***************************************************************************************

    # # FIXME: Enable this 4/10/2019
    # # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # # ***********************************************************************
    # # Setup templates
    # # ***********************************************************************
    # template(name="d_catch_all" type="string" string="/var/log/remote_syslog/catch_all/%FROMHOST%/%$YEAR%-%$MONTH%-%$DAY%.log")
    # template(name="d_firewall_log" type="string" string="/var/log/remote_syslog/firewall/%FROMHOST%/%$YEAR%-%$MONTH%-%$DAY%.log")
    # SOURCE: https://github.com/dcv-cloud/dcv-kubernetes/blob/50221c0fb3ee7b2f799930b336662ab1d896f871/kubeManifests/infra-repo-197/datafeed/addConfigMapDatafeed.yaml
    # template(name="dcloud-metrics" type="list"){
    #   constant(value="{\"msg\":")
    #   property(name="msg")
    #   constant(value=",")
    #   constant(value="\"timereported\":\"")
    #   property(name="timereported" dateFormat="rfc3339" caseConversion="lower")
    #   constant(value="\",")
    #   constant(value="\"hostname\":\"")
    #   property(name="hostname")
    #   constant(value="\",")
    #   constant(value="\"fromhost\":\"")
    #   property(name="fromhost")
    #   constant(value="\",")
    #   constant(value="\"fromhost-ip\":\"")
    #   property(name="fromhost-ip")
    #   constant(value="\",")
    #   constant(value="\"pri\":\"")
    #   property(name="pri")
    #   constant(value="\",")
    #   constant(value="\"syslogseverity\":\"")
    #   property(name="syslogseverity")
    #   constant(value="\",")
    #   constant(value="\"timegenerated\":\"")
    #   property(name="timegenerated")
    #   constant(value="\",")
    #   constant(value="\"programname\":\"")
    #   property(name="programname")
    #   constant(value="\",")
    #   constant(value="\"app-name\":\"")
    #   property(name="app-name")
    #   constant(value="\"}")
    #   constant(value="\n")
    # }

    # # FIXME: Enable this 4/10/2019
    # # INFO: https://www.aplura.com/wp-content/uploads/2017/08/syslog_cheatsheet.pdf
    # # ***********************************************************************
    # # Setup output
    # # ***********************************************************************
    # ruleset(name="f_remote_all" queue.type="LinkedList" queue.size="100000") {
    # if ($fromhost-ip startswith '192.168.100.') then {
    #  action(type="omfile" DynaFile="d_firewall_log")
    #  stop
    # }
    #   # Lets setup the catch all logging
    #   action(type="omfile" DynaFile="d_catch_all")
    # }
    # ruleset(name="f_remote_all") {
    #  action(type="omfile" DynaFile="d_catch_all")
    # }
    # ruleset(name="f_firewall") {
    #  action(type="omfile" DynaFile="d_firewall_log")
    # }

    #################### default ruleset begins ####################
    ###########################
    #### RULES            ####
    ###########################

    #########################################################################
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    # SOURCE: https://github.com/lilgreenwein/rsyslog-examples/blob/a546ad2c62b478bf57b004b5b9c522251686e6cf/rsyslog.d/rules/99_fallback.conf
    # <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
    #########################################################################
    if not $fromhost-ip or $fromhost-ip startswith "127." then {
        if prifilt("*.info;mail.none;authpriv.none;cron.none") then {
            # action(
            #     name     = "LOCAL_MessagesToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_MessagesFileTemplate"
            #     sync     = "on"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )
            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("syslog.*") then {
            # action(
            #     name     = "LOCAL_SyslogToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_SyslogFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("authpriv.*") then {
            # action(
            #     name     = "LOCAL_SecureToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_SecureFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("mail.*") then {
            # action(
            #     name     = "LOCAL_MailToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_MailFileTemplate"
            #     sync     = "on"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("cron.*") then {
            # action(
            #     name     = "LOCAL_CronToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_CronFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("uucp,news.crit") then {
            # action(
            #     name     = "LOCAL_SpoolToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_SpoolerFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("local7.*") then {
            # action(
            #     name     = "LOCAL_BootToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_BootFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("*.emerg") then {
            # action(
            #     name     = "LOCAL_EmergToFile"
            #     type     = "omusrmsg"
            #     users    = "*"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("*.*") then {
            # action(
            #     name     = "LOCAL_UncategorizedToFile"
            #     type     = "omfile"
            #     dynaFile = "LOCAL_UncategorizedFileTemplate"
            #     template = "RSYSLOG_TraditionalFileFormat"
            # )
            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
    } else {
        if prifilt("*.info;mail.none;authpriv.none;cron.none") then {
            action(
                name     = "REMOTE_MessagesToFile"
                type     = "omfile"
                dynaFile = "REMOTE_MessagesFileTemplate"
                sync     = "on"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("syslog.*") then {
            action(
                name     = "LOCAL_SyslogToFile"
                type     = "omfile"
                dynaFile = "REMOTE_SyslogFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("authpriv.*") then {
            action(
                name     = "REMOTE_SecureToFile"
                type     = "omfile"
                dynaFile = "REMOTE_SecureFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("mail.*") then {
            action(
                name     = "REMOTE_MailToFile"
                type     = "omfile"
                dynaFile = "REMOTE_MailFileTemplate"
                sync     = "on"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("cron.*") then {
            action(
                name     = "REMOTE_CronToFile"
                type     = "omfile"
                dynaFile = "REMOTE_CronFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("uucp,news.crit") then {
            action(
                name     = "REMOTE_SpoolToFile"
                type     = "omfile"
                dynaFile = "REMOTE_SpoolerFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("local7.*") then {
            action(
                name     = "REMOTE_BootToFile"
                type     = "omfile"
                dynaFile = "REMOTE_BootFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
        if prifilt("*.*") then {
            action(
                name     = "REMOTE_UncategorizedToFile"
                type     = "omfile"
                dynaFile = "REMOTE_UncategorizedFileTemplate"
                template = "RSYSLOG_TraditionalFileFormat"
            )

            action(type="omstdout"
                   template="RSYSLOG_TraditionalFileFormat")
            stop
        }
    }

    # DISABLED: 4/15/2019    ruleset(name="catchall") {
    # DISABLED: 4/15/2019      *.* :omstdout:
    # DISABLED: 4/15/2019    }
    # DISABLED: 4/15/2019
    # DISABLED: 4/15/2019    $DefaultRuleset catchall
    # DISABLED: 4/15/2019
    # DISABLED: 4/15/2019    ruleset(name="remote") {
    # DISABLED: 4/15/2019        $IncludeConfig /etc/rsyslog.d/*.remote
    # DISABLED: 4/15/2019
    # DISABLED: 4/15/2019        action(type="omfile" DynaFile="RemoteHost")
    # DISABLED: 4/15/2019    }

    # input(type="imudp" port=`echo $_IMUDP_PORT` ruleset="remote")
    # input(type="imtcp" port=`echo $_IMTCP_PORT` ruleset="remote")


    # we emit our own messages to docker console:
    # syslog.* :omstdout:
    # *.* :omstdout:
    # action(type="omstdout")

    include(file="/config/droprules.conf" mode="optional")  # this permits the user to easily drop unwanted messages

    action(name="main_utf8fix" type="mmutf8fix" replacementChar="?")

    # include(text=`echo $CNF_CALL_LOG_TO_LOGFILES`)
    # include(text=`echo $CNF_CALL_LOG_TO_LOGSENE`)
    # include(text=`echo $CNF_CALL_LOG_TO_STDOUT`)
    # include(text=`echo $_IMTCP_PORT`)
    # include(text=`echo $_IMUDP_PORT`)
    # include(text=`echo $_IMRELP_PORT`)

    # SOURCE: https://itnext.io/metrics-from-kubernetes-logs-82cb1dcb3551
    # ***************************************************************************************
    # Compose RFC5424 message
    # template(name="rfc5424" type="list") {
    #   constant(value="<")
    #   property(name="PRI")
    #   constant(value=">1 ")
    #   property(name="TIMESTAMP" dateFormat="rfc3339" date.inUTC="on")
    #   constant(value=" ")
    #   property(name="!custom_hostname" position.from="1" position.to="255" caseConversion="lower")
    #   constant(value=" ")
    #   property(name="!custom_appname" position.from="1" position.to="48" caseConversion="lower")
    #   constant(value=" ")
    #   property(name="PROCID" position.from="1" position.to="128")
    #   constant(value=" ")
    #   property(name="MSGID" position.from="1" position.to="32")
    #   constant(value=" ")
    #   property(name="!custom_sddata")
    #   constant(value=" ")
    #   property(name="msg" droplastlf="on")
    #   constant(value="\n")
    # }
    # action(type="mmutf8fix")
    # action(type="omfwd"
    #   target="127.0.0.1"
    #   port="6514"
    #   protocol="tcp"
    #   tcp_framing="octet-counted"
    #   template="rfc5424"
    #   queue.type="LinkedList"
    #   queue.size="5000000"
    #   queue.filename="forwarding"
    #   queue.maxdiskspace="1g")
    # ***************************************************************************************
    # SOURCE: https://selivan.github.io/2017/02/07/rsyslog-log-forward-save-filename-handle-multi-line-failover.html
    # Write all messages of auth and authpriv facilities into file /var/log/auth.log and continue processing this messages
    # modern
    # SOURCE: https://bsamuels.net/2014/08/30/redirecting-rsyslog-output.html
    # if ( $fromhost == "UniFiSecurityGateway3P" ) then {
    #     action(type="omfile" file="/log/unifi.log")
    #     action(type="omfile" file="/log/unifi.log")
    #     stop
    # }

    # if ($msg contains "UniFiSecurityGateway3P") then {
    #   action(type="omfile" file="/log/unifi.log")
    #   # action(type="omfile" DynaFile="RemoteHostCronLog")
    #   # action(type="omfile" DynaFile="RemoteServiceCronLog")
    #   stop
    # }

    # *.* action(type="omfile" File="/log/rsyslog_messages.log" template="dcloud-metrics")
    #

# {{boss__rsyslog__centralized__path_to_logs_inside_container}}
boss__rsyslog__centralized__container_config_config_map: |-
  # rsyslog syslog appliance container configuration.
  # This file will be sourced upon container startup.
  # Uncomment those settings that you need and set them to your
  # desired values.

  tree {{boss__rsyslog__centralized__path_to_logs_inside_container}}

  # general container app settings:
  export TZ=UTC
  #export CONTAINER_SILENT=on  # do not emit startup message
  export ENABLE_STATISTICS=on

  # Do we write log files?
  # yes, we do (comment out to disable)
  export ENABLE_LOGFILES=on
  # Where do we write to?
  # path for host-specific files is: {{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/HOSTNAME
  # export LOGFILES_STORE="{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/%hostname:::secpath-replace%/messages.log"
  # you can of course overwrite this. For example, the below definition
  # uses the program name instead of a fixed name "messages.log". That means
  # for each host, a separate file for each program will be generated.
  export LOGFILES_STORE="{{boss__rsyslog__centralized__path_to_logs_inside_container}}/client_logs/%hostname:::secpath-replace%/%programname:::secpath-replace%.log"

  # If you have an account with Logsene, enter your access
  # information below:
  #export LOGSENE_TOKEN=
  #export LOGSENE_URL=logsene-receiver.eu.sematext.com

  # export CNF_CALL_LOG_TO_STDOUT="off"

  # Settings for debugging the container
  #export USE_VALGRIND=on
  # export RSYSLOG_DEBUG="debug nostdout"
  # export RSYSLOG_DEBUG="debug DebugOnDemand LogFuncFlow FileTrace=imptcp.c FileTrace=impudp.c FileTrace=rsyslogd.c FileTrace=action.c"
  # export RSYSLOG_DEBUGLOG="/log/rsyslog-internal-debug.log"

  export _IMTCP_PORT=${_IMTCP_PORT}
  export _IMUDP_PORT=${_IMUDP_PORT}
  export _IMRELP_PORT=${_IMRELP_PORT}
  export _RSYSLOG_SPOOL_PATH=${_RSYSLOG_SPOOL_PATH:-/var/spool/rsyslog}

# [20:34:19,104] <inform-707> INFO inform - from [MAC ADDRESS](UniFi Security Gateway 3P, UGW3, 4.4.36.5146617): state=CONNECTED, last_inform=3, ext/stun_ip=192.168.1.1, dev_ip=192.168.0.3, up=4955230

vagrant_playbook_enable_role_tools: "enabled"
vagrant_playbook_enable_role_bootstrap: "enabled"
vagrant_playbook_enable_role_nfs_master: "enabled"
vagrant_playbook_enable_role_nfs_client: "enabled"
vagrant_playbook_enable_role_rsyslogd: "enabled"
vagrant_playbook_enable_role_journald: "enabled"
vagrant_playbook_enable_role_core: "enabled"
vagrant_playbook_enable_role_update_hosts: "enabled"
vagrant_playbook_enable_role_fact: "enabled"
vagrant_playbook_enable_role_environment: "enabled"
vagrant_playbook_enable_role_etckeeper: "enabled"
vagrant_playbook_enable_role_timezone: "enabled"
vagrant_playbook_enable_role_ntp: "enabled"
vagrant_playbook_enable_role_debug: "enabled"
vagrant_playbook_enable_role_logrotate: "disabled"
vagrant_playbook_enable_role_netdata: "disabled"


# Template playbooks
vagrant_tools_roles: |
  - role: boss-ansible-role-tools
    task: tools
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: kube
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: docker
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: glances
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools
  - role: boss-ansible-role-tools
    task: rust
    timezone: 'UTC'
    bossjones__user: vagrant
    bossjones__group: vagrant
    boss__tools__install_docker_tools: True
    boss__tools__install_kube_tools: True
    boss__tools__install_rust_tools: False
    tags:
      - boss-ansible-role-tools

vagrant_tools_all_roles: |
    - role: boss-ansible-role-tools
      task: tools
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: kube
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: docker
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: glances
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: rust
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools
    - role: boss-ansible-role-tools
      task: npm
      timezone: 'UTC'
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: False
      tags:
        - boss-ansible-role-tools

vagrant_playbook_role_core_data: |
    - role: boss-ansible-role-core
      tags:
        - boss-ansible-role-core

vagrant_playbook_role_update_hosts_data: |
    - role: boss-ansible-role-update-hosts
      boss__update__hosts__hosts_file: /etc/hosts
      # ansible group to use when finding ip addresses
      boss__update__hosts__ansible_group: "servers"

      boss__update__hosts__networking_interface: "{{main_network_interface}}" # {{main_network_interface}} (if vagrant)

      ### NEW vars
      # SOURCE: https://github.com/bertvv/ansible-role-hosts/blob/master/defaults/main.yml

      boss__update__hosts__hosts_playbook_version: "1.0.1"

      # If set to true, an entry for `ansible_hostname`, bound to the host's default IPv4 address is added added.
      boss__update__hosts__hosts_add_default_ipv4: true

      # If set to true, basic IPv6 entries (localhost6, ip6-localnet, etc) are added.
      boss__update__hosts__hosts_add_basic_ipv6: true

      # If set to true, an entry for every host managed by Ansible is added. Remark that this makes `boss__update__hosts__hosts_add_default_ipv4` unnecessary, as it will be added as wel by this setting.
      boss__update__hosts__hosts_add_ansible_managed_hosts: true

      # Select specific groups of Ansible managed hosts to be added in the hosts file.
      boss__update__hosts__hosts_add_ansible_managed_hosts_groups: ['servers']

      # Custom hosts entries to be added
      boss__update__hosts__hosts_entries: []

      # Custom host file snippets to be added
      boss__update__hosts__hosts_file_snippets: []

      # IP protocol to use
      boss__update__hosts__hosts_ip_protocol: 'ipv4'

      # Network interface to use
      boss__update__hosts__hosts_network_interface: "{{ boss__update__hosts__networking_interface }}"

      # convenience variable that has ansible_ as part of name for dynamic loading
      boss__update__hosts__hosts_ansible_network_interface: "ansible_{{ boss__update__hosts__networking_interface }}"

      # Backup of previous host
      boss__update__hosts__host_file_backup: yes

      # Use old 'override' style or new 'smart' style
      boss__update__hosts__default_task: "smart"
      tags:
        - boss-ansible-role-update-hosts

vagrant_playbook_role_bootstrap_data: |
    - role: boss-ansible-role-bootstrap
      # Disable raw commands to avoid sudo issues.
      boss__bootstrap_raw: False
      # Don't set domain on Travis.
      boss__bootstrap_domain: ''
      # Try bootstrapping a different IP address to avoid idempotency loop.
      # boss__bootstrap_ipv4: '127.0.1.2'
      # boss__hosts_file: /etc/hosts.molecule
      boss__bootstrap_admin_default_users:
        - name: bossjones
      boss__bootstrap_admin_groups: [ 'admins', 'staff', 'adm', 'sudo', 'bossjones' ]
      boss__bootstrap_admin_system: False
      tags:
        - boss-ansible-role-bootstrap

vagrant_playbook_role_fact_data: |
    - role: boss-ansible-role-fact
      tags:
        - boss-ansible-role-fact

vagrant_playbook_role_environment_data: |
    - role: boss-ansible-role-environment
      tags:
        - boss-ansible-role-environment

vagrant_playbook_role_etckeeper_data: |
    - role: boss-ansible-role-etckeeper
      tags:
        - boss-ansible-role-etckeeper

    # # - role: geerlingguy.pip
vagrant_playbook_role_timezone_data: |
    - role: boss-ansible-role-timezone
      timezone: 'UTC'
      timezone_update_hardware_clock: False
      tags:
        - boss-ansible-role-timezone

vagrant_playbook_role_ntp_data: |
    - role: boss-ansible-role-ntp
      task: install
      bossjones__user: vagrant
      bossjones__group: vagrant
      timezone: 'UTC'
      timezone_update_hardware_clock: False
      # defaults file for ansible-ntp
      # Defines if host is ntp_master
      # set ntp_master to true on specific group_vars/group
      ntp_master: False

      # Define your ntp_master_servers
      ntp_master_servers:
        - 0.ubuntu.pool.ntp.org
        - 1.ubuntu.pool.ntp.org
        - 2.ubuntu.pool.ntp.org
        - 3.ubuntu.pool.ntp.org
      tags:
        - boss-ansible-role-ntp

vagrant_playbook_role_tools: |
      bossjones__user: vagrant
      bossjones__group: vagrant
      boss__tools__install_docker_tools: True
      boss__tools__install_kube_tools: True
      boss__tools__install_rust_tools: True
      tags:
        - boss-ansible-role-tools

vagrant_playbook_role_journald: |
      bossjones__user: vagrant
      bossjones__group: vagrant
      tags:
        - boss-ansible-role-journald

vagrant_playbook_role_nfs_master_data: |
    - role: boss-ansible-role-nfs
      boss__nfs__sysctl_fileno: 30000
      boss__nfs__systemd_limit_mem_lock: infinity
      boss__nfs__systemd_limit_no_file: 30000
      boss__nfs__systemd_limit_nproc: infinity
      boss__nfs__systemd_limit_core: infinity
      boss__nfs__etc_default_nfs_rpcnfsdcount: 64
      boss__nfs__nfs_server_group: nfs_masters
      boss__nfs__nfs_client_group: nfs_clients
      task: master
      boss__nfs__nfs_interface: '{{main_network_interface}}'
      # FIXME:(3/2/2019) THIS IS NORMALLY ENABLED, USING OVERRIDE FOR NOW # boss__nfs__master_node_ip: "{{ hostvars[groups[boss__nfs__nfs_server_group][0]]['ansible_' + boss__nfs__nfs_interface].ipv4.address }}"
      boss__nfs__master_node_ip: "{{nfs_server_ip_override}}"
      tags:
        - boss-ansible-role-nfs

vagrant_playbook_role_nfs_client_data: |
    - role: boss-ansible-role-nfs
      boss__nfs__sysctl_fileno: 30000
      boss__nfs__systemd_limit_mem_lock: infinity
      boss__nfs__systemd_limit_no_file: 30000
      boss__nfs__systemd_limit_nproc: infinity
      boss__nfs__systemd_limit_core: infinity
      boss__nfs__etc_default_nfs_rpcnfsdcount: 64
      boss__nfs__nfs_server_group: nfs_masters
      boss__nfs__nfs_client_group: nfs_clients
      task: client
      boss__nfs__nfs_interface: '{{main_network_interface}}'
      # FIXME:(3/2/2019) THIS IS NORMALLY ENABLED, USING OVERRIDE FOR NOW # boss__nfs__master_node_ip: "{{ hostvars[groups[boss__nfs__nfs_server_group][0]]['ansible_' + boss__nfs__nfs_interface].ipv4.address }}"
      boss__nfs__master_node_ip: "{{nfs_server_ip_override}}"
      # boss__nfs__nfs_interface: 'enp0s3'
      tags:
        - boss-ansible-role-nfs

# vagrant_playbook_role_netdata: |
#     - role: boss-ansible-role-netdata
#       ##############################################################
#       # SOURCE: https://blog.codybunch.com/2018/03/26/Metrics-Part-2-InfluxDB/
#       ##############################################################
#       # netdata_configure_archive: true
#       # netdata_archive_enabled: 'yes'
#       # netdata_archive_type: 'opentsdb'
#       # netdata_archive_destination: ":4242"
#       # netdata_archive_prefix: 'netdata'
#       # netdata_archive_data_source: 'average'
#       # netdata_archive_update: 1
#       # netdata_archive_buffer_on_failures: 30
#       # netdata_archive_timeout: 20000
#       # netdata_archive_send_names: true
#       ##############################################################
#       boss__netdata__netdata_proxy_enabled: False
#       boss__netdata__netdata_nginx_enabled: False
#       # graphite_secret_key: testtest123
#       # version: 1.1.3
#       # graphite_install_version: "{{ version }}"
#       # graphite_cache_graphite_url: 'http://127.0.0.1:8080'
#       # bossjones__user: vagrant
#       # bossjones__group: vagrant
#       boss__netdata__netdata_interface: '{{main_network_interface}}'

#       netdata_registry_enabled: True
#       netdata_registry_to_announce: "http://{{ netdata_stream_master_node }}:{{ netdata_default_port }}"
#       pri_domain_name: scarlett-office.local
#       netdata_stream_enabled: True
#       # You can generate API keys, with the linux command: uuidgen
#       netdata_stream_api_key: 6E6C3CA0-7B78-4115-AE2C-9681032F71D1
#       netdata_stream_master_node: "{{boss__netdata__netdata_master_ip}}"
#       nginx_listen_port: 8080
#       tags:
#         - boss-ansible-role-netdata

vagrant_playbook_role_logrotate: |
  # defaults file for ansible-logrotate
  # Defines if you want your log files compressed
  logrotate_compress: false

  # packages drop log rotation information into this directory
  logrotate_conf_dir: '/etc/logrotate.d/'

  logrotate_config: true

  logrotate_configs:
    - name: 'netdata'
      compress: true
      copytruncate: false
      create:
        mode: '640'
        owner: 'netdata'
        group: 'netdata'
      delaycompress: true
      logs:
        - '/var/log/netdata/*.log'
      # If the log file is missing, go on to the next one without
      # issuing an error message.
      missingok: true
      # Do not rotate the log if it is empty
      notifempty: true
      postrotate:
        - '/bin/kill -HUP `pidof netdata 2>/dev/null` 2>/dev/null || true'
      rotate: '14'
      rotation: 'daily'
      sharedscripts: true



  # create new (empty) log files after rotating old ones
  logrotate_create_new: true

  logrotate_default_backlogs_rotate: '4'

  logroate_default_configs:
    - 'apt'
    - 'dpkg'
    - 'rsyslog'
    - 'ufw'

  # Defines the default rotate schedule
  # hourly | daily | weekly | monthly
  logrotate_default_rotate: 'daily'

  # Defines if logrotate configs defined in logroate_default_configs
  # should be removed or not
  logrotate_remove_default_configs: false

  tags:
    - ansible-logrotate

# ---
# local_release_dir: /tmp/releases

# # Used to only evaluate vars from download role
# skip_downloads: false

# # if this is set to true will only download files once. Doesn't work
# # on Container Linux by CoreOS unless the download_localhost is true and localhost
# # is running another OS type. Default compress level is 1 (fastest).
# download_run_once: False
# download_compress: 1

# # if this is set to true, uses the localhost for download_run_once mode
# # (requires docker and sudo to access docker). You may want this option for
# # local caching of docker images or for Container Linux by CoreOS cluster nodes.
# # Otherwise, uses the first node in the kube-master group to store images
# # in the download_run_once mode.
# download_localhost: False

# # Always pull images if set to True. Otherwise check by the repo's tag/digest.
# download_always_pull: False

# # Use the first kube-master if download_localhost is not set
# download_delegate: "{% if download_localhost %}localhost{% else %}{{groups['kube-master'][0]}}{% endif %}"

# # Versions
# kube_version: v1.9.5
# kubeadm_version: "{{ kube_version }}"
# etcd_version: v3.2.4
# # TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults
# # after migration to container download
# calico_version: "v2.6.8"
# calico_ctl_version: "v1.6.3"
# calico_cni_version: "v1.11.4"
# calico_policy_version: "v1.0.3"
# calico_rr_version: "v0.4.2"
# flannel_version: "v0.10.0"
# flannel_cni_version: "v0.3.0"
# istio_version: "0.2.6"
# vault_version: 0.8.1
# weave_version: 2.2.1
# pod_infra_version: 3.0
# contiv_version: 1.1.7
# cilium_version: "v1.0.0-rc8"

# # Download URLs
# istioctl_download_url: "https://storage.googleapis.com/istio-release/releases/{{ istio_version }}/istioctl/istioctl-linux"
# kubeadm_download_url: "https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/amd64/kubeadm"
# vault_download_url: "https://releases.hashicorp.com/vault/{{ vault_version }}/vault_{{ vault_version }}_linux_amd64.zip"

# # Checksums
# istioctl_checksum: fd703063c540b8c0ab943f478c05ab257d88ae27224c746a27d0526ddbf7c370
# kubeadm_checksum: 12b6e9ac1624852b7c978bde70b9bde9ca0e4fc6581d09bddfb117bb41f93c74
# vault_binary_checksum: 3c4d70ba71619a43229e65c67830e30e050eab7a81ac6b28325ff707e5914188

# # Containers
# etcd_image_repo: "quay.io/coreos/etcd"
# etcd_image_tag: "{{ etcd_version }}"
# flannel_image_repo: "quay.io/coreos/flannel"
# flannel_image_tag: "{{ flannel_version }}"
# flannel_cni_image_repo: "quay.io/coreos/flannel-cni"
# flannel_cni_image_tag: "{{ flannel_cni_version }}"
# calicoctl_image_repo: "quay.io/calico/ctl"
# calicoctl_image_tag: "{{ calico_ctl_version }}"
# calico_node_image_repo: "quay.io/calico/node"
# calico_node_image_tag: "{{ calico_version }}"
# calico_cni_image_repo: "quay.io/calico/cni"
# calico_cni_image_tag: "{{ calico_cni_version }}"
# calico_policy_image_repo: "quay.io/calico/kube-controllers"
# calico_policy_image_tag: "{{ calico_policy_version }}"
# calico_rr_image_repo: "quay.io/calico/routereflector"
# calico_rr_image_tag: "{{ calico_rr_version }}"
# istio_proxy_image_repo: docker.io/istio/proxy
# istio_proxy_image_tag: "{{ istio_version }}"
# istio_proxy_init_image_repo: docker.io/istio/proxy_init
# istio_proxy_init_image_tag: "{{ istio_version }}"
# istio_ca_image_repo: docker.io/istio/istio-ca
# istio_ca_image_tag: "{{ istio_version }}"
# istio_mixer_image_repo: docker.io/istio/mixer
# istio_mixer_image_tag: "{{ istio_version }}"
# istio_pilot_image_repo: docker.io/istio/pilot
# istio_pilot_image_tag: "{{ istio_version }}"
# istio_proxy_debug_image_repo: docker.io/istio/proxy_debug
# istio_proxy_debug_image_tag: "{{ istio_version }}"
# istio_sidecar_initializer_image_repo: docker.io/istio/sidecar_initializer
# istio_sidecar_initializer_image_tag: "{{ istio_version }}"
# istio_statsd_image_repo: prom/statsd-exporter
# istio_statsd_image_tag: latest
# hyperkube_image_repo: "gcr.io/google-containers/hyperkube"
# hyperkube_image_tag: "{{ kube_version }}"
# pod_infra_image_repo: "gcr.io/google_containers/pause-amd64"
# pod_infra_image_tag: "{{ pod_infra_version }}"
# install_socat_image_repo: "xueshanf/install-socat"
# install_socat_image_tag: "latest"
# netcheck_version: "v1.0"
# netcheck_agent_img_repo: "quay.io/l23network/k8s-netchecker-agent"
# netcheck_agent_tag: "{{ netcheck_version }}"
# netcheck_server_img_repo: "quay.io/l23network/k8s-netchecker-server"
# netcheck_server_tag: "{{ netcheck_version }}"
# weave_kube_image_repo: "weaveworks/weave-kube"
# weave_kube_image_tag: "{{ weave_version }}"
# weave_npc_image_repo: "weaveworks/weave-npc"
# weave_npc_image_tag: "{{ weave_version }}"
# contiv_image_repo: "contiv/netplugin"
# contiv_image_tag: "{{ contiv_version }}"
# contiv_auth_proxy_image_repo: "contiv/auth_proxy"
# contiv_auth_proxy_image_tag: "{{ contiv_version }}"
# cilium_image_repo: "docker.io/cilium/cilium"
# cilium_image_tag: "{{ cilium_version }}"
# nginx_image_repo: nginx
# nginx_image_tag: 1.13
# dnsmasq_version: 2.78
# dnsmasq_image_repo: "andyshinn/dnsmasq"
# dnsmasq_image_tag: "{{ dnsmasq_version }}"
# kubedns_version: 1.14.8
# kubedns_image_repo: "gcr.io/google_containers/k8s-dns-kube-dns-amd64"
# kubedns_image_tag: "{{ kubedns_version }}"
# coredns_version: 1.1.0
# coredns_image_repo: "docker.io/coredns/coredns"
# coredns_image_tag: "{{ coredns_version }}"
# dnsmasq_nanny_image_repo: "gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64"
# dnsmasq_nanny_image_tag: "{{ kubedns_version }}"
# dnsmasq_sidecar_image_repo: "gcr.io/google_containers/k8s-dns-sidecar-amd64"
# dnsmasq_sidecar_image_tag: "{{ kubedns_version }}"
# dnsmasqautoscaler_version: 1.1.2
# dnsmasqautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# dnsmasqautoscaler_image_tag: "{{ dnsmasqautoscaler_version }}"
# kubednsautoscaler_version: 1.1.2
# kubednsautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# kubednsautoscaler_image_tag: "{{ kubednsautoscaler_version }}"
# test_image_repo: busybox
# test_image_tag: latest
# elasticsearch_version: "v2.4.1"
# elasticsearch_image_repo: "gcr.io/google_containers/elasticsearch"
# elasticsearch_image_tag: "{{ elasticsearch_version }}"
# fluentd_version: "1.22"
# fluentd_image_repo: "gcr.io/google_containers/fluentd-elasticsearch"
# fluentd_image_tag: "{{ fluentd_version }}"
# kibana_version: "v4.6.1"
# kibana_image_repo: "gcr.io/google_containers/kibana"
# kibana_image_tag: "{{ kibana_version }}"
# helm_version: "v2.8.1"
# helm_image_repo: "lachlanevenson/k8s-helm"
# helm_image_tag: "{{ helm_version }}"
# tiller_image_repo: "gcr.io/kubernetes-helm/tiller"
# tiller_image_tag: "{{ helm_version }}"
# vault_image_repo: "vault"
# vault_image_tag: "{{ vault_version }}"
# registry_image_repo: "registry"
# registry_image_tag: "2.6"
# registry_proxy_image_repo: "gcr.io/google_containers/kube-registry-proxy"
# registry_proxy_image_tag: "0.4"
# local_volume_provisioner_image_repo: "quay.io/external_storage/local-volume-provisioner"
# local_volume_provisioner_image_tag: "v2.0.0"
# cephfs_provisioner_image_repo: "quay.io/kubespray/cephfs-provisioner"
# cephfs_provisioner_image_tag: "92295a30"
# ingress_nginx_controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
# ingress_nginx_controller_image_tag: "0.12.0"
# ingress_nginx_default_backend_image_repo: "gcr.io/google_containers/defaultbackend"
# ingress_nginx_default_backend_image_tag: "1.4"
# cert_manager_version: "v0.2.3"
# cert_manager_controller_image_repo: "quay.io/jetstack/cert-manager-controller"
# cert_manager_controller_image_tag: "{{ cert_manager_version }}"
# cert_manager_ingress_shim_image_repo: "quay.io/jetstack/cert-manager-ingress-shim"
# cert_manager_ingress_shim_image_tag: "{{ cert_manager_version }}"

# downloads:
#   netcheck_server:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_server_img_repo }}"
#     tag: "{{ netcheck_server_tag }}"
#     sha256: "{{ netcheck_server_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   netcheck_agent:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_agent_img_repo }}"
#     tag: "{{ netcheck_agent_tag }}"
#     sha256: "{{ netcheck_agent_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   etcd:
#     enabled: true
#     container: true
#     repo: "{{ etcd_image_repo }}"
#     tag: "{{ etcd_image_tag }}"
#     sha256: "{{ etcd_digest_checksum|default(None) }}"
#     groups:
#       - etcd
#   kubeadm:
#     enabled: "{{ kubeadm_enabled }}"
#     file: true
#     version: "{{ kubeadm_version }}"
#     dest: "kubeadm"
#     sha256: "{{ kubeadm_checksum }}"
#     source_url: "{{ kubeadm_download_url }}"
#     url: "{{ kubeadm_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - k8s-cluster
#   istioctl:
#     enabled: "{{ istio_enabled }}"
#     file: true
#     version: "{{ istio_version }}"
#     dest: "istio/istioctl"
#     sha256: "{{ istioctl_checksum }}"
#     source_url: "{{ istioctl_download_url }}"
#     url: "{{ istioctl_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - kube-master
#   istio_proxy:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_image_repo }}"
#     tag: "{{ istio_proxy_image_tag }}"
#     sha256: "{{ istio_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_init:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_init_image_repo }}"
#     tag: "{{ istio_proxy_init_image_tag }}"
#     sha256: "{{ istio_proxy_init_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_ca:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_ca_image_repo }}"
#     tag: "{{ istio_ca_image_tag }}"
#     sha256: "{{ istio_ca_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_mixer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_mixer_image_repo }}"
#     tag: "{{ istio_mixer_image_tag }}"
#     sha256: "{{ istio_mixer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_pilot:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_pilot_image_repo }}"
#     tag: "{{ istio_pilot_image_tag }}"
#     sha256: "{{ istio_pilot_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_debug:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_debug_image_repo }}"
#     tag: "{{ istio_proxy_debug_image_tag }}"
#     sha256: "{{ istio_proxy_debug_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_sidecar_initializer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_sidecar_initializer_image_repo }}"
#     tag: "{{ istio_sidecar_initializer_image_tag }}"
#     sha256: "{{ istio_sidecar_initializer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_statsd:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_statsd_image_repo }}"
#     tag: "{{ istio_statsd_image_tag }}"
#     sha256: "{{ istio_statsd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   hyperkube:
#     enabled: true
#     container: true
#     repo: "{{ hyperkube_image_repo }}"
#     tag: "{{ hyperkube_image_tag }}"
#     sha256: "{{ hyperkube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   cilium:
#     enabled: "{{ kube_network_plugin == 'cilium' }}"
#     container: true
#     repo: "{{ cilium_image_repo }}"
#     tag: "{{ cilium_image_tag }}"
#     sha256: "{{ cilium_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel:
#     enabled: "{{ kube_network_plugin == 'flannel' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ flannel_image_repo }}"
#     tag: "{{ flannel_image_tag }}"
#     sha256: "{{ flannel_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel_cni:
#     enabled: "{{ kube_network_plugin == 'flannel' }}"
#     container: true
#     repo: "{{ flannel_cni_image_repo }}"
#     tag: "{{ flannel_cni_image_tag }}"
#     sha256: "{{ flannel_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calicoctl:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calicoctl_image_repo }}"
#     tag: "{{ calicoctl_image_tag }}"
#     sha256: "{{ calicoctl_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_node:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_node_image_repo }}"
#     tag: "{{ calico_node_image_tag }}"
#     sha256: "{{ calico_node_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_cni:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_cni_image_repo }}"
#     tag: "{{ calico_cni_image_tag }}"
#     sha256: "{{ calico_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_policy:
#     enabled: "{{ enable_network_policy or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_policy_image_repo }}"
#     tag: "{{ calico_policy_image_tag }}"
#     sha256: "{{ calico_policy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_rr:
#     enabled: "{{ peer_with_calico_rr is defined and peer_with_calico_rr and kube_network_plugin == 'calico' }}"
#     container: true
#     repo: "{{ calico_rr_image_repo }}"
#     tag: "{{ calico_rr_image_tag }}"
#     sha256: "{{ calico_rr_digest_checksum|default(None) }}"
#     groups:
#       - calico-rr
#   weave_kube:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_kube_image_repo }}"
#     tag: "{{ weave_kube_image_tag }}"
#     sha256: "{{ weave_kube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   weave_npc:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_npc_image_repo }}"
#     tag: "{{ weave_npc_image_tag }}"
#     sha256: "{{ weave_npc_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_image_repo }}"
#     tag: "{{ contiv_image_tag }}"
#     sha256: "{{ contiv_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv_auth_proxy:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_auth_proxy_image_repo }}"
#     tag: "{{ contiv_auth_proxy_image_tag }}"
#     sha256: "{{ contiv_auth_proxy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   pod_infra:
#     enabled: true
#     container: true
#     repo: "{{ pod_infra_image_repo }}"
#     tag: "{{ pod_infra_image_tag }}"
#     sha256: "{{ pod_infra_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   install_socat:
#     enabled: "{{ ansible_os_family in ['CoreOS', 'Container Linux by CoreOS'] }}"
#     container: true
#     repo: "{{ install_socat_image_repo }}"
#     tag: "{{ install_socat_image_tag }}"
#     sha256: "{{ install_socat_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   nginx:
#     enabled: "{{ loadbalancer_apiserver_localhost }}"
#     container: true
#     repo: "{{ nginx_image_repo }}"
#     tag: "{{ nginx_image_tag }}"
#     sha256: "{{ nginx_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq:
#     enabled: "{{ dns_mode == 'dnsmasq_kubedns' }}"
#     container: true
#     repo: "{{ dnsmasq_image_repo }}"
#     tag: "{{ dnsmasq_image_tag }}"
#     sha256: "{{ dnsmasq_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubedns:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubedns_image_repo }}"
#     tag: "{{ kubedns_image_tag }}"
#     sha256: "{{ kubedns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   coredns:
#     enabled: "{{ dns_mode in ['coredns', 'coredns_dual'] }}"
#     container: true
#     repo: "{{ coredns_image_repo }}"
#     tag: "{{ coredns_image_tag }}"
#     sha256: "{{ coredns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_nanny:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_nanny_image_repo }}"
#     tag: "{{ dnsmasq_nanny_image_tag }}"
#     sha256: "{{ dnsmasq_nanny_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_sidecar:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_sidecar_image_repo }}"
#     tag: "{{ dnsmasq_sidecar_image_tag }}"
#     sha256: "{{ dnsmasq_sidecar_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubednsautoscaler:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubednsautoscaler_image_repo }}"
#     tag: "{{ kubednsautoscaler_image_tag }}"
#     sha256: "{{ kubednsautoscaler_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   testbox:
#     enabled: false
#     container: true
#     repo: "{{ test_image_repo }}"
#     tag: "{{ test_image_tag }}"
#     sha256: "{{ testbox_digest_checksum|default(None) }}"
#   elasticsearch:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ elasticsearch_image_repo }}"
#     tag: "{{ elasticsearch_image_tag }}"
#     sha256: "{{ elasticsearch_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   fluentd:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ fluentd_image_repo }}"
#     tag: "{{ fluentd_image_tag }}"
#     sha256: "{{ fluentd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kibana:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ kibana_image_repo }}"
#     tag: "{{ kibana_image_tag }}"
#     sha256: "{{ kibana_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   helm:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ helm_image_repo }}"
#     tag: "{{ helm_image_tag }}"
#     sha256: "{{ helm_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   tiller:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ tiller_image_repo }}"
#     tag: "{{ tiller_image_tag }}"
#     sha256: "{{ tiller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   vault:
#     enabled: "{{ cert_management == 'vault' }}"
#     container: "{{ vault_deployment_type != 'host' }}"
#     file: "{{ vault_deployment_type == 'host' }}"
#     dest: "vault/vault_{{ vault_version }}_linux_amd64.zip"
#     mode: "0755"
#     owner: "vault"
#     repo: "{{ vault_image_repo }}"
#     sha256: "{{ vault_binary_checksum if vault_deployment_type == 'host' else vault_digest_checksum|d(none) }}"
#     source_url: "{{ vault_download_url }}"
#     tag: "{{ vault_image_tag }}"
#     unarchive: true
#     url: "{{ vault_download_url }}"
#     version: "{{ vault_version }}"
#     groups:
#       - vault
#   registry:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_image_repo }}"
#     tag: "{{ registry_image_tag }}"
#     sha256: "{{ registry_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   registry_proxy:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_proxy_image_repo }}"
#     tag: "{{ registry_proxy_image_tag }}"
#     sha256: "{{ registry_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   local_volume_provisioner:
#     enabled: "{{ local_volume_provisioner_enabled }}"
#     container: true
#     repo: "{{ local_volume_provisioner_image_repo }}"
#     tag: "{{ local_volume_provisioner_image_tag }}"
#     sha256: "{{ local_volume_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cephfs_provisioner:
#     enabled: "{{ cephfs_provisioner_enabled }}"
#     container: true
#     repo: "{{ cephfs_provisioner_image_repo }}"
#     tag: "{{ cephfs_provisioner_image_tag }}"
#     sha256: "{{ cephfs_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   ingress_nginx_controller:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_controller_image_repo }}"
#     tag: "{{ ingress_nginx_controller_image_tag }}"
#     sha256: "{{ ingress_nginx_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   ingress_nginx_default_backend:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_default_backend_image_repo }}"
#     tag: "{{ ingress_nginx_default_backend_image_tag }}"
#     sha256: "{{ ingress_nginx_default_backend_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   cert_manager_controller:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_controller_image_repo }}"
#     tag: "{{ cert_manager_controller_image_tag }}"
#     sha256: "{{ cert_manager_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cert_manager_ingress_shim:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_ingress_shim_image_repo }}"
#     tag: "{{ cert_manager_ingress_shim_image_tag }}"
#     sha256: "{{ cert_manager_ingress_shim_digest_checksum|default(None) }}"
#     groups:
#       - kube-node

# download_defaults:
#   container: false
#   file: false
#   repo: None
#   tag: None
#   enabled: false
#   dest: None
#   version: None
#   url: None
#   unarchive: false
#   owner: kube
#   mode: None



# ---
# elasticsearch_cpu_limit: 1000m
# elasticsearch_mem_limit: 0M
# elasticsearch_cpu_requests: 100m
# elasticsearch_mem_requests: 0M
# elasticsearch_service_port: 9200
