domain_root: scarlettlab.com

# boss__efk__manifest_path
base_dir_path: ~/dev/bossjones/bosslab-playbooks
dist_dir_path: "{{base_dir_path}}/dist"
manifest_dir_name: manifests/kubernetes-cluster-manifests
manifest_dir_path: "{{dist_dir_path}}/{{manifest_dir_name}}"
path_to_network_disk: "/mnt/publicdata"
main_network_interface: 'enp0s8'
nfs_server_group: nfs_masters
nfs_client_group: nfs_clients
nfs_server_ip_override: 192.168.205.12
k8_admin_config_dir: ~/dev/bossjones/bosslab-playbooks
k8_admin_config_path: "{{k8_admin_config_dir}}/vagrant-admin.conf"
queen_host: "k8s-head"

# export KUBECONFIG=~/dev/bossjones/bosslab-playbooks/borg-admin.conf

#      .########..######..##.....##..#######...######..########.########..##.....##.########.########.
#      .##.......##....##.##.....##.##.....##.##....##.##.......##.....##.##.....##.##.......##.....##
#      .##.......##.......##.....##.##.....##.##.......##.......##.....##.##.....##.##.......##.....##
#      .######...##.......#########.##.....##..######..######...########..##.....##.######...########.
#      .##.......##.......##.....##.##.....##.......##.##.......##...##....##...##..##.......##...##..
#      .##.......##....##.##.....##.##.....##.##....##.##.......##....##....##.##...##.......##....##.
#      .########..######..##.....##..#######...######..########.##.....##....###....########.##.....##


boss__echoserver__echoserver_subdomain: echoserver
boss__echoserver__manifest_path: "{{manifest_dir_path}}/echoserver"
boss__echoserver__namespace_name: echoserver
boss__echoserver__deployment_name: echoserver
boss__echoserver__echoserver_version: 2.1
boss__echoserver__echoserver_image_repo: "gcr.io/kubernetes-e2e-test-images/echoserver"
boss__echoserver__echoserver_image_tag: "{{ boss__echoserver__echoserver_version }}"
boss__echoserver__echoserver_cpu_limit: 100m
boss__echoserver__echoserver_mem_limit: 55Mi
boss__echoserver__echoserver_cpu_requests: 100m
boss__echoserver__echoserver_mem_requests: 20Mi
boss__echoserver__deployment_annotations_list:
  - name: nginx.ingress.kubernetes.io/ssl-redirect
    val: \"false\"
boss__echoserver__deployment_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
boss__echoserver__deployment_labels: |
  run: nginx
boss__echoserver__deployment_spec_replicas: 2
# boss__echoserver__deployment_spec_template_metadata_labels: "{{boss__echoserver__deployment_labels}}"
boss__echoserver__ingress_labels: |
  run: nginx
boss__echoserver__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__echoserver__service_labels: |
  run: nginx
boss__echoserver__service__spec_selector: |
  run: nginx
# boss__echoserver__echoserver_service_port: 9200


# # Kubernetes dashboard
# # RBAC required. see docs/getting-started.md for access details.
# dashboard_enabled: true

# # Addons which can be enabled
# efk_enabled: false
# helm_enabled: false
# istio_enabled: false
# registry_enabled: false
# enable_network_policy: false
# local_volume_provisioner_enabled: "{{ local_volumes_enabled | default('false') }}"
# persistent_volumes_enabled: false
# cephfs_provisioner_enabled: false
# ingress_nginx_enabled: false
# cert_manager_enabled: false

##########################################################################################################
#                   ..######.....###....##.......####..######...#######.
#                   .##....##...##.##...##........##..##....##.##.....##
#                   .##........##...##..##........##..##.......##.....##
#                   .##.......##.....##.##........##..##.......##.....##
#                   .##.......#########.##........##..##.......##.....##
#                   .##....##.##.....##.##........##..##....##.##.....##
#                   ..######..##.....##.########.####..######...#######.
##########################################################################################################

boss__calico__manifest_path: "{{manifest_dir_path}}/calico"
boss__calico__namespace_name: kube-system
boss__calico__prometheus_metrics_enabled: false
# NOTE: See https://docs.projectcalico.org/v3.5/usage/configuration/mtu for recomendations
boss__calico__veth_mtu: 1480 # 1480,
boss__calico__node_env_CALICO_IPV4POOL_IPIP: "Always"
boss__calico__debugger_deployment_spec_replicas: 1
boss__calico__debugger_enabled: "enabled"

boss__calico__node_version: v3.3.2
boss__calico__node_image_repo: "quay.io/calico/node"
boss__calico__node_image_tag: "{{ boss__calico__node_version }}"


boss__calico__cni_version: v3.3.2
boss__calico__cni_image_repo: "quay.io/calico/cni"
boss__calico__cni_image_tag: "{{ boss__calico__node_version }}"

boss__calico__enable_prometheus_exporter_servicemonitor: "enabled"

boss__calico__prometheus_exporter_namespace_name: monitoring

boss__calico__enable_prometheus_felix: "false" # false,enabled
boss__calico__felix_PrometheusMetricsPort: 9091
boss__calico__typha_PrometheusMetricsPort: 9093


boss__calico__node_daemonset_annotations: |
  scheduler.alpha.kubernetes.io/critical-pod: ''
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__felix_PrometheusMetricsPort}}"

boss__calico__enable_typha: "false" # false,enabled
boss__calico__enable_prometheus_typha: "false" # false,enabled
boss__calico__typha_service_name: "none"  # or none,calico-typha

boss__calico__typha_deployment_spec_replicas: 0 # or 0,3

boss__calico__typha_service_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"

boss__calico__typha_deployment_annotations: |
  # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
  # add-on, ensuring it gets priority scheduling and that its resources are reserved
  # if it ever gets evicted.
  scheduler.alpha.kubernetes.io/critical-pod: ''
  cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
  prometheus.io/scrape: "true"
  prometheus.io/port: "{{boss__calico__typha_PrometheusMetricsPort}}"



# {{ calico_node_image_repo }}:{{ calico_node_image_tag }}

##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__manifest_path: "{{manifest_dir_path}}/dashboard"
boss__dashboard__namespace_name: kube-system

boss__dashboard__version: v1.10.0
boss__dashboard__image_repo: "k8s.gcr.io/kubernetes-dashboard-amd64"
boss__dashboard__image_tag: "{{ boss__dashboard__version }}"

boss__dashboard__deployment_container_args: |
  - --auto-generate-certificates
  # Uncomment the following line to manually specify Kubernetes API server Host
  # If not specified, Dashboard will attempt to auto discover the API server and connect
  # to it. Uncomment only if the default does not work.
  # - --apiserver-host=http://my-address:port


##########################################################################################################
#      .########.....###.....######..##.....##.########...#######.....###....########..########.
#      .##.....##...##.##...##....##.##.....##.##.....##.##.....##...##.##...##.....##.##.....##
#      .##.....##..##...##..##.......##.....##.##.....##.##.....##..##...##..##.....##.##.....##
#      .##.....##.##.....##..######..#########.########..##.....##.##.....##.########..##.....##
#      .##.....##.#########.......##.##.....##.##.....##.##.....##.#########.##...##...##.....##
#      .##.....##.##.....##.##....##.##.....##.##.....##.##.....##.##.....##.##....##..##.....##
#      .########..##.....##..######..##.....##.########...#######..##.....##.##.....##.########.
##########################################################################################################

boss__dashboard__admin__manifest_path: "{{manifest_dir_path}}/dashboard-admin"
boss__dashboard__admin__namespace_name: kube-system


##########################################################################################################
#   .########.########.##....##
#   .##.......##.......##...##.
#   .##.......##.......##..##..
#   .######...######...#####...
#   .##.......##.......##..##..
#   .##.......##.......##...##.
#   .########.##.......##....##
##########################################################################################################
boss__efk__elasticsearch_subdomain: elasticsearch
boss__efk__manifest_path: "{{manifest_dir_path}}/efk"
boss__efk__namespace_name: kube-system
boss__efk__deployment_name: efk


##########################################################################################################
#    .########.##..........###.....######..########.####..######...######..########....###....########...######..##.....##
#    .##.......##.........##.##...##....##....##.....##..##....##.##....##.##.........##.##...##.....##.##....##.##.....##
#    .##.......##........##...##..##..........##.....##..##.......##.......##........##...##..##.....##.##.......##.....##
#    .######...##.......##.....##..######.....##.....##..##........######..######...##.....##.########..##.......#########
#    .##.......##.......#########.......##....##.....##..##.............##.##.......#########.##...##...##.......##.....##
#    .##.......##.......##.....##.##....##....##.....##..##....##.##....##.##.......##.....##.##....##..##....##.##.....##
#    .########.########.##.....##..######.....##....####..######...######..########.##.....##.##.....##..######..##.....##
##########################################################################################################
boss__efk__elasticsearch_ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__efk__elasticsearch_ingress_labels: |
  k8s-app: elasticsearch-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__elasticsearch_service_labels: |
  k8s-app: elasticsearch-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "Elasticsearch"
  boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_persistent_volume_claim_spec_resources_requests_storage: "1Gi"
boss__efk__elasticsearch_persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: elasticsearch-logging
  boss-part-of: efk
boss__efk__elasticsearch_version: v5.6.2
boss__efk__elasticsearch_image_repo: "bossjones/elasticsearch"
boss__efk__elasticsearch_image_tag: "{{ boss__efk__elasticsearch_version }}"
boss__efk__elasticsearch_cpu_limit: 1000m
boss__efk__elasticsearch_mem_limit: 3048Mi
boss__efk__elasticsearch_cpu_requests: 100m
boss__efk__elasticsearch_mem_requests: 2350Mi

boss__efk__elasticsearch_stateful_set_env_ES_JAVA_OPTS: "-Xms2048 -Xmx2048"

# boss__efk__elasticsearch_stateful_set_spec_resources: False
boss__efk__elasticsearch_stateful_set_spec_resources: |
  # need more cpu upon initialization, therefore burstable class
  requests:
    cpu: 100m
    memory: 2350Mi
  limits:
    cpu: 1000m
    memory: 4048Mi

boss__efk__elasticsearch_service_account_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_cluster_role_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_cluster_role_binding_labels: |
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk

boss__efk__elasticsearch_stateful_set_labels: |
    k8s-app: elasticsearch-logging
    version: "{{boss__efk__elasticsearch_version}}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__elasticsearch_stateful_set_spec_replicas: 1
boss__efk__elasticsearch_stateful_set_spec_selector_match_labels: |
        k8s-app: elasticsearch-logging
        version: v5.6.2
boss__efk__elasticsearch_stateful_set_spec_template_metadata_labels: |
        k8s-app: elasticsearch-logging
        version: v5.6.2
        kubernetes.io/cluster-service: "true"

#########################

##########################################################################################################
#     .########..######...........######..##.....##.########.....###....########..#######..########.
#     .##.......##....##.........##....##.##.....##.##.....##...##.##......##....##.....##.##.....##
#     .##.......##...............##.......##.....##.##.....##..##...##.....##....##.....##.##.....##
#     .######....######..#######.##.......##.....##.########..##.....##....##....##.....##.########.
#     .##.............##.........##.......##.....##.##...##...#########....##....##.....##.##...##..
#     .##.......##....##.........##....##.##.....##.##....##..##.....##....##....##.....##.##....##.
#     .########..######...........######...#######..##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__efk__elasticsearch_curator_deployment_labels: |
  k8s-app: es-curator
  boss-part-of: efk

boss__efk__elasticsearch_curator_version: 5.3.0-1
boss__efk__elasticsearch_curator_image_repo: "aknudsen/es-curator-service"
boss__efk__elasticsearch_curator_image_tag: "{{ boss__efk__elasticsearch_curator_version }}"
boss__efk__elasticsearch_curator_cpu_limit: 200m
boss__efk__elasticsearch_curator_mem_limit: 500Mi
boss__efk__elasticsearch_curator_cpu_requests: 100m
boss__efk__elasticsearch_curator_mem_requests: 200Mi

#########################


##########################################################################################################
#    .########.##.......##.....##.########.##....##.########.########.
#    .##.......##.......##.....##.##.......###...##....##....##.....##
#    .##.......##.......##.....##.##.......####..##....##....##.....##
#    .######...##.......##.....##.######...##.##.##....##....##.....##
#    .##.......##.......##.....##.##.......##..####....##....##.....##
#    .##.......##.......##.....##.##.......##...###....##....##.....##
#    .##.......########..#######..########.##....##....##....########.
##########################################################################################################
boss__efk__fluentd_version: v2.3.0
boss__efk__fluentd_image_repo: "bossjones/fluentd-elasticsearch"
boss__efk__fluentd_image_tag: "{{ boss__efk__fluentd_version }}"
boss__efk__fluentd_mem_limit: 500Mi
boss__efk__fluentd_cpu_requests: 100m
boss__efk__fluentd_mem_requests: 200Mi
boss__efk__fluentd_config_map_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_service_account_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_cluster_role_binding_labels: |
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_labels: |
    k8s-app: fluentd-es
    version: v2.2.1
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_selector_match_labels: |
      k8s-app: fluentd-es
      # version: v2.2.1
boss__efk__fluentd_daemon_set_spec_template_metadata_labels: |
        k8s-app: fluentd-es
        # kubernetes.io/cluster-service: "true"
        # version: v2.2.1
        # boss-part-of: efk
boss__efk__fluentd_daemon_set_spec_template_metadata_annotations: |
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'


boss__efk__fluentd_daemon_set_spec_template_spec_securityContext: |
  runAsNonRoot: false
  # runAsUser: 65534
  privileged: true

boss__efk__fluentd_daemon_set_spec_template_spec_resources: |
  requests:
    cpu: 100m
    memory: 200Mi
  limits:
    memory: 500Mi
    # memory: 200Mi

boss__efk__fluentd_daemon_set_spec_template_spec_volumeMounts: |
  # - name: libsystemddir
  #   mountPath: /host/lib
  #   readOnly: true
  - name: varlog
    mountPath: /var/log
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true
  - name: config-volume
    mountPath: /etc/fluent/config.d
  - name: docker-sock
    # mountPath: /var/run/docker.sock
    mountPath: /run/containerd/containerd.sock
  - name: varrun
    mountPath: /var/run
  - name: libsystemd
    # mountPath: /usr/lib64/libsystemd.so.0
    mountPath: /usr/lib64/libsystemd.so.0

boss__efk__fluentd_daemon_set_spec_template_spec_volumes: |
  # It is needed to copy systemd library to decompress journals
  # - name: libsystemddir
  #   hostPath:
  #     path: /usr/lib64
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
  - name: config-volume
    configMap:
      # name: fluentd-es-config-v0.1.0
      name: fluentd-es-config-v0.1.6
  - name: docker-sock
    hostPath:
      # path: /var/run/docker/libcontainerd/docker-containerd.sock
      path: /var/run/docker.sock
  - name: varrun
    hostPath:
      path: /var/run
  - name: libsystemd
    hostPath:
      path: /lib/x86_64-linux-gnu/libsystemd.so.0



############################################


##########################################################################################################
#   .##....##.####.########.....###....##....##....###...
#   .##...##...##..##.....##...##.##...###...##...##.##..
#   .##..##....##..##.....##..##...##..####..##..##...##.
#   .#####.....##..########..##.....##.##.##.##.##.....##
#   .##..##....##..##.....##.#########.##..####.#########
#   .##...##...##..##.....##.##.....##.##...###.##.....##
#   .##....##.####.########..##.....##.##....##.##.....##
##########################################################################################################
boss__efk__kibana_subdomain: kibana
boss__efk__kibana_version: 5.6.2
boss__efk__kibana_image_repo: "bossjones/kibana"
boss__efk__kibana_image_tag: "{{ boss__efk__kibana_version }}"
boss__efk__kibana_cpu_limit: 1000m
boss__efk__kibana_cpu_requests: 1000m

boss__efk__kibana_deployment_labels: |
  k8s-app: kibana-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__kibana_deployment_spec_replicas: 1
boss__efk__kibana_deployment_spec_selector_match_labels: |
  k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_metadata_labels: |
  k8s-app: kibana-logging
boss__efk__kibana_deployment_spec_template_spec_node_selector: |
  kubernetes.io/hostname: "k8s-node-1"
boss__efk__kibana_ingress_labels: |
  k8s-app: kibana-ingress
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: efk
boss__efk__kibana_ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix
boss__efk__kibana_service_labels: |
  k8s-app: kibana-logging
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "Kibana"
  boss-part-of: efk

boss__efk__kibana_service_annotations: False
# boss__efk__kibana_service_annotations: |
#   nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
#   traefik.frontend.rule.type: PathPrefix


boss__efk__kibana_deployment_spec_template_spec_resources: False
# boss__efk__kibana_deployment_spec_template_spec_resources: |
  # # need more cpu upon initialization, therefore burstable class
  # requests:
  #   cpu: 1000m
  # limits:
  #   cpu: 1000m
############################################


boss__efk__elasticsearch_persistent_volume_spec_capacity_storage: 2Gi
boss__efk__elasticsearch_persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/elasticsearch"
# boss__efk__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__efk__nfs_master_node_ip: "{{ nfs_server_ip_override }}"


##########################################


##########################################################################################################
#      .########..########..######...####..######..########.########..##....##
#      .##.....##.##.......##....##...##..##....##....##....##.....##..##..##.
#      .##.....##.##.......##.........##..##..........##....##.....##...####..
#      .########..######...##...####..##...######.....##....########.....##...
#      .##...##...##.......##....##...##........##....##....##...##......##...
#      .##....##..##.......##....##...##..##....##....##....##....##.....##...
#      .##.....##.########..######...####..######.....##....##.....##....##...
##########################################################################################################
boss__registry__subdomain: registry
boss__registry__manifest_path: "{{manifest_dir_path}}/registry"
boss__registry__namespace_name: kube-system
boss__registry__deployment_name: registry
boss__registry__enable_pvc: false
boss__registry__enable_tls: "enabled"
boss__registry__tls_cert_cluster_issuer_name: selfsigning-issuer
boss__registry__ingress_tls_config: |
  - hosts:
    - {{boss__registry__subdomain}}.{{domain_root}}
    secretName: docker-registry-tls-certificate


boss__registry__ingress_annotations: |
  traefik.frontend.rule.type: PathPrefix
  nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  nginx.ingress.kubernetes.io/rewrite-target: \"/\"

boss__registry__ingress_labels: |
  k8s-app: registry
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__service_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeRegistry"
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: registry
  boss-part-of: registry
  version: "{{ boss__registry__version }}"

boss__registry__version: 2.6
boss__registry__image_repo: "registry"
boss__registry__image_tag: "{{ boss__registry__version }}"
boss__registry__cpu_limit: 1000m
boss__registry__mem_limit: 500Mi
boss__registry__cpu_requests: 100m
boss__registry__mem_requests: 200Mi

boss__registry__service_account_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"
boss__registry__cluster_role_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__cluster_role_binding_labels: |
    k8s-app: registry
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry
    version: "{{ boss__registry__version }}"

boss__registry__stateful_set_labels: |
    k8s-app: registry
    version: "{{ boss__registry__version }}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: registry

boss__registry__stateful_set_spec_replicas: 1
boss__registry__stateful_set_spec_selector_match_labels: |
  k8s-app: registry
  version: "{{ boss__registry__version }}"

boss__registry__stateful_set_spec_template_metadata_labels: |
        k8s-app: registry
        version: "{{ boss__registry__version }}"
        kubernetes.io/cluster-service: "true"

boss__registry__persistent_volume_spec_capacity_storage: 2Gi
boss__registry__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__nfs_master_node_ip: "{{ nfs_server_ip_override }}"



##########################################################################################################
#         ..######..########.########..########.........##.....##....###....##....##....###.....######...########.########.
#         .##....##.##.......##.....##....##............###...###...##.##...###...##...##.##...##....##..##.......##.....##
#         .##.......##.......##.....##....##............####.####..##...##..####..##..##...##..##........##.......##.....##
#         .##.......######...########.....##....#######.##.###.##.##.....##.##.##.##.##.....##.##...####.######...########.
#         .##.......##.......##...##......##............##.....##.#########.##..####.#########.##....##..##.......##...##..
#         .##....##.##.......##....##.....##............##.....##.##.....##.##...###.##.....##.##....##..##.......##....##.
#         ..######..########.##.....##....##............##.....##.##.....##.##....##.##.....##..######...########.##.....##
##########################################################################################################

boss__certmanager__manifest_path: "{{manifest_dir_path}}/cert-manager"
boss__certmanager__namespace_name: kube-system
boss__certmanager__example_com_namespace_name: default
boss__certmanager__my_cluster_namespace_name: default
boss__certmanager__my_cluster_certificate_name: wildcard-vagrantlab
boss__certmanager__my_cluster_commonName: "*.{{domain_root}}"
boss__certmanager__my_cluster_organization: |
  - Vagrantlab
boss__certmanager__my_cluster_dnsNames: |
  - "*.{{domain_root}}"
boss__certmanager__my_cluster_generate_tls: "force"


##########################################################################################################
#    .########..########..######...####..######..########.########..##....##.........##.....##.####
#    .##.....##.##.......##....##...##..##....##....##....##.....##..##..##..........##.....##..##.
#    .##.....##.##.......##.........##..##..........##....##.....##...####...........##.....##..##.
#    .########..######...##...####..##...######.....##....########.....##....#######.##.....##..##.
#    .##...##...##.......##....##...##........##....##....##...##......##............##.....##..##.
#    .##....##..##.......##....##...##..##....##....##....##....##.....##............##.....##..##.
#    .##.....##.########..######...####..######.....##....##.....##....##.............#######..####
##########################################################################################################
boss__registry__ui__subdomain: registry-ui
boss__registry__ui__manifest_path: "{{manifest_dir_path}}/registry-ui"
boss__registry__ui__namespace_name: default
boss__registry__ui__deployment_name: docker-registry-ui
boss__registry__ui__enable_pvc: false
boss__registry__ui__enable_tls: "disabled"
boss__registry__ui__ingress_tls_config: |
  - hosts:
    - {{boss__registry__ui__subdomain}}.{{domain_root}}
    secretName: docker-registry-ui-tls-certificate
boss__registry__ui__tls_cert_cluster_issuer_name: selfsigning-issuer

boss__registry__ui__version: 0.6
boss__registry__ui__image_repo: "joxit/docker-registry-ui"
boss__registry__ui__image_tag: "{{ boss__registry__ui__version }}"

boss__registry__ui__cpu_limit: 1000m
boss__registry__ui__mem_limit: 500Mi
boss__registry__ui__cpu_requests: 100m
boss__registry__ui__mem_requests: 200Mi

# joxit/docker-registry-ui:0.6

boss__registry__ui__ingress_annotations: |
  # FIXME: Fix this
  # certmanager.k8s.io/issuer: "selfsigning-issuer"
  # NOTE: This is what was configured in
  nginx.ingress.kubernetes.io/proxy-body-size: "0"
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # This is what we had configured
  traefik.frontend.rule.type: PathPrefix
  # nginx.ingress.kubernetes.io/ssl-redirect: \"true\"
  # nginx.ingress.kubernetes.io/rewrite-target: \"/\"

boss__registry__ui__ingress_labels: |
  app: {{boss__registry__ui__deployment_name}}
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_labels: |
    app: {{boss__registry__ui__deployment_name}}
    k8s-app: {{boss__registry__ui__deployment_name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeRegistry"
    boss-part-of: {{boss__registry__ui__deployment_name}}
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix

boss__registry__ui__deployment_labels: |
  app: {{boss__registry__ui__deployment_name}}
  k8s-app: {{boss__registry__ui__deployment_name}}
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"


boss__registry__ui__deployment_spec_template_metadata_labels: |
        app: {{boss__registry__ui__deployment_name}}
        k8s-app: {{boss__registry__ui__deployment_name}}
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
        boss-part-of: {{boss__registry__ui__deployment_name}}
        version: "{{ boss__registry__ui__version }}"

boss__registry__ui__deployment_spec_template_spec_containers_resources: |
  requests:
    cpu: 250m
    memory: 64Mi
  limits:
    # This is 0.5 of a cpu
    cpu: 500m
    memory: 128Mi

boss__registry__ui__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: {{boss__registry__ui__deployment_name}}
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__registry__ui__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: {{boss__registry__ui__deployment_name}}
  boss-part-of: {{boss__registry__ui__deployment_name}}
  version: "{{ boss__registry__ui__version }}"

boss__registry__ui__service_account_labels: |
    k8s-app: {{boss__registry__ui__deployment_name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: {{boss__registry__ui__deployment_name}}
    version: "{{ boss__registry__ui__version }}"
boss__registry__ui__cluster_role_labels: |
    k8s-app: {{boss__registry__ui__deployment_name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: {{boss__registry__ui__deployment_name}}
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__cluster_role_binding_labels: |
    k8s-app: {{boss__registry__ui__deployment_name}}
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: {{boss__registry__ui__deployment_name}}
    version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_labels: |
    k8s-app: {{boss__registry__ui__deployment_name}}
    version: "{{ boss__registry__ui__version }}"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: {{boss__registry__ui__deployment_name}}

boss__registry__ui__stateful_set_spec_replicas: 1
boss__registry__ui__stateful_set_spec_selector_match_labels: |
        k8s-app: {{boss__registry__ui__deployment_name}}
        version: "{{ boss__registry__ui__version }}"

boss__registry__ui__stateful_set_spec_template_metadata_labels: |
        k8s-app: {{boss__registry__ui__deployment_name}}
        version: "{{ boss__registry__ui__version }}"
        kubernetes.io/cluster-service: "true"

boss__registry__ui__persistent_volume_spec_capacity_storage: 2Gi
boss__registry__ui__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/registry-ui"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__registry__ui__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__registry__ui__nfs_master_node_ip: "{{ nfs_server_ip_override }}"




##########################################################################################################
#     .......##.########.##....##.##....##.####.##....##..######.
#     .......##.##.......###...##.##...##...##..###...##.##....##
#     .......##.##.......####..##.##..##....##..####..##.##......
#     .......##.######...##.##.##.#####.....##..##.##.##..######.
#     .##....##.##.......##..####.##..##....##..##..####.......##
#     .##....##.##.......##...###.##...##...##..##...###.##....##
#     ..######..########.##....##.##....##.####.##....##..######.
##########################################################################################################
boss__jenkins__subdomain: jenkins
boss__jenkins__version: lts
boss__jenkins__image_repo: "jenkins/jenkins"
boss__jenkins__image_tag: "{{ boss__jenkins__version }}"
boss__jenkins__cpu_limit: 1000m
boss__jenkins__cpu_requests: 1000m
boss__jenkins__manifest_path: "{{manifest_dir_path}}/jenkins-k8"
boss__jenkins__namespace_name: kube-system

boss__jenkins__deployment_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__deployment_spec_replicas: 1
boss__jenkins__deployment_spec_selector_match_labels: |
      k8s-app: jenkins
boss__jenkins__deployment_spec_template_metadata_labels: |
        k8s-app: jenkins
boss__jenkins__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "k8s-node-1"
boss__jenkins__ingress_labels: |
    k8s-app: jenkins-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: jenkins
boss__jenkins__ingress_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix
boss__jenkins__service_labels: |
    k8s-app: jenkins
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "jenkins"
    boss-part-of: jenkins

boss__jenkins__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__jenkins__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: jenkins
  boss-part-of: jenkins
  version: "{{ boss__jenkins__version }}"

boss__jenkins__persistent_volume_spec_capacity_storage: 2Gi
boss__jenkins__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/jenkins"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__jenkins__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__jenkins__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#              .##.....##.########....###....########...######..########.########.########.
#              .##.....##.##.........##.##...##.....##.##....##....##....##.......##.....##
#              .##.....##.##........##...##..##.....##.##..........##....##.......##.....##
#              .#########.######...##.....##.########...######.....##....######...########.
#              .##.....##.##.......#########.##..............##....##....##.......##...##..
#              .##.....##.##.......##.....##.##........##....##....##....##.......##....##.
#              .##.....##.########.##.....##.##.........######.....##....########.##.....##
##########################################################################################################
boss__heapster__subdomain: heapster
boss__heapster__version: lts
boss__heapster__image_repo: "heapster/heapster"
boss__heapster__image_tag: "{{ boss__heapster__version }}"
boss__heapster__cpu_limit: 1000m
boss__heapster__cpu_requests: 1000m
boss__heapster__manifest_path: "{{manifest_dir_path}}/heapster2"
boss__heapster__namespace_name: kube-system

boss__heapster__deployment_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__deployment_spec_replicas: 1
boss__heapster__deployment_spec_selector_match_labels: |
      k8s-app: heapster
boss__heapster__deployment_spec_template_metadata_labels: |
        k8s-app: heapster
boss__heapster__ingress_labels: |
    k8s-app: heapster-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: heapster
boss__heapster__ingress_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix
boss__heapster__service_labels: |
    k8s-app: heapster
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "heapster"
    boss-part-of: heapster

boss__heapster__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__heapster__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: heapster
  boss-part-of: heapster
  version: "{{ boss__heapster__version }}"

boss__heapster__persistent_volume_spec_capacity_storage: 2Gi
boss__heapster__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/heapster"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__heapster__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__heapster__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################

##########################################################################################################
# ................##.....##.########.########.########..####..######...######...........######..########.########..##.....##.########.########.
# ................###...###.##..........##....##.....##..##..##....##.##....##.........##....##.##.......##.....##.##.....##.##.......##.....##
# ................####.####.##..........##....##.....##..##..##.......##...............##.......##.......##.....##.##.....##.##.......##.....##
# ................##.###.##.######......##....########...##..##........######..#######..######..######...########..##.....##.######...########.
# ................##.....##.##..........##....##...##....##..##.............##...............##.##.......##...##....##...##..##.......##...##..
# ................##.....##.##..........##....##....##...##..##....##.##....##.........##....##.##.......##....##....##.##...##.......##....##.
# ................##.....##.########....##....##.....##.####..######...######...........######..########.##.....##....###....########.##.....##
##########################################################################################################
boss__metrics__server__subdomain: metrics-server
boss__metrics__server__version: lts
boss__metrics__server__image_repo: "metrics-server/metrics-server"
boss__metrics__server__image_tag: "{{ boss__metrics__server__version }}"
boss__metrics__server__cpu_limit: 1000m
boss__metrics__server__cpu_requests: 1000m
boss__metrics__server__manifest_path: "{{manifest_dir_path}}/metrics-server"
boss__metrics__server__namespace_name: kube-system

boss__metrics__server__deployment_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__deployment_spec_replicas: 1
boss__metrics__server__deployment_spec_selector_match_labels: |
      k8s-app: metrics-server
boss__metrics__server__deployment_spec_template_metadata_labels: |
        k8s-app: metrics-server
boss__metrics__server__ingress_labels: |
    k8s-app: metrics-server-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: metrics-server
boss__metrics__server__ingress_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix
boss__metrics__server__service_labels: |
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "metrics-server"
    boss-part-of: metrics-server

boss__metrics__server__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__metrics__server__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: metrics-server
  boss-part-of: metrics-server
  version: "{{ boss__metrics__server__version }}"

boss__metrics__server__persistent_volume_spec_capacity_storage: 2Gi
boss__metrics__server__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/metrics-server"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__metrics__server__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__metrics__server__nfs_master_node_ip: "{{ nfs_server_ip_override }}"
############################################


##########################################################################################################
#   .########.##.....##.########.########.########..##....##....###....##...............########..##....##..######.
#   .##........##...##.....##....##.......##.....##.###...##...##.##...##...............##.....##.###...##.##....##
#   .##.........##.##......##....##.......##.....##.####..##..##...##..##...............##.....##.####..##.##......
#   .######......###.......##....######...########..##.##.##.##.....##.##.......#######.##.....##.##.##.##..######.
#   .##.........##.##......##....##.......##...##...##..####.#########.##...............##.....##.##..####.......##
#   .##........##...##.....##....##.......##....##..##...###.##.....##.##...............##.....##.##...###.##....##
#   .########.##.....##....##....########.##.....##.##....##.##.....##.########.........########..##....##..######.
##########################################################################################################
boss__external__dns__subdomain: external-dns
boss__external__dns__version: lts
boss__external__dns__image_repo: "external-dns/external-dns"
boss__external__dns__image_tag: "{{ boss__external__dns__version }}"
boss__external__dns__cpu_limit: 1000m
boss__external__dns__cpu_requests: 1000m
boss__external__dns__manifest_path: "{{manifest_dir_path}}/external-dns"
boss__external__dns__namespace_name: default

boss__external__dns__deployment_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__deployment_spec_replicas: 1
boss__external__dns__deployment_spec_selector_match_labels: |
      k8s-app: external-dns
boss__external__dns__deployment_spec_template_metadata_labels: |
        k8s-app: external-dns
boss__external__dns__ingress_labels: |
    k8s-app: external-dns-ingress
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    boss-part-of: external-dns
boss__external__dns__ingress_annotations: |
    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
    traefik.frontend.rule.type: PathPrefix
boss__external__dns__service_labels: |
    k8s-app: external-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "external-dns"
    boss-part-of: external-dns

boss__external__dns__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_claim_spec_resources_requests_storage: "2Gi"
boss__external__dns__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: external-dns
  boss-part-of: external-dns
  version: "{{ boss__external__dns__version }}"

boss__external__dns__persistent_volume_spec_capacity_storage: 2Gi
boss__external__dns__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/external-dns"
# TODO: FIXME, this needs to be dynamic going forward, but for now, we need to hardcode it
# boss__external__dns__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__external__dns__nfs_master_node_ip: "{{ nfs_server_ip_override }}"


##########################################################################################################
#              .##.....##.########.##.......##.....##
#              .##.....##.##.......##.......###...###
#              .##.....##.##.......##.......####.####
#              .#########.######...##.......##.###.##
#              .##.....##.##.......##.......##.....##
#              .##.....##.##.......##.......##.....##
#              .##.....##.########.########.##.....##
##########################################################################################################
boss__helm__subdomain: helm
boss__helm__manifest_path: "{{manifest_dir_path}}/helm"
boss__helm__namespace_name: kube-system




##########################################################################################################
#     .##.....##.########.########....###....##...............##.......########.
#     .###...###.##..........##......##.##...##...............##.......##.....##
#     .####.####.##..........##.....##...##..##...............##.......##.....##
#     .##.###.##.######......##....##.....##.##.......#######.##.......########.
#     .##.....##.##..........##....#########.##...............##.......##.....##
#     .##.....##.##..........##....##.....##.##...............##.......##.....##
#     .##.....##.########....##....##.....##.########.........########.########.
##########################################################################################################
boss__metallb__subdomain: metallb
boss__metallb__manifest_path: "{{manifest_dir_path}}/metallb"
boss__metallb__namespace_name: metallb-system
boss__metallb__address_pools: '192.168.205.13-192.168.205.14'



##########################################################################################################
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...########..########..######...######.
#              .###...##.##....##...##..###...##..##...##...........##..###...##.##....##..##.....##.##.......##....##.##....##
#              .####..##.##.........##..####..##...##.##............##..####..##.##........##.....##.##.......##.......##......
#              .##.##.##.##...####..##..##.##.##....###....#######..##..##.##.##.##...####.########..######....######...######.
#              .##..####.##....##...##..##..####...##.##............##..##..####.##....##..##...##...##.............##.......##
#              .##...###.##....##...##..##...###..##...##...........##..##...###.##....##..##....##..##.......##....##.##....##
#              .##....##..######...####.##....##.##.....##.........####.##....##..######...##.....##.########..######...######.
##########################################################################################################
boss__ingress__nginx__subdomain: ingress-nginx
boss__ingress__nginx__manifest_path: "{{manifest_dir_path}}/ingress-nginx"
boss__ingress__nginx__namespace_name: kube-system

boss__ingress__nginx__controller_version: 0.21.0
boss__ingress__nginx__controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
boss__ingress__nginx__controller_image_tag: "{{ boss__ingress__nginx__controller_version }}"


boss__ingress__nginx__defaultbackend_version: 1.4
boss__ingress__nginx__defaultbackend_image_repo: "gcr.io/google_containers/defaultbackend"
boss__ingress__nginx__defaultbackend_image_tag: "{{ boss__ingress__nginx__defaultbackend_version }}"

boss__ingress__nginx__controller_runAsUser: 33

boss__ingress__nginx__clusterrole_serviceaccount_labels: |
  kubernetes.io/bootstrapping: rbac-defaults
  addonmanager.kubernetes.io/mode: Reconcile
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__role_nginx_ingress_role_labels: |
  kubernetes.io/bootstrapping: rbac-defaults
  addonmanager.kubernetes.io/mode: Reconcile
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_labels: |
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__service_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "10254"

boss__ingress__nginx__serviceAccountName: nginx-ingress

boss__ingress__nginx__configmap_nginx_load_balancer_conf_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__configmap_tcp_services_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx
boss__ingress__nginx__configmap_udp_services_labels: |
  # addonmanager.kubernetes.io/mode: EnsureExists
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__deployment_labels: |
    app.kubernetes.io/name: ingress-nginx
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_replicas: 1
boss__ingress__nginx__deployment_spec_selector_match_labels: |
      app.kubernetes.io/name: ingress-nginx
      # app.kubernetes.io/part-of: kube-system
      app.kubernetes.io/part-of: ingress-nginx
      addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_labels: |
        app.kubernetes.io/name: ingress-nginx
        # app.kubernetes.io/part-of: kube-system
        app.kubernetes.io/part-of: ingress-nginx
        addonmanager.kubernetes.io/mode: Reconcile
boss__ingress__nginx__deployment_spec_template_metadata_annotations: |
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
boss__ingress__nginx__deployment_spec_template_spec_node_selector: |
        kubernetes.io/hostname: "{{queen_host}}"
        # app.kubernetes.io/name: ingress-nginx
        # app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_account_labels: |
    addonmanager.kubernetes.io/mode: Reconcile
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_selector_labels: |
  # app.kubernetes.io/name: ingress-nginx
  # app.kubernetes.io/part-of: ingress-nginx
  app.kubernetes.io/name: ingress-nginx
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_defaulthttpbackend_selector_labels: |
  app.kubernetes.io/name: default-http-backend
  app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service_default_http_backend_spec_type: NodePort
boss__ingress__nginx__service_ingress_nginx_spec_type: NodePort

boss__ingress__nginx__service_spec_ports: |
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  - name: stats
    port: 18080
    targetPort: 18080
    protocol: TCP
  - name: metrics
    port: 10254
    targetPort: 10254
    protocol: TCP

boss__ingress__nginx__service_defaulthttpbackend_spec_ports: |
  - port: 80
    # Assign 8080 to <POD-IP>:<targetPort>
    targetPort: 8080
    protocol: TCP
    name: http
    # nodePort: The Service created in the last section already used NodePort, so your nginx HTTPS replica is ready to serve traffic on the internet if your node has a public IP.
    # curl https://<WORKER-NODE-IP>:<NODE-PORT> -k
    # nodePort: 30001
    # - name: http
    #   port: 80
    #   targetPort: 80
    #   protocol: TCP
    # - name: https
    #   port: 443
    #   targetPort: 443
    #   protocol: TCP

boss__ingress__nginx__role_binding_labels: |
    # SOURCE: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Many of these are system: prefixed, which indicates that the resource is owned by the infrastructure. Modifications to these resources can result in non-functional clusters. One example is the system:node ClusterRole. This role defines permissions for kubelets. If the role is modified, it can prevent kubelets from working.
    # All of the default cluster roles and rolebindings are labeled with kubernetes.io/bootstrapping=rbac-defaults
    kubernetes.io/bootstrapping: rbac-defaults
    # addonmanager.kubernetes.io/mode: EnsureExists
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

boss__ingress__nginx__service__default_http_backend_labels: |
    app.kubernetes.io/name: default-http-backend
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    # kubernetes.io/minikube-addons: ingress
    # kubernetes.io/minikube-addons-endpoint: ingress
    addonmanager.kubernetes.io/mode: Reconcile

boss__ingress__nginx__service__default_http_backend_annotations: False
# boss__ingress__nginx__service__default_http_backend_annotations: |
#   prometheus.io/scrape: "true"
#   prometheus.io/port: "10254"

boss__ingress__nginx__deployment__default_http_backend_labels: |
    app.kubernetes.io/name: default-http-backend
    # app.kubernetes.io/part-of: kube-system
    app.kubernetes.io/part-of: ingress-nginx
    # kubernetes.io/minikube-addons: ingress
    # kubernetes.io/minikube-addons-endpoint: ingress
    addonmanager.kubernetes.io/mode: Reconcile

boss__ingress__nginx__defaulthttpbackend_deployment_spec_labels_matchLabels: |
  app.kubernetes.io/name: default-http-backend
  # addonmanager.kubernetes.io/mode: Reconcile


boss__ingress__nginx__defaulthttpbackend_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 20m
    memory: 30Mi
  requests:
    cpu: 20m
    memory: 30Mi

boss__ingress__nginx__defaulthttpbackend_deployment_spec_template_spec_containers_ports: |
  - containerPort: 8080

boss__ingress__nginx__nginx_ingress_controller_deployment_spec_template_spec_containers_ports: |
  - name: http
    containerPort: 80
    hostPort: 80
  - name: https
    containerPort: 443
    hostPort: 443
  # (Optional) we expose 18080 to access nginx stats in url /nginx-status
  - name: stats
    containerPort: 18080
    hostPort: 18080
  - name: metrics
    containerPort: 10254
    hostPort: 10254
    protocol: TCP

# boss__ingress__nginx__nginx_ingress_controller_deployment_spec_template_spec_containers_args: |
#   - /nginx-ingress-controller
#   # Service used to serve HTTP requests not matching any known server name (catch-all). Takes the form "namespace/name". The controller configures NGINX to forward requests to the first port of this Service. If not specified, a 404 page will be returned directly from NGINX.
#   - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
#   # Name of the ConfigMap containing custom global configurations for the controller.
#   - --configmap=$(POD_NAMESPACE)/nginx-load-balancer-conf
#   # Name of the ConfigMap containing the definition of the TCP services to expose. The key in the map indicates the external port to be used. The value is a reference to a Service in the form "namespace/name:port", where "port" can either be a port number or name. TCP ports 80 and 443 are reserved by the controller for servicing HTTP traffic.
#   - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
#   # Name of the ConfigMap containing the definition of the UDP services to expose. The key in the map indicates the external port to be used. The value is a reference to a Service in the form "namespace/name:port", where "port" can either be a port name or number.
#   - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
#   # Prefix of the Ingress annotations specific to the NGINX controller. (default "nginx.ingress.kubernetes.io")
#   - --annotations-prefix=nginx.ingress.kubernetes.io
#   # NOTE: 'report-node-internal-ip-address' - Set the load-balancer status of Ingress objects to internal Node addresses instead of external. Requires the update-status parameter.
#   # use minikube IP address in ingress status field
#   - --report-node-internal-ip-address
#   # log level for V logs
#   # - --v=5
#   # log to standard error instead of files (default true)
#   - --logtostderr
#   # Enable profiling via web interface host:port/debug/pprof/ (default true)
#   - --profiling

#   - --publish-service=$(POD_NAMESPACE)/ingress-nginx

##########################################################################################################
#    .##.....##....###....########..##....##.########...#######..##......##.##....##.########..########.##....##.########..########.########.
#    .###...###...##.##...##.....##.##...##..##.....##.##.....##.##..##..##.###...##.##.....##.##.......###...##.##.....##.##.......##.....##
#    .####.####..##...##..##.....##.##..##...##.....##.##.....##.##..##..##.####..##.##.....##.##.......####..##.##.....##.##.......##.....##
#    .##.###.##.##.....##.########..#####....##.....##.##.....##.##..##..##.##.##.##.########..######...##.##.##.##.....##.######...########.
#    .##.....##.#########.##...##...##..##...##.....##.##.....##.##..##..##.##..####.##...##...##.......##..####.##.....##.##.......##...##..
#    .##.....##.##.....##.##....##..##...##..##.....##.##.....##.##..##..##.##...###.##....##..##.......##...###.##.....##.##.......##....##.
#    .##.....##.##.....##.##.....##.##....##.########...#######...###..###..##....##.##.....##.########.##....##.########..########.##.....##
##########################################################################################################
boss__markdownrender__subdomain: markdownrender
boss__markdownrender__manifest_path: "{{manifest_dir_path}}/markdownrender"
boss__markdownrender__namespace_name: default


boss__markdownrender__version: latest-armhf
boss__markdownrender__image_repo: "functions/markdownrender"
boss__markdownrender__image_tag: "{{ boss__markdownrender__version }}"

####################################################################################################
#                  .########.########.....###....########.########.####.##....##.........####.##....##.########.########.########..##....##....###....##......
#                  ....##....##.....##...##.##...##.......##........##..##...##...........##..###...##....##....##.......##.....##.###...##...##.##...##......
#                  ....##....##.....##..##...##..##.......##........##..##..##............##..####..##....##....##.......##.....##.####..##..##...##..##......
#                  ....##....########..##.....##.######...######....##..#####....#######..##..##.##.##....##....######...########..##.##.##.##.....##.##......
#                  ....##....##...##...#########.##.......##........##..##..##............##..##..####....##....##.......##...##...##..####.#########.##......
#                  ....##....##....##..##.....##.##.......##........##..##...##...........##..##...###....##....##.......##....##..##...###.##.....##.##......
#                  ....##....##.....##.##.....##.########.##.......####.##....##.........####.##....##....##....########.##.....##.##....##.##.....##.########
##########################################################################################################
boss__traefik__internal__subdomain: traefik-internal
boss__traefik__internal__manifest_path: "{{manifest_dir_path}}/traefik-internal"
boss__traefik__internal__namespace_name: traefik-system
boss__traefik__internal__service_spec_type: LoadBalancer
boss__traefik__internal__service_spec_loadBalancerIP: 192.168.205.13


####################################################################################################
#          .##......##.########....###....##.....##.########..........######...######...#######..########..########
#          .##..##..##.##.........##.##...##.....##.##...............##....##.##....##.##.....##.##.....##.##......
#          .##..##..##.##........##...##..##.....##.##...............##.......##.......##.....##.##.....##.##......
#          .##..##..##.######...##.....##.##.....##.######...#######..######..##.......##.....##.########..######..
#          .##..##..##.##.......#########..##...##..##.....................##.##.......##.....##.##........##......
#          .##..##..##.##.......##.....##...##.##...##...............##....##.##....##.##.....##.##........##......
#          ..###..###..########.##.....##....###....########..........######...######...#######..##........########
##########################################################################################################
boss__weave__scope__subdomain: weave-scope
boss__weave__scope__manifest_path: "{{manifest_dir_path}}/weave-scope"
boss__weave__scope__namespace_name: weave
boss__weave__scope__manifest_filename_based_on_networking: "weave-calico-networking.yaml"
boss__weave__scope__enable_ingress_traefik: "enabled"

boss__weave__scope__ingress_metadata_annotations: |
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__subdomain: prometheus-operator
boss__prometheus__operator__manifest_path: "{{manifest_dir_path}}/prometheus-operator-v0-27-0"
boss__prometheus__operator__namespace_name: monitoring

# This doesn't have a last tag or anything like that
boss__prometheus__operator__alertmanager_subdomain: alertmanager
boss__prometheus__operator__alertmanager_version: v0.16.0
boss__prometheus__operator__alertmanager_crd_baseImage: "quay.io/prometheus/alertmanager"
boss__prometheus__operator__alertmanager_image_tag: "{{ boss__prometheus__operator__alertmanager_version }}"
boss__prometheus__operator__alertmanager_replicas: 2

boss__prometheus__operator__grafana_subdomain: grafana
boss__prometheus__operator__grafana_version: 6.0.2
boss__prometheus__operator__grafana_image_repo: "grafana/grafana"
boss__prometheus__operator__grafana_image_tag: "{{ boss__prometheus__operator__grafana_version }}"
boss__prometheus__operator__grafana_cpu_limit: 2000m
boss__prometheus__operator__grafana_mem_limit: 500Mi
boss__prometheus__operator__grafana_cpu_requests: 1000m
boss__prometheus__operator__grafana_mem_requests: 200Mi
boss__prometheus__operator__grafana_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 2000m
    memory: 200Mi
  requests:
    cpu: 1000m
    memory: 100Mi

boss__prometheus__operator__prometheus_adapter_version: v0.4.1
boss__prometheus__operator__prometheus_adapter_crd_baseImage: "quay.io/coreos/k8s-prometheus-adapter-amd64"
boss__prometheus__operator__prometheus_adapter_image_tag: "{{ boss__prometheus__operator__prometheus_adapter_version }}"

boss__prometheus__operator__prometheus_subdomain: prometheus
boss__prometheus__operator__prometheus_version: v2.5.0
boss__prometheus__operator__prometheus_crd_baseImage: "quay.io/prometheus/prometheus"
boss__prometheus__operator__prometheus_image_tag: "{{ boss__prometheus__operator__prometheus_version }}"

boss__prometheus__operator__node_exporter_version: v0.17.0
boss__prometheus__operator__node_exporter_image_repo: "quay.io/prometheus/node-exporter"
boss__prometheus__operator__node_exporter_image_tag: "{{ boss__prometheus__operator__node_exporter_version }}"
boss__prometheus__operator__node_exporter_daemonset_spec_template_spec_containers_resources: |
  limits:
    cpu: 250m
    memory: 180Mi
  requests:
    cpu: 102m
    memory: 180Mi

boss__prometheus__operator__kube_rbac_proxy_version: v0.4.1
boss__prometheus__operator__kube_rbac_proxy_image_repo: "quay.io/coreos/kube-rbac-proxy"
boss__prometheus__operator__kube_rbac_proxy_image_tag: "{{ boss__prometheus__operator__kube_rbac_proxy_version }}"
boss__prometheus__operator__kube_rbac_proxy_daemonset_spec_template_spec_containers_resources: |
  # NOTE: Orig below ( 3/3/2019 )
  #limits:
  #  cpu: 20m
  #  memory: 40Mi
  #requests:
  #  cpu: 10m
  #  memory: 20Mi
  limits:
    cpu: 100m
    memory: 120Mi
  requests:
    cpu: 90m
    memory: 60Mi

boss__prometheus__operator__kube_rbac_proxy_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 20m
    memory: 40Mi
  requests:
    cpu: 10m
    memory: 20Mi

boss__prometheus__operator__kube_state_metrics_version: v1.5.0
boss__prometheus__operator__kube_state_metrics_image_repo: "quay.io/coreos/kube-state-metrics"
boss__prometheus__operator__kube_state_metrics_image_tag: "{{ boss__prometheus__operator__kube_state_metrics_version }}"
boss__prometheus__operator__kube_state_metrics_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 100m
    memory: 150Mi
  requests:
    cpu: 100m
    memory: 150Mi

boss__prometheus__operator__addon_resizer_version: 2.1
boss__prometheus__operator__addon_resizer_image_repo: "gcr.io/google-containers/addon-resizer-amd64"
boss__prometheus__operator__addon_resizer_image_tag: "{{ boss__prometheus__operator__addon_resizer_version }}"
boss__prometheus__operator__addon_resizer_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 50m
    memory: 30Mi
  requests:
    cpu: 10m
    memory: 30Mi

boss__prometheus__operator__prometheus_operator_version: v0.29.0
boss__prometheus__operator__prometheus_operator_image_repo: "quay.io/coreos/prometheus-operator"
boss__prometheus__operator__prometheus_operator_image_tag: "{{ boss__prometheus__operator__prometheus_operator_version }}"
boss__prometheus__operator__prometheus_operator_deployment_spec_template_spec_containers_resources: |
  limits:
    cpu: 200m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

boss__prometheus__operator__configmap_reload_version: v0.0.1
boss__prometheus__operator__configmap_reload_image_repo: "quay.io/coreos/configmap-reload"
boss__prometheus__operator__configmap_reload_image_tag: "{{ boss__prometheus__operator__configmap_reload_version }}"

boss__prometheus__operator__prometheus_operator_config_reloader_version: v0.29.0
boss__prometheus__operator__prometheus_operator_config_reloader_image_repo: "quay.io/coreos/prometheus-config-reloader"
boss__prometheus__operator__prometheus_operator_config_reloader_image_tag: "{{ boss__prometheus__operator__prometheus_operator_config_reloader_version }}"
boss__prometheus__operator__prometheus_operator_config_reloader_deployment_spec_template_spec_containers_resources: |
  limits:
    # cpu: 200m
    memory: 300Mi
    # k8s-infra version # memory: 500Mi
  requests:
    # cpu: 100m
    memory: 100Mi
    # k8s-infra version # memory: 300Mi


boss__prometheus__operator__alertmanager_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__alertmanager_ingress_metadata_labels: |
  run: nginx
  alertmanager: main
  app: alertmanager


boss__prometheus__operator__prometheus_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__prometheus_ingress_metadata_labels: |
  app: prometheus
  prometheus: k8s
  run: nginx
  k8s-app: prometheus-operator


boss__prometheus__operator__grafana_ingress_metadata_annotations: |
  # Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  # SOURCE: https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/multiple-ingress-controllers
  # NOTE: To designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource:

  # to designate that a particular Ingress resource must be handled only by the NGINX or NGINX Plus controller add the following annotation along with the value to the Ingress resource
  # kubernetes.io/ingress.class: "nginx"
  # kubernetes.io/tls-acme: "false"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-body-size: \"0\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"
  # FIXME: something wrong with this # nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # https://github.com/kubernetes/ingress-nginx/issues/1567
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  # INFO: Note the nginx.ingress.kubernetes.io/ssl-redirect annotation. It is used since we are not specifying a host. When no host is specified, then the default-server is hit, which is configured with a self-signed certificate, and redirects http to https. This issue explains more.
  # SOURCE: https://medium.com/@Oskarr3/setting-up-ingress-on-minikube-6ae825e98f82
  # nginx.ingress.kubernetes.io/rewrite-target: /
  traefik.frontend.rule.type: PathPrefix

boss__prometheus__operator__grafana_ingress_metadata_labels: |
  run: nginx
  app: grafana


################# Persistent volume claims

# persistent volume claim - grafana
boss__prometheus__operator__grafana_pvc_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator

boss__prometheus__operator__grafana_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__grafana_pvc_spec_resources_requests_storage: "2Gi"

# persistent volume - grafana
boss__prometheus__operator__grafana_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__grafana_pv_spec_capacity_storage: 2Gi
boss__prometheus__operator__grafana_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/grafana"


# persistent volume claim - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_tmpfs_pvc_spec_resources_requests_storage: "250Mi"

# persistent volume - prometheus-adapter-tmpfs
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: prometheus-operator
  boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_capacity_storage: 250Mi
boss__prometheus__operator__prometheus_adapter_tmpfs_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/prometheus-adapter-tmpfs"

# persistent volume claim - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_storageClassName: "nfs-dynamic-class"
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pvc_spec_resources_requests_storage: "250Mi"

# persistent volume - prometheus-adapter-volume-serving-cert
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_labels: |
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: prometheus-operator
    boss-part-of: prometheus-operator

boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_capacity_storage: 250Mi
boss__prometheus__operator__prometheus_adapter_volume_serving_cert_pv_spec_nfs: |
    server: {{nfs_server_ip_override}}
    path: "/mnt/publicdata/prometheus-adapter-volume-serving-cert"

boss__prometheus__operator__prometheus_additional_scrape_configs: |
  # RUN: kubectl --namespace monitoring create secret generic additional-scrape-configs --from-file=PLAINTEXT-SECRET-prometheus-additional.yaml --dry-run -oyaml > additional-scrape-configs.yaml
  # global:
  #     scrape_interval: 10s
  - job_name: 'netdata-scrape'

    metrics_path: '/api/v1/allmetrics'
    params:
      # format: prometheus | prometheus_all_hosts
      # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP
      format: [prometheus]
      #
      # sources: as-collected | raw | average | sum | volume
      # default is: average
      #source: [as-collected]
      #
      # server name for this prometheus - the default is the client IP
      # for netdata to uniquely identify it
      #server: ['prometheus1']
    honor_labels: true

    # 217 is rpi cluster
    static_configs:
      - targets: ['192.168.205.10:19999','192.168.205.11:19999','192.168.205.12:19999', '192.168.1.217:19999','192.168.1.184:19999','192.168.1.22:19999', '192.168.1.23:19999', '192.168.1.175:19999']

  - job_name: 'ingress-nginx-endpoints'
    kubernetes_sd_configs:
    - role: pod
      namespaces:
        names:
        # - ingress-nginx
        - kube-system

    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
      action: replace
      target_label: __scheme__
      regex: (https?)
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      target_label: __address__
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2

    - source_labels: [__meta_kubernetes_service_name]
      regex: prometheus-server
      action: drop

# SOURCE: https://techtran.science/2018/06/11/unifi-to-grafana-using-prometheus-and-unifi_exporter/
# - job_name: 'unifi_exporter'
#   static_configs:
#     - targets: ['dockerswarm:9130']
#       labels:
#         alias: unifi_exporter

boss__prometheus__operator__alertmanager_plaintext_configs: |
  global:
    resolve_timeout: 5m
  route:
    group_by: ['job']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: 'null'
    routes:
    - match:
        alertname: Watchdog
      receiver: 'null'
  receivers:
  - name: 'null'


####################################################################################################
#   .##.....##.##....##.####.########.####.........########.##.....##.########...#######..########..########.########.########.
#   .##.....##.###...##..##..##........##..........##........##...##..##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.####..##..##..##........##..........##.........##.##...##.....##.##.....##.##.....##....##....##.......##.....##
#   .##.....##.##.##.##..##..######....##..#######.######......###....########..##.....##.########.....##....######...########.
#   .##.....##.##..####..##..##........##..........##.........##.##...##........##.....##.##...##......##....##.......##...##..
#   .##.....##.##...###..##..##........##..........##........##...##..##........##.....##.##....##.....##....##.......##....##.
#   ..#######..##....##.####.##.......####.........########.##.....##.##.........#######..##.....##....##....########.##.....##
####################################################################################################
boss__unifi__exporter__subdomain: unifi-exporter
boss__unifi__exporter__manifest_path: "{{manifest_dir_path}}/unifi-exporter"
boss__unifi__exporter__namespace_name: monitoring
boss__unifi__exporter__apply_changes_immediately: False
boss__unifi__exporter__delete_secrets: True

boss__unifi__exporter__port: 9130
boss__unifi__exporter__listen_address: ":{{boss__unifi__exporter__port}}"
boss__unifi__exporter__listen_metricspath: /metrics
boss__unifi__exporter__unifi_address: https://192.168.1.8:8443
boss__unifi__exporter__unifi_site: Hyenanet
boss__unifi__exporter__unifi_insecure: true
boss__unifi__exporter__unifi_timeout: 5s


##########################################################################################################
#   .####.##....##.########.##.......##.....##.##.....##.########..########...........#######..########..########.########.....###....########..#######..########.
#   ..##..###...##.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##.....##.##.......##.....##...##.##......##....##.....##.##.....##
#   ..##..####..##.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##.....##.##.......##.....##..##...##.....##....##.....##.##.....##
#   ..##..##.##.##.######...##.......##.....##....###....##.....##.########..#######.##.....##.########..######...########..##.....##....##....##.....##.########.
#   ..##..##..####.##.......##.......##.....##...##.##...##.....##.##.....##.........##.....##.##........##.......##...##...#########....##....##.....##.##...##..
#   ..##..##...###.##.......##.......##.....##..##...##..##.....##.##.....##.........##.....##.##........##.......##....##..##.....##....##....##.....##.##....##.
#   .####.##....##.##.......########..#######..##.....##.########..########...........#######..##........########.##.....##.##.....##....##.....#######..##.....##
##########################################################################################################
boss__influxdb__operator__subdomain: influxdb
boss__influxdb__operator__manifest_path: "{{manifest_dir_path}}/influxdb-operator"
boss__influxdb__operator__namespace_name: monitoring
boss__influxdb__operator__name: influxdata-operator


boss__influxdb__operator__ingress_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__influxdb__operator__ingress_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__deployment_labels: |
  app: influxdata-operator
  name: influxdata-operator

boss__influxdb__operator__service_labels: |
    k8s-app: influxdata-operator
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "influxdata-operator"
    name: influxdata-operator
    boss-part-of: influxdb
    app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__persistent_volume_claim_spec_resources_requests_storage: "250Mi"

boss__influxdb__operator__persistent_volume_labels: |
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  k8s-app: influxdata-operator
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__version: 1.6.6
boss__influxdb__operator__image_repo: "influxdb"
boss__influxdb__operator__image_tag: "{{ boss__influxdb__operator__version }}"

boss__influxdb__operator__service_account_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator

boss__influxdb__operator__cluster_role_binding_labels: |
  k8s-app: influxdata-operator
  kubernetes.io/cluster-service: "true"
  addonmanager.kubernetes.io/mode: Reconcile
  kubernetes.io/name: "influxdata-operator"
  name: influxdata-operator
  boss-part-of: influxdb
  app: influxdata-operator



boss__influxdb__operator__persistent_volume_spec_capacity_storage: 250Mi
boss__influxdb__operator__persistent_volume_spec_nfs_path: "{{path_to_network_disk}}/influxdb-operator"
# boss__influxdb__operator__nfs_master_node_ip: "{{ hostvars[groups[nfs_server_group][0]]['ansible_' + main_network_interface].ipv4.address }}"
boss__influxdb__operator__nfs_master_node_ip: "{{ nfs_server_ip_override }}"


##########################################################################################################
#   .########.########.##....##
#   .##.......##.......##...##.
#   .##.......##.......##..##..
#   .######...######...#####...
#   .##.......##.......##..##..
#   .##.......##.......##...##.
#   .########.##.......##....##
##########################################################################################################
boss__bootstrap__subdomain: bootstrap
boss__bootstrap__manifest_path: "{{manifest_dir_path}}/bootstrap"
boss__bootstrap__namespace_name: kube-system
boss__bootstrap__deployment_name: bootstrap


##########################################################################################################
#   .########.##.......##.....##.########.##....##.########.........########..####.########..........######..########.##....##.########.########.....###....##.......####.########.########.########.
#   .##.......##.......##.....##.##.......###...##....##............##.....##..##.....##............##....##.##.......###...##....##....##.....##...##.##...##........##.......##..##.......##.....##
#   .##.......##.......##.....##.##.......####..##....##............##.....##..##.....##............##.......##.......####..##....##....##.....##..##...##..##........##......##...##.......##.....##
#   .######...##.......##.....##.######...##.##.##....##....#######.########...##.....##....#######.##.......######...##.##.##....##....########..##.....##.##........##.....##....######...##.....##
#   .##.......##.......##.....##.##.......##..####....##............##.....##..##.....##............##.......##.......##..####....##....##...##...#########.##........##....##.....##.......##.....##
#   .##.......##.......##.....##.##.......##...###....##............##.....##..##.....##............##....##.##.......##...###....##....##....##..##.....##.##........##...##......##.......##.....##
#   .##.......########..#######..########.##....##....##............########..####....##.............######..########.##....##....##....##.....##.##.....##.########.####.########.########.########.
##########################################################################################################
boss__fluent__bit__centralized__subdomain: fluent-bit-centralized
boss__fluent__bit__centralized__manifest_path: "{{manifest_dir_path}}/fluent-bit-centralized"
boss__fluent__bit__centralized__namespace_name: kube-system
boss__fluent__bit__centralized__fluent_bit_version: 1.0.5
boss__fluent__bit__centralized__fluent_bit_image_repo: "fluent/fluent-bit"
boss__fluent__bit__centralized__fluent_bit_image_tag: "{{ boss__fluent__bit__centralized__fluent_bit_version }}"
boss__fluent__bit__centralized__shared_name: fluent-bit-centralized
boss__fluent__bit__centralized__configmap_suffix: "-0-1-0"
boss__fluent__bit__centralized__create_namespace: "absent"

boss__fluent__bit__centralized__deployment_spec_template_containers_resources: |
  limits:
    cpu: ".2"
    memory: "100Mi"
  requests:
    cpu: "0.05"
    memory: "10Mi"

boss__fluent__bit__centralized__serviceAccountName: "{{boss__fluent__bit__centralized__shared_name}}"
boss__fluent__bit__centralized__deployment_spec_replicas: 2

# boss__fluent__bit__centralized__deployment_spec_template_containers_command: []

boss__fluent__bit__centralized__deployment_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__deployment_spec_matchLabels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1

boss__fluent__bit__centralized__deployment_spec_template_metadata_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__deployment_spec_template_metadata_annotations: |
  prometheus.io/scrape: "true"
  prometheus.io/port: "2020"
  prometheus.io/path: /api/v1/metrics/prometheus

boss__fluent__bit__centralized__deployment_spec_template_containers_ports: |-
  - containerPort: 2020
    name: http-metrics
  - containerPort: 5140
    name: syslog

boss__fluent__bit__centralized__deployment_spec_template_containers_env: |-
  # - name: FLUENT_ELASTICSEARCH_HOST
  #   value: "elasticsearch"
  # - name: FLUENT_ELASTICSEARCH_PORT
  #   value: "9200"

    # Memory limit that the file tail plugin can use when appending data to the Engine
  - name: TAIL_BUF_LIMIT
    value: "5MB"

  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

boss__fluent__bit__centralized__deployment_spec_template_spec_volumeMounts: |
  - name: logging-volume # The volume where logs will be delivered by the Docker logging driver
    mountPath: /logging-volume
  # The volume where Fluent Bit stores persistent data (position databases for tracking ingested files)
  - name: fluent-data
    mountPath: /var/fluent-bit
  # The Fluent Bit config file to use
  - name: fluent-bit-config
    mountPath: /fluent-bit/etc

boss__fluent__bit__centralized__deployment_spec_template_spec_volumes: |
  - name: logging-volume
    emptyDir: {}
  - name: fluent-bit-config
    configMap:
      name: {{boss__fluent__bit__centralized__shared_name}}-config{{boss__fluent__bit__centralized__configmap_suffix}}
  - name: fluent-data
    emptyDir: {}

boss__fluent__bit__centralized__service_labels: |-
  k8s-app: {{boss__fluent__bit__centralized__shared_name}}
  version: v1
  kubernetes.io/cluster-service: "true"

boss__fluent__bit__centralized__service_annotations: "disabled"

boss__fluent__bit__centralized__service_spec_type: ClusterIP

boss__fluent__bit__centralized__service_spec_ports: |-
  - port: 2020
    targetPort: 2020
    protocol: TCP
    name: http-metrics
  - port: 5140
    targetPort: 5140
    protocol: TCP
    name: syslog

boss__fluent__bit__centralized__service_spec_selector: |-
  app: {{boss__fluent__bit__centralized__shared_name}}

boss__fluent__bit__centralized__ingress_spec_rules_http_paths: |
  - path: /
    backend:
      serviceName: {{boss__fluent__bit__centralized__shared_name}}
      servicePort: 5140


boss__fluent__bit__centralized__grafana_ingress_metadata_annotations: |
  nginx.ingress.kubernetes.io/ssl-redirect: \"false\"
  traefik.frontend.rule.type: PathPrefix

boss__fluent__bit__centralized__grafana_ingress_metadata_labels: |
  run: nginx
  app: grafana


# ---
# local_release_dir: /tmp/releases

# # Used to only evaluate vars from download role
# skip_downloads: false

# # if this is set to true will only download files once. Doesn't work
# # on Container Linux by CoreOS unless the download_localhost is true and localhost
# # is running another OS type. Default compress level is 1 (fastest).
# download_run_once: False
# download_compress: 1

# # if this is set to true, uses the localhost for download_run_once mode
# # (requires docker and sudo to access docker). You may want this option for
# # local caching of docker images or for Container Linux by CoreOS cluster nodes.
# # Otherwise, uses the first node in the kube-master group to store images
# # in the download_run_once mode.
# download_localhost: False

# # Always pull images if set to True. Otherwise check by the repo's tag/digest.
# download_always_pull: False

# # Use the first kube-master if download_localhost is not set
# download_delegate: "{% if download_localhost %}localhost{% else %}{{groups['kube-master'][0]}}{% endif %}"

# # Versions
# kube_version: v1.9.5
# kubeadm_version: "{{ kube_version }}"
# etcd_version: v3.2.4
# # TODO(mattymo): Move calico versions to roles/network_plugins/calico/defaults
# # after migration to container download
# calico_version: "v2.6.8"
# calico_ctl_version: "v1.6.3"
# calico_cni_version: "v1.11.4"
# calico_policy_version: "v1.0.3"
# calico_rr_version: "v0.4.2"
# flannel_version: "v0.10.0"
# flannel_cni_version: "v0.3.0"
# istio_version: "0.2.6"
# vault_version: 0.8.1
# weave_version: 2.2.1
# pod_infra_version: 3.0
# contiv_version: 1.1.7
# cilium_version: "v1.0.0-rc8"

# # Download URLs
# istioctl_download_url: "https://storage.googleapis.com/istio-release/releases/{{ istio_version }}/istioctl/istioctl-linux"
# kubeadm_download_url: "https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/amd64/kubeadm"
# vault_download_url: "https://releases.hashicorp.com/vault/{{ vault_version }}/vault_{{ vault_version }}_linux_amd64.zip"

# # Checksums
# istioctl_checksum: fd703063c540b8c0ab943f478c05ab257d88ae27224c746a27d0526ddbf7c370
# kubeadm_checksum: 12b6e9ac1624852b7c978bde70b9bde9ca0e4fc6581d09bddfb117bb41f93c74
# vault_binary_checksum: 3c4d70ba71619a43229e65c67830e30e050eab7a81ac6b28325ff707e5914188

# # Containers
# etcd_image_repo: "quay.io/coreos/etcd"
# etcd_image_tag: "{{ etcd_version }}"
# flannel_image_repo: "quay.io/coreos/flannel"
# flannel_image_tag: "{{ flannel_version }}"
# flannel_cni_image_repo: "quay.io/coreos/flannel-cni"
# flannel_cni_image_tag: "{{ flannel_cni_version }}"
# calicoctl_image_repo: "quay.io/calico/ctl"
# calicoctl_image_tag: "{{ calico_ctl_version }}"
# calico_node_image_repo: "quay.io/calico/node"
# calico_node_image_tag: "{{ calico_version }}"
# calico_cni_image_repo: "quay.io/calico/cni"
# calico_cni_image_tag: "{{ calico_cni_version }}"
# calico_policy_image_repo: "quay.io/calico/kube-controllers"
# calico_policy_image_tag: "{{ calico_policy_version }}"
# calico_rr_image_repo: "quay.io/calico/routereflector"
# calico_rr_image_tag: "{{ calico_rr_version }}"
# istio_proxy_image_repo: docker.io/istio/proxy
# istio_proxy_image_tag: "{{ istio_version }}"
# istio_proxy_init_image_repo: docker.io/istio/proxy_init
# istio_proxy_init_image_tag: "{{ istio_version }}"
# istio_ca_image_repo: docker.io/istio/istio-ca
# istio_ca_image_tag: "{{ istio_version }}"
# istio_mixer_image_repo: docker.io/istio/mixer
# istio_mixer_image_tag: "{{ istio_version }}"
# istio_pilot_image_repo: docker.io/istio/pilot
# istio_pilot_image_tag: "{{ istio_version }}"
# istio_proxy_debug_image_repo: docker.io/istio/proxy_debug
# istio_proxy_debug_image_tag: "{{ istio_version }}"
# istio_sidecar_initializer_image_repo: docker.io/istio/sidecar_initializer
# istio_sidecar_initializer_image_tag: "{{ istio_version }}"
# istio_statsd_image_repo: prom/statsd-exporter
# istio_statsd_image_tag: latest
# hyperkube_image_repo: "gcr.io/google-containers/hyperkube"
# hyperkube_image_tag: "{{ kube_version }}"
# pod_infra_image_repo: "gcr.io/google_containers/pause-amd64"
# pod_infra_image_tag: "{{ pod_infra_version }}"
# install_socat_image_repo: "xueshanf/install-socat"
# install_socat_image_tag: "latest"
# netcheck_version: "v1.0"
# netcheck_agent_img_repo: "quay.io/l23network/k8s-netchecker-agent"
# netcheck_agent_tag: "{{ netcheck_version }}"
# netcheck_server_img_repo: "quay.io/l23network/k8s-netchecker-server"
# netcheck_server_tag: "{{ netcheck_version }}"
# weave_kube_image_repo: "weaveworks/weave-kube"
# weave_kube_image_tag: "{{ weave_version }}"
# weave_npc_image_repo: "weaveworks/weave-npc"
# weave_npc_image_tag: "{{ weave_version }}"
# contiv_image_repo: "contiv/netplugin"
# contiv_image_tag: "{{ contiv_version }}"
# contiv_auth_proxy_image_repo: "contiv/auth_proxy"
# contiv_auth_proxy_image_tag: "{{ contiv_version }}"
# cilium_image_repo: "docker.io/cilium/cilium"
# cilium_image_tag: "{{ cilium_version }}"
# nginx_image_repo: nginx
# nginx_image_tag: 1.13
# dnsmasq_version: 2.78
# dnsmasq_image_repo: "andyshinn/dnsmasq"
# dnsmasq_image_tag: "{{ dnsmasq_version }}"
# kubedns_version: 1.14.8
# kubedns_image_repo: "gcr.io/google_containers/k8s-dns-kube-dns-amd64"
# kubedns_image_tag: "{{ kubedns_version }}"
# coredns_version: 1.1.0
# coredns_image_repo: "docker.io/coredns/coredns"
# coredns_image_tag: "{{ coredns_version }}"
# dnsmasq_nanny_image_repo: "gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64"
# dnsmasq_nanny_image_tag: "{{ kubedns_version }}"
# dnsmasq_sidecar_image_repo: "gcr.io/google_containers/k8s-dns-sidecar-amd64"
# dnsmasq_sidecar_image_tag: "{{ kubedns_version }}"
# dnsmasqautoscaler_version: 1.1.2
# dnsmasqautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# dnsmasqautoscaler_image_tag: "{{ dnsmasqautoscaler_version }}"
# kubednsautoscaler_version: 1.1.2
# kubednsautoscaler_image_repo: "gcr.io/google_containers/cluster-proportional-autoscaler-amd64"
# kubednsautoscaler_image_tag: "{{ kubednsautoscaler_version }}"
# test_image_repo: busybox
# test_image_tag: latest
# elasticsearch_version: "v2.4.1"
# elasticsearch_image_repo: "gcr.io/google_containers/elasticsearch"
# elasticsearch_image_tag: "{{ elasticsearch_version }}"
# fluentd_version: "1.22"
# fluentd_image_repo: "gcr.io/google_containers/fluentd-elasticsearch"
# fluentd_image_tag: "{{ fluentd_version }}"
# kibana_version: "v4.6.1"
# kibana_image_repo: "gcr.io/google_containers/kibana"
# kibana_image_tag: "{{ kibana_version }}"
# helm_version: "v2.8.1"
# helm_image_repo: "lachlanevenson/k8s-helm"
# helm_image_tag: "{{ helm_version }}"
# tiller_image_repo: "gcr.io/kubernetes-helm/tiller"
# tiller_image_tag: "{{ helm_version }}"
# vault_image_repo: "vault"
# vault_image_tag: "{{ vault_version }}"
# registry_image_repo: "registry"
# registry_image_tag: "2.6"
# registry_proxy_image_repo: "gcr.io/google_containers/kube-registry-proxy"
# registry_proxy_image_tag: "0.4"
# local_volume_provisioner_image_repo: "quay.io/external_storage/local-volume-provisioner"
# local_volume_provisioner_image_tag: "v2.0.0"
# cephfs_provisioner_image_repo: "quay.io/kubespray/cephfs-provisioner"
# cephfs_provisioner_image_tag: "92295a30"
# ingress_nginx_controller_image_repo: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"
# ingress_nginx_controller_image_tag: "0.12.0"
# ingress_nginx_default_backend_image_repo: "gcr.io/google_containers/defaultbackend"
# ingress_nginx_default_backend_image_tag: "1.4"
# cert_manager_version: "v0.2.3"
# cert_manager_controller_image_repo: "quay.io/jetstack/cert-manager-controller"
# cert_manager_controller_image_tag: "{{ cert_manager_version }}"
# cert_manager_ingress_shim_image_repo: "quay.io/jetstack/cert-manager-ingress-shim"
# cert_manager_ingress_shim_image_tag: "{{ cert_manager_version }}"

# downloads:
#   netcheck_server:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_server_img_repo }}"
#     tag: "{{ netcheck_server_tag }}"
#     sha256: "{{ netcheck_server_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   netcheck_agent:
#     enabled: "{{ deploy_netchecker }}"
#     container: true
#     repo: "{{ netcheck_agent_img_repo }}"
#     tag: "{{ netcheck_agent_tag }}"
#     sha256: "{{ netcheck_agent_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   etcd:
#     enabled: true
#     container: true
#     repo: "{{ etcd_image_repo }}"
#     tag: "{{ etcd_image_tag }}"
#     sha256: "{{ etcd_digest_checksum|default(None) }}"
#     groups:
#       - etcd
#   kubeadm:
#     enabled: "{{ kubeadm_enabled }}"
#     file: true
#     version: "{{ kubeadm_version }}"
#     dest: "kubeadm"
#     sha256: "{{ kubeadm_checksum }}"
#     source_url: "{{ kubeadm_download_url }}"
#     url: "{{ kubeadm_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - k8s-cluster
#   istioctl:
#     enabled: "{{ istio_enabled }}"
#     file: true
#     version: "{{ istio_version }}"
#     dest: "istio/istioctl"
#     sha256: "{{ istioctl_checksum }}"
#     source_url: "{{ istioctl_download_url }}"
#     url: "{{ istioctl_download_url }}"
#     unarchive: false
#     owner: "root"
#     mode: "0755"
#     groups:
#       - kube-master
#   istio_proxy:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_image_repo }}"
#     tag: "{{ istio_proxy_image_tag }}"
#     sha256: "{{ istio_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_init:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_init_image_repo }}"
#     tag: "{{ istio_proxy_init_image_tag }}"
#     sha256: "{{ istio_proxy_init_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_ca:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_ca_image_repo }}"
#     tag: "{{ istio_ca_image_tag }}"
#     sha256: "{{ istio_ca_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_mixer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_mixer_image_repo }}"
#     tag: "{{ istio_mixer_image_tag }}"
#     sha256: "{{ istio_mixer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_pilot:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_pilot_image_repo }}"
#     tag: "{{ istio_pilot_image_tag }}"
#     sha256: "{{ istio_pilot_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_proxy_debug:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_proxy_debug_image_repo }}"
#     tag: "{{ istio_proxy_debug_image_tag }}"
#     sha256: "{{ istio_proxy_debug_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_sidecar_initializer:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_sidecar_initializer_image_repo }}"
#     tag: "{{ istio_sidecar_initializer_image_tag }}"
#     sha256: "{{ istio_sidecar_initializer_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   istio_statsd:
#     enabled: "{{ istio_enabled }}"
#     container: true
#     repo: "{{ istio_statsd_image_repo }}"
#     tag: "{{ istio_statsd_image_tag }}"
#     sha256: "{{ istio_statsd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   hyperkube:
#     enabled: true
#     container: true
#     repo: "{{ hyperkube_image_repo }}"
#     tag: "{{ hyperkube_image_tag }}"
#     sha256: "{{ hyperkube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   cilium:
#     enabled: "{{ kube_network_plugin == 'cilium' }}"
#     container: true
#     repo: "{{ cilium_image_repo }}"
#     tag: "{{ cilium_image_tag }}"
#     sha256: "{{ cilium_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel:
#     enabled: "{{ kube_network_plugin == 'flannel' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ flannel_image_repo }}"
#     tag: "{{ flannel_image_tag }}"
#     sha256: "{{ flannel_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   flannel_cni:
#     enabled: "{{ kube_network_plugin == 'flannel' }}"
#     container: true
#     repo: "{{ flannel_cni_image_repo }}"
#     tag: "{{ flannel_cni_image_tag }}"
#     sha256: "{{ flannel_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calicoctl:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calicoctl_image_repo }}"
#     tag: "{{ calicoctl_image_tag }}"
#     sha256: "{{ calicoctl_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_node:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_node_image_repo }}"
#     tag: "{{ calico_node_image_tag }}"
#     sha256: "{{ calico_node_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_cni:
#     enabled: "{{ kube_network_plugin == 'calico' or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_cni_image_repo }}"
#     tag: "{{ calico_cni_image_tag }}"
#     sha256: "{{ calico_cni_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_policy:
#     enabled: "{{ enable_network_policy or kube_network_plugin == 'canal' }}"
#     container: true
#     repo: "{{ calico_policy_image_repo }}"
#     tag: "{{ calico_policy_image_tag }}"
#     sha256: "{{ calico_policy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   calico_rr:
#     enabled: "{{ peer_with_calico_rr is defined and peer_with_calico_rr and kube_network_plugin == 'calico' }}"
#     container: true
#     repo: "{{ calico_rr_image_repo }}"
#     tag: "{{ calico_rr_image_tag }}"
#     sha256: "{{ calico_rr_digest_checksum|default(None) }}"
#     groups:
#       - calico-rr
#   weave_kube:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_kube_image_repo }}"
#     tag: "{{ weave_kube_image_tag }}"
#     sha256: "{{ weave_kube_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   weave_npc:
#     enabled: "{{ kube_network_plugin == 'weave' }}"
#     container: true
#     repo: "{{ weave_npc_image_repo }}"
#     tag: "{{ weave_npc_image_tag }}"
#     sha256: "{{ weave_npc_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_image_repo }}"
#     tag: "{{ contiv_image_tag }}"
#     sha256: "{{ contiv_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   contiv_auth_proxy:
#     enabled: "{{ kube_network_plugin == 'contiv' }}"
#     container: true
#     repo: "{{ contiv_auth_proxy_image_repo }}"
#     tag: "{{ contiv_auth_proxy_image_tag }}"
#     sha256: "{{ contiv_auth_proxy_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   pod_infra:
#     enabled: true
#     container: true
#     repo: "{{ pod_infra_image_repo }}"
#     tag: "{{ pod_infra_image_tag }}"
#     sha256: "{{ pod_infra_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   install_socat:
#     enabled: "{{ ansible_os_family in ['CoreOS', 'Container Linux by CoreOS'] }}"
#     container: true
#     repo: "{{ install_socat_image_repo }}"
#     tag: "{{ install_socat_image_tag }}"
#     sha256: "{{ install_socat_digest_checksum|default(None) }}"
#     groups:
#       - k8s-cluster
#   nginx:
#     enabled: "{{ loadbalancer_apiserver_localhost }}"
#     container: true
#     repo: "{{ nginx_image_repo }}"
#     tag: "{{ nginx_image_tag }}"
#     sha256: "{{ nginx_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq:
#     enabled: "{{ dns_mode == 'dnsmasq_kubedns' }}"
#     container: true
#     repo: "{{ dnsmasq_image_repo }}"
#     tag: "{{ dnsmasq_image_tag }}"
#     sha256: "{{ dnsmasq_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubedns:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubedns_image_repo }}"
#     tag: "{{ kubedns_image_tag }}"
#     sha256: "{{ kubedns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   coredns:
#     enabled: "{{ dns_mode in ['coredns', 'coredns_dual'] }}"
#     container: true
#     repo: "{{ coredns_image_repo }}"
#     tag: "{{ coredns_image_tag }}"
#     sha256: "{{ coredns_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_nanny:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_nanny_image_repo }}"
#     tag: "{{ dnsmasq_nanny_image_tag }}"
#     sha256: "{{ dnsmasq_nanny_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   dnsmasq_sidecar:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ dnsmasq_sidecar_image_repo }}"
#     tag: "{{ dnsmasq_sidecar_image_tag }}"
#     sha256: "{{ dnsmasq_sidecar_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kubednsautoscaler:
#     enabled: "{{ dns_mode in ['kubedns', 'dnsmasq_kubedns'] }}"
#     container: true
#     repo: "{{ kubednsautoscaler_image_repo }}"
#     tag: "{{ kubednsautoscaler_image_tag }}"
#     sha256: "{{ kubednsautoscaler_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   testbox:
#     enabled: false
#     container: true
#     repo: "{{ test_image_repo }}"
#     tag: "{{ test_image_tag }}"
#     sha256: "{{ testbox_digest_checksum|default(None) }}"
#   elasticsearch:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ elasticsearch_image_repo }}"
#     tag: "{{ elasticsearch_image_tag }}"
#     sha256: "{{ elasticsearch_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   fluentd:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ fluentd_image_repo }}"
#     tag: "{{ fluentd_image_tag }}"
#     sha256: "{{ fluentd_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   kibana:
#     enabled: "{{ efk_enabled }}"
#     container: true
#     repo: "{{ kibana_image_repo }}"
#     tag: "{{ kibana_image_tag }}"
#     sha256: "{{ kibana_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   helm:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ helm_image_repo }}"
#     tag: "{{ helm_image_tag }}"
#     sha256: "{{ helm_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   tiller:
#     enabled: "{{ helm_enabled }}"
#     container: true
#     repo: "{{ tiller_image_repo }}"
#     tag: "{{ tiller_image_tag }}"
#     sha256: "{{ tiller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   vault:
#     enabled: "{{ cert_management == 'vault' }}"
#     container: "{{ vault_deployment_type != 'host' }}"
#     file: "{{ vault_deployment_type == 'host' }}"
#     dest: "vault/vault_{{ vault_version }}_linux_amd64.zip"
#     mode: "0755"
#     owner: "vault"
#     repo: "{{ vault_image_repo }}"
#     sha256: "{{ vault_binary_checksum if vault_deployment_type == 'host' else vault_digest_checksum|d(none) }}"
#     source_url: "{{ vault_download_url }}"
#     tag: "{{ vault_image_tag }}"
#     unarchive: true
#     url: "{{ vault_download_url }}"
#     version: "{{ vault_version }}"
#     groups:
#       - vault
#   registry:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_image_repo }}"
#     tag: "{{ registry_image_tag }}"
#     sha256: "{{ registry_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   registry_proxy:
#     enabled: "{{ registry_enabled }}"
#     container: true
#     repo: "{{ registry_proxy_image_repo }}"
#     tag: "{{ registry_proxy_image_tag }}"
#     sha256: "{{ registry_proxy_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   local_volume_provisioner:
#     enabled: "{{ local_volume_provisioner_enabled }}"
#     container: true
#     repo: "{{ local_volume_provisioner_image_repo }}"
#     tag: "{{ local_volume_provisioner_image_tag }}"
#     sha256: "{{ local_volume_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cephfs_provisioner:
#     enabled: "{{ cephfs_provisioner_enabled }}"
#     container: true
#     repo: "{{ cephfs_provisioner_image_repo }}"
#     tag: "{{ cephfs_provisioner_image_tag }}"
#     sha256: "{{ cephfs_provisioner_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   ingress_nginx_controller:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_controller_image_repo }}"
#     tag: "{{ ingress_nginx_controller_image_tag }}"
#     sha256: "{{ ingress_nginx_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   ingress_nginx_default_backend:
#     enabled: "{{ ingress_nginx_enabled }}"
#     container: true
#     repo: "{{ ingress_nginx_default_backend_image_repo }}"
#     tag: "{{ ingress_nginx_default_backend_image_tag }}"
#     sha256: "{{ ingress_nginx_default_backend_digest_checksum|default(None) }}"
#     groups:
#       - kube-ingress
#   cert_manager_controller:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_controller_image_repo }}"
#     tag: "{{ cert_manager_controller_image_tag }}"
#     sha256: "{{ cert_manager_controller_digest_checksum|default(None) }}"
#     groups:
#       - kube-node
#   cert_manager_ingress_shim:
#     enabled: "{{ cert_manager_enabled }}"
#     container: true
#     repo: "{{ cert_manager_ingress_shim_image_repo }}"
#     tag: "{{ cert_manager_ingress_shim_image_tag }}"
#     sha256: "{{ cert_manager_ingress_shim_digest_checksum|default(None) }}"
#     groups:
#       - kube-node

# download_defaults:
#   container: false
#   file: false
#   repo: None
#   tag: None
#   enabled: false
#   dest: None
#   version: None
#   url: None
#   unarchive: false
#   owner: kube
#   mode: None



# ---
# elasticsearch_cpu_limit: 1000m
# elasticsearch_mem_limit: 0M
# elasticsearch_cpu_requests: 100m
# elasticsearch_mem_requests: 0M
# elasticsearch_service_port: 9200
