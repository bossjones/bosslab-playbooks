# SOURCE: https://github.com/dacort/syslog-to-athena/blob/a20869e540d5fac9c7913a968fb4282ff5b553a8/s3-fluentd/fluent.conf
# REQUIRES: fluent-plugin-s3
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  tag syslog
</source>

<match syslog.**>
  @type s3

  # Specify the bucket and region to write logs to
  s3_bucket dacort-logs
  s3_region us-west-2

  # This is the prefix that logs get written into
  # By default, Hive-style partitions by day are used
  path syslog/year=%Y/month=%m/day=%d/

  # Set these or pass in environment variables via Docker command
  # aws_key_id <ACCESS_KEY>
  # aws_sec_key <SECRET_KEY>

  # This configuration buffers logs on the local filesystem
  # and uploads them to S3 every "flush_interval"
  # if you want to use ${tag} or %Y/%m/%d/ like syntax in path / s3_object_key_format,
  # need to specify tag for ${tag} and time for %Y/%m/%d in <buffer> argument
  <buffer tag, time>
    @type file
    #path /var/log/fluent/s3
    path /tmp/fluent_s3

    # Control how chunks are written
    # Reference: https://docs.fluentd.org/v1.0/articles/buffer-section#time
    timekey 3600 # 1 hour partition
    timekey_wait 5m
    timekey_use_utc true # use utc
    chunk_limit_size 256m

    # Enable during testing to have data delivered faster
    flush_interval 30s
    flush_mode interval
    flush_at_shutdown true
  </buffer>

  format json
  include_time_key true

</match>
